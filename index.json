[{"content":"一个简单的博客 :link: https://lartpang.github.io/blog 这是一个基于 GitHub Issue 和 Page 的简单的静态博客。\n","date":"March 17, 2025","permalink":"/blog/about/about/","summary":"一个简单的博客 :link: https://lartpang.github.io/blog 这是一个基于 GitHub Issue 和 Page 的简单的静态博客。\n","title":"啊，哈！","type":"about"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/23 1. 引言 数据共享是生物医学创新的重要推动力。公共数据仓库和生物银行使不同组织的研究人员能够分析他们自己可能无法收集的大量人类主体数据。许多学术实验室、商业企业和医院已经联合起来形成合作联盟，共享生物医学数据，希望从数据集中提取出由于数据集规模有限而无法被单个实体访问的洞察。政府实体（例如，美国国立卫生研究院数据管理和共享政策；https://sharing.nih.gov ）和国际标准制定组织如全球基因组学和健康联盟（1）制定的政策和指南在维护生物医学界的数据共享文化中发挥了关键作用，这一传统根植于人类基因组计划等里程碑式的合作努力中。\n随着我们进入个性化医疗时代，生物医学数据的更广泛共享变得比以往任何时候都更加重要。现有生物医学数据集中所代表的人类群体的多样性有限，加剧了不同群体从生物医学进步中获益的不平等（2）。研究罕见疾病通常需要合并跨组织的小型患者队列以增强统计能力（3）。此外，准确推断与每个独特个体相关的健康洞察需要访问在大型和多模态数据集上训练的计算模型，这些数据集捕捉了健康和疾病中个体变化的广泛范围。尽管最近创建的生物银行 [例如，All of Us 研究计划（4）] 在招募多样化的研究参与者方面迈出了重要一步，但这些资源越来越多地存储在孤立的计算环境中，限制了这些数据集的范围和使用。\n为了进一步扩大生物医学领域的数据共享工作，必须用强有力的缓解措施解决日益增长的隐私风险。没有这些措施，我们可能会看到对受限数据孤岛的更大依赖，这种情况因最近更严格的隐私法规的激增（例如，欧盟的通用数据保护条例（GDPR]）和生物医学数据规模及计算进步带来的风险增加而进一步加剧。此外，大规模数据泄露可能会侵蚀公众对科学企业的信任。信任的丧失不仅可能阻碍收集大型数据集的努力，还可能通过不成比例地影响某些人群参与研究的意愿而加剧不平等。\n隐私增强技术（PETs）提供了有希望的技术解决方案，通过采用各种数学、算法和硬件设计方法，使敏感数据的共享和分析成为可能，同时保护隐私（图 1）。PETs 涵盖了广泛的技术，这些技术解决了不同的数据共享场景，并在支持的分析类型、计算成本和提供的隐私保护程度方面引入了各种权衡。在这篇综述中，我们专注于文献中最广泛研究的技术，包括同态加密（HE）、安全多方计算（MPC）、可信执行环境（TEE）、差分隐私（DP）和联邦学习（FL）。最近的进展极大地增加了这些技术在生物医学领域的适用性，正如我们在这篇综述中所说明的。与将 PETs 描述为解决生物医学数据共享挑战的潜在解决方案的现有综述（5-9）不同，我们专注于提供 PETs 最新进展的易于理解的总结，检查其技术基础和生物医学应用。\n这篇综述的其余部分组织如下。第 2 节介绍了生物医学数据隐私的背景，涵盖了其历史背景和关键概念。第 3 节概述了涉及数据共享的生物医学研究场景。第 4 节深入探讨了每种 PET，提供了技术概述、最新进展、局限性和最近出版物，并探索了其在生物医学任务中的应用。第 5 节回顾了有助于促进生物医学数据共享的相关技术。最后，在第 6 节中，我们通过讨论开放性挑战和突出未来工作的关键方向来结束。\n2. 生物医学数据隐私：挑战和现有保障 生物医学领域的数据隐私挑战在过去几十年中不断演变，受到技术进步、公众意识提高和政策法律变化的影响。从 1960 年代到 1980 年代，生物医学界见证了人体研究伦理原则的建立，如贝尔蒙特报告（10）所体现的文件。与此同时，由于医疗记录的数字化，对患者隐私的关注增加。1990 年代见证了通用规则和健康保险流通与责任法案（HIPAA）（11）的建立，标志着为保护研究和医疗保健中的生物医学数据而创建法律框架的初步努力。人类基因组计划的完成和 2000 年代遗传研究的快速增长加剧了对人类主体及其遗传隐私保护的关注。2010 年代见证了更广泛的社会背景下隐私问题的激增，这些担忧由社交媒体的兴起、大规模数据泄露和围绕政府监控的争议（12, 13）引发。国际社会通过加强对个人信息收集、共享和使用的监督来回应这些担忧，例如 2018 年颁布的 GDPR。更近的例子包括亚利桑那州和加利福尼亚州 2021 年的遗传隐私法律（14），这些法律加强了存储和共享遗传数据的隐私要求，以及国家标准与技术研究院的基因组网络安全倡议（https://www.nccoe.nist.gov/projects/cybersecurity-genomic-data ），该倡议呼吁在基因组学中制定新的安全标准。目前，隐私问题持续存在并扩展到新的生物医学领域，如数字健康 [例如，电子健康记录（EHR）、移动应用和可穿戴设备]、多组学和 COVID-19 大流行后的流行病响应（15）。大型生物银行和数据仓库的数量不断增加，进一步放大了这些挑战。\n生物医学数据泄露的风险是多方面的。虽然个人对可接受的隐私风险有不同的看法，但未经授权的暴露与个人生物学和健康相关的私密信息可能导致情感困扰、污名化和在就业、教育和保险机会中的歧视。也许更令人担忧的是，这些伤害可能会扩展到家庭和人口群体，例如，通过披露个人之间的遗传关系或群体内较高的健康风险。从管理敏感生物医学数据的组织的角度来看，数据泄露可能导致罚款、法律后果、运营中断和声誉损害。这些风险不仅仅是假设性的——在撰写本文时，已对 23andMe 提起诉讼，原因是数据泄露泄露了近 100 万客户的信息，包括他们的全名、出生日期和 DNA 档案，这些信息在暗网上被出售，每个泄露的个人高达 10 美元（16）。如果这种泄露事件重演，将导致对生物医学研究企业和卫生系统的信任下降，这将进一步阻碍未来的研究工作，阻碍科学进步。\n生物医学隐私泄露可能通过多种途径发生，每个途径都带来独特的挑战（8, 17）。表型推断旨在从不同类型的生物医学数据中推断出个人的特征或健康状况，通常是以意想不到的方式。再识别发生在不足以匿名化的数据被重新链接回个人时。这可能涉及结合多个数据集、利用辅助信息或利用去识别过程中的漏洞。数据链接整合了不同数据集的信息，构建了个人的更全面资料，允许攻击者揭示身份、健康状况或其他敏感信息。即使不直接共享包括个人层面信息的完整数据集，隐私仍可能被泄露：数据重建攻击试图组装可用信息的片段以揭示原始数据的一部分；成员资格推断攻击专注于确定个人是否属于特定数据集，这可能透露该人是否属于敏感或污名化的群体，可能导致隐私侵犯。\n保护生物医学数据隐私的传统方法包括政策和法律、技术安全措施和合同协议。法律和财务处罚有助于防止生物医学信息的滥用；HIPAA 和 GDPR 都为不合规规定了这样的处罚。保护生物医学数据的标准做法涉及对静态数据进行加密、采用安全的计算基础设施和去识别策略（18）。访问控制和用户认证机制也被广泛使用，以确保只有授权的个人才能访问敏感数据。此外，共享敏感数据的研究人员和组织通常会建立数据使用协议（DUAs）来定义数据的允许使用和数据管理指南。类似地，业务关联协议，即 HIPAA 覆盖实体（例如，医院）和它们的业务关联（例如，第三方服务提供商或合作伙伴）之间的合同，是确保处理受保护健康信息的实体遵守数据保护标准的的重要工具。\n尽管这些方法为生物医学数据提供了有用的保障，但它们并没有消除数据泄露或再识别的可能性，主要依赖于限制对数据的访问来实现安全，这导致许多数据集被隔离并置于大多数研究人员无法触及的地方。此外，检测违规行为和执行处罚的能力在实践中可能受到限制；确定哪些数据被视为私密或足够去识别以共享的法律和政策标准缺乏明确的定义。因此，许多现有的数据集要么基于信任不安全地共享，要么由于隐私风险被认为不适合共享。\n3. 数据共享场景和限制 隐私风险和数据共享限制在生物医学研究和实践的不同场景中各不相同。在这里，我们概述了典型的数据共享场景（如图 1 所示）以及每个背景下利益相关者面临的挑战。\n3.1. 研究参与 个人可能自愿为研究（无论是临床还是非临床）、数据仓库和第三方服务贡献健康数据和其他个人信息。参与者通常提供知情同意，其中概述了数据共享的细节，包括目的、范围和涉及的潜在风险。数据共享使参与者受益，因为它提高了对健康和疾病的集体理解，可能导致更好的治疗或其他健康相关决策。在某些服务的背景下，参与者也可能获得个性化的数据洞察。隐私关注是个人决定参与研究的一个关键因素（19）。知情同意的充分性仍然是一个持续辩论的话题，特别是关于获取广泛同意用于数据的二次使用（20）的伦理问题。有效地传达隐私风险也可能具有挑战性，因为一些风险需要技术专长才能完全理解，甚至在研究人员中也知之甚少。\n3.2. 可查询数据库 可查询数据库专门设计为允许用户通过查询检索信息，为访问生物医学数据提供结构化和高效的方式。例子包括用于研究招募的患者注册表和包含注释遗传变异的各种数据库（21）、EHR（22）和临床试验数据（23）。虽然查询的限制性质最小化了信息泄露，但研究表明，即使有这些限制，意外的披露仍然可能发生（24）。这样的担忧可能会阻碍个人参与这些数据库，并可能对数据库提供商构成数据管理挑战。\n3.3. 分析服务 这指的是将计算任务委托给可以访问受限模型或数据资源或更多计算资源的外部实体的做法。生物医学分析工作流程的日益增长的计算需求促使研究人员越来越多地使用这些第三方服务（25），其中许多服务托管在云环境中。然而，隐私问题或法规可能限制这些服务的使用（例如，希望将数据上传到在非欧盟国家运营的服务的欧盟研究人员）。此外，服务提供商可能需要引入措施来保护服务器使用的辅助模型和数据，这些模型和数据可能通过返回给用户的分析结果泄露（26）。\n3.4. 合作研究 不同机构或国家的研究人员越来越需要通过合并数据来共享研究目标，以获得对感兴趣的生物医学现象更完整的理解。这种场景通常涉及建立专注于特定健康状况或研究领域的联盟。然而，组织可能需要遵守限制或禁止数据外部共享的政策，如果实体在不同的监管环境或国家中运营，这个问题会加剧。\n3.5. 公共数据发布 这涉及使生物医学数据集和分析结果对更广泛的科学界和公众公开可访问。这种做法支持科学研究的透明度和可重复性，并促进了数据科学竞赛（例如，Kaggle；https://www.kaggle.com/ ）等合作努力。它还通过允许全球的研究人员重新分析现有数据，扩展了收集的数据集的效用。然而，发布包含个人层面信息的数据集存在极大的隐私风险，并且只有在罕见的情况下才可行。在发布基于部分私有数据的模拟或编辑数据集时，也必须小心，因为这些数据集仍可能导致隐私泄露。\n3.6. 公共卫生监测 COVID-19 大流行促使设计能够监测疾病爆发并促进响应（例如，暴露通知应用程序）的公共卫生系统。这些系统的有效运行可能需要收集广泛的个人信息，包括健康状况之外的人口统计、地理位置和社会活动数据。此外，跨管辖区共享这些数据可能对于更准确地了解传染病代理是必要的。然而，由于披露私密信息可能对个人造成伤害的可能性仍然是一个重大关切（15），这可能阻止这些系统的广泛采用。\n4. 隐私增强技术及其在生物医学中的应用 PETs 代表了一类计算技术，用于保护敏感的生物医学数据。总的来说，这些技术使开发用于共享和分析生物医学数据的隐私设计方法成为可能。这些方法提高了生物医学数据的隐私和效用，超出了现有安全实践和合同保障（例如，DUAs）所可行的范围。我们详细描述了每项技术，并讨论了最近的技术进步和在生物医学领域中的应用。\n4.1. 安全多方计算 MPC 允许多个参与方共同在他们的私有输入上执行计算，而不会向彼此透露输入。MPC 主要使用两种技术：混乱电路和秘密共享。\n1986 年，姚（Yao）首次介绍了混乱电路（27），它允许在两个拥有私有输入的参与方之间安全地评估一个函数，该函数表示为布尔电路。混乱电路中的每个逻辑门的输入和输出都是随机掩蔽的，以防止评估器在电路评估过程中获得信息。一方的输入使用称为无意识传输的密码原语安全地传达给另一方，然后允许接收者在不知道原始输入的情况下评估电路。尽管对于复杂的分析任务，电路大小的指数级扩展通常会导致高通信和计算成本，但几项增强（28-31）已经提高了这些方案的效率（32-34）。\n秘密共享方案（35, 36）允许一组参与方共同对一个私有数字进行编码，将其划分为由各方单独持有的随机份额。只有当预定义数量的份额结合时，才能重建私有数字。例如，在实际设置中最常用的加法秘密共享方案中，秘密份额是环（一个代数结构，由一组整数模某个特定数字表示）的随机元素，它们加起来等于私有值。这确保了只有当所有参与方的份额结合时才能揭示秘密；任何子集都不会透露可以用来推断秘密的信息。安全地添加两个秘密共享数字，x 和 y，涉及每个参与方添加他们各自的 x 和 y 份额，从而产生代表 x + y 的新份额。安全乘法需要参与方之间的交互（37），但通过掩蔽各方之间共享的数字来保护私有输入的保密性。其他操作，如除法、平方根和比较，是使用加法、乘法和利用私有值的位表示的特殊例程执行的。这些操作可以结合使用，以安全地执行多个参与方持有的私有数据的各种分析。\n已经开发了许多框架和编译器，以简化利用各种构建块协议的 MPC 算法的实现（38）。混合方案（39, 40）和编译器（32, 38, 41），结合不同的 MPC 方法以提高效率，也已经提出。例如，ABY（41）提出了在混乱电路和不同类型的秘密共享（整数或布尔）之间切换，以在最高效的域中执行每个操作（例如，在两方设置中使用混乱电路评估比较，或在涉及超过两方的其他位操作中使用布尔秘密共享）。这些框架已经扩展和优化，用于机器学习（ML）应用，如神经网络模型的训练和推理（42-45）。核心操作（46）的最新增强进一步提高了 MPC 框架的性能和多功能性。\nMPC 的主要限制是其大量的通信成本。虽然混乱电路允许大部分计算通过单轮通信非交互性地执行，但电路通常限于布尔操作，对于复杂的数值计算，电路的大小可能变得不切实际的大。与混乱电路相比，秘密共享通常在一般情况下提供更大的分析灵活性和效率，但基于秘密共享的 MPC 通常需要许多轮交互来完成复杂任务，这可能是有限通信设置（例如，具有大往返延迟的广域网）的潜在瓶颈。此外，要求在整个输入数据集中秘密共享的要求可能是大规模生物医学数据集的一个障碍。\n最近的工作已经开发了 MPC 协议，用于生物医学领域的一系列分析任务（47-52）。这些工作的共同目标是通过重新设计分析任务，使其更适用于使用 MPC 操作进行高效计算，从而提高 MPC 的效率。例如，Cho 等人（47）引入了秘密共享技术的一种泛化，旨在最小化冗余计算，从而实现了一种高效的全基因组关联研究（GWAS）算法，涉及复杂的线性代数任务，如主成分分析。这项工作被扩展到使用神经网络模型进行药物 - 靶标相互作用的协作预测（53）。Jagadeesh 等人（49）使用混乱电路有效地执行布尔操作（如集合交集和差集），以识别患者基因组中感兴趣的遗传变异。Von Maltitz 等人（54）引入了一种基于 Kaplan-Meier 估计器的生存分析的 MPC 协议。Smajlovi´c 等人（55）采取了不同的方法，开发了一种基于 Python 的编译器，将高级分析代码转换为 MPC 可执行文件，结合基于静态代码分析的自动优化。这些工具可以帮助加速 MPC 应用程序的开发，用于各种生物医学任务，通过使技术对生物医学从业者更易于访问。\n4.2. 同态加密 HE 指的是一种加密形式，它允许在加密数据上直接进行计算。早期的 HE 方案，如 Rivest 等人（56）、Elgamal（57）、Paillier（58）和 Goldwasser \u0026amp; Micali（59）的方案，被称为分层或某种 HE 方案，支持特定类型的操作，例如，仅加法或仅乘法，或者有限数量的操作。2009 年，Gentry（60）引入了第一个完全同态加密（FHE）方案，它通过一种自举技术，允许任意算术计算，该技术刷新密文（加密数据）以支持额外的操作。为了解决 Gentry 最初方案的有限具体效率问题，该方案需要每比特操作几分钟的运行时间（61），后来提出了几个方案（62-65），降低了 FHE 的整体计算成本，从而使其在实际应用中得以使用。\n与标准加密方案类似，HE 的安全性基于对研究充分的数学问题的难度。许多 HE 方案基于环学习误差（RLWE）问题（66, 67），这是一个基于格的问题，目标是区分一组环元素是随机采样的，还是近似于将已知一组元素与一个共同的秘密元素相乘的结果。这个问题被证明在不知道秘密的情况下极其难以解决，但否则很容易，转化为只有知道解密密钥的实体才能解密密文的保证。引入到密文中的随机噪声，以保持这个问题的难度，随着每次同态操作而增加。与精确执行计算以牺牲减少编码值范围的方案（62-64）不同，Cheon 等人（65）的 CKKS 方案直接将噪声添加到数据值中，实现了在小精度损失下的有效操作。CKKS 已在可以容忍少量噪声的科学应用中得到广泛采用。在所有基于 RLWE 的方案中，单个密文编码多个值，并且同态操作如加法和乘法同时在密文中的所有值上执行——被称为单指令多数据属性。利用这一属性可以提高这些方案的可扩展性。\n值得注意的最新发展包括更有效的自举技术（68, 69）和提供不同类型操作之间权衡的替代构造。例如，Chillotti 等人（70）基于环的数学结构构建的 TFHE 方案，允许有效的自举，但限于布尔或位操作。此外，已经提出了几个 HE 编译器（71），以简化 HE 算法的开发和优化，例如，简化密文噪声的管理。还为安全训练预测性 ML 模型（72, 73）开发了定制框架。\n在生物医学领域，HE 主要应用于涉及敏感数据的计算任务的外包。这些计算可能因问题规模（就数据集大小和计算复杂性而言）或对分析所需的额外数据或模型的有限访问而对个人用户来说具有挑战性。HE 有助于确保用户的数据在委托给第三方进行分析时保持私密。例如，已经提出了基于 HE 的解决方案，用于私密外包心电图数据中的心脏状况检测，以及基于健康记录的心血管风险预测。许多工作已经解决了在加密数据上计算 GWAS 统计量的问题，涉及一系列统计量和应用设置（76-80）。文献中探索的其他任务包括对基因组和医疗数据库的计数查询（例如，用于队列探索）（81, 82），检测遗传亲子关系（83），以及使用临床和基因组信息进行疾病风险预测（74, 84）。最后，Kim 等人（85）和 Gürsoy 等人（86）最近展示了对加密私有基因组的安全插补。\n这些进步使基于 HE 的解决方案更接近满足生物医学应用的要求。然而，由于几个因素，这些应用的范围仍然受到限制，包括与未加密分析相比同态操作的大量计算开销、需要使用加法和乘法近似非线性操作，以及由于自举的高成本而对分析任务的复杂性的实际限制。此外，大多数上述解决方案要求将所有输入数据加密并传输给执行计算的实体，这对于大型数据集可能是一个重大负担。在标题为“扩展同态加密到协作分析设置：多方同态加密”的侧边栏中，我们描述了最近的一项技术进步，有助于解决这些限制。\n4.3. 可信执行环境 TEE 是主处理器内的安全区域，也称为飞地，它确保软件的安全和隔离执行。这种隔离保证了内存内容、与外部实体的端到端通信以及应用程序的控制流受到保护，不受同一硬件上运行的不受信任或恶意进程的影响，包括恶意操作系统或虚拟机管理程序（103, 104）。在某些 TEE 体系结构中，应用程序的二进制可执行文件也可以通过称为远程认证的过程进行验证（105）。为了实现这些安全属性，TEE 依赖于内置于处理器的核心硬件安全组件，这些组件不能被软件操纵。这些组件通常包括一个内存加密引擎和控制器以隔离内存访问，以及用于加密密钥存储和操作的集成电路。\n最近的 TEE 发展集中在支持在不受信任的云环境中部署第三方软件，解决用户级应用程序和虚拟机（VM）的部署。流行的 TEE 平台包括 Intel Software Guard Extension（SGX）（106）用于用户级应用程序，以及 Intel Trust Domain Extensions（107）和 AMD Secure Encrypted Virtualization（108）用于 VM。Nvidia 最近推出了其图形处理单元（GPU）架构的更新，使 GPU 计算能够在 TEE 中进行（109）。在移动环境中，Arm TrustZone（104）是 Arm 中央处理单元（CPU）上无处不在的 TEE 平台，但通常移动应用程序可用的 TEE 功能集有限。\n尽管 TEE 提供了在类似传统计算环境中保密分析私有数据的能力和功能，它们的主要缺点在于实现基于硬件的安全的复杂性。与依赖于最小且确立的密码原语的 MPC 和 HE 不同，TEE 的基于硬件的方法引入了独特的漏洞。一些漏洞是由于 CPU 架构错误导致的，这些错误允许恶意进程从飞地中提取受保护的数据（110）；制造商通常会在发现这些问题后立即修补。其他限制是 TEE 体系结构固有的，导致了侧信道问题——信息泄露的间接途径（111, 112）。例如，TEE 飞地对内存页面的访问模式可能会无意中向攻击者泄露存储在安全区域内的敏感信息。尽管这类攻击需要大量的努力，但在需要最高安全级别的软件级别缓解是必要的。一种策略是确保程序的内存访问或时序模式不依赖于敏感信息（113）。然而，这种缓解可能会增加额外的计算负担，并在算法开发过程中需要相关专业知识。\n尽管存在这些缺点，TEE 有着光明的未来，主要 CPU 生产商如 Intel、AMD 和 Arm 对它们有着浓厚的兴趣，他们继续解决安全问题并改进他们的 TEE 平台。云服务提供商如 Google Cloud Platform 和 Microsoft Azure 提供 TEE 启用的计算基础设施。此外，还成立了诸如机密计算联盟（https://confidentialcomputing.io ）和互联网工程任务组的可信执行环境配置（TEEP）工作组（https://datatracker.ietf.org/wg/teep/about ），以支持与 TEE 相关的开源项目和标准开发。在研究社区中，近年来引入了许多软件工具，以简化现有软件转换为在 TEE 平台上安全运行的过程（112）。\n在生物医学领域，TEE 由于其能够安全地外包生物医学数据分析，并促进在大规模上开发和部署健康人工智能（AI）工具，已经获得了显著的吸引力。值得注意的真实世界例子包括 BeeKeeperAI（114），这是一家保护隐私的医疗保健 AI 公司，以及 AOK，这是德国 11 个地区健康保险公司的网络。这些组织利用 Intel SGX 来保护机密患者数据，遵守 HIPAA、GDPR 和德国患者数据保护法（115）等法规。基因组学中 TEE 的应用也在出现。一个例子是基于 Intel SGX 的联合 GWAS 服务，它安全地聚合来自多个站点的数据，并随着研究参与者的增加或减少而逐步更新统计数据（116）。还提出了增强 Intel SGX 中 GWAS 计算效率的数据草图技术（117）。考虑到其他基因组分析任务，Widanage 等人（118）在 Intel SGX 中展示了读取映射，并描述了将他们的工具概括化为其他工作流程。Dokmai 等人（113）提出了一种基于 TEE 的安全基因型插补服务，引入了在保持准确的插补性能的同时实现对侧信道攻击的弹性的技术。\n4.4. 差分隐私 DP 是隐私的数学定义，它通过确保数据集中移除或添加单个个体不会导致分析结果的可区分变化，从而提供严格的隐私保护（119,120）。形式上，给定ε ≥ 0，一个随机机制 A 满足ε-DP，如果对于所有相差一个记录的数据集 D1 和 D2，以及 A 的任何可能输出子集 O，我们有 P [A(D1) ∈ O] ≤ e^εP [A(D2) ∈ O]——直观地说，这意味着在相似的数据集之间，任何结果的可能性都是相似的。参数ε称为隐私预算，用于指定隐私保护的水平。DP 机制通常通过向数据添加噪声来满足隐私保证，其中较小的ε提供更多的隐私，以增加噪声的方式牺牲准确性。标准 DP 技术包括拉普拉斯、高斯和指数机制，代表不同的采样噪声分析结果的方法。\n已经开发了各种技术来最小化噪声添加，并在隐私和效用之间获得更可取的权衡。例如，一些 DP 公式放宽了隐私概念以获得更好的效用：(ε, δ)-DP，也称为近似 DP（120），要求以至少 1 − δ的概率满足ε-DP。集中 DP（121）、零集中 DP（122）和 Rényi DP（123）将隐私损失视为一个随机变量，并限制平均损失而不是最坏情况损失。\nDP 的关键属性包括后处理，它确保对满足 DP 的数据的进一步分析不会导致任何额外的隐私泄露，以及组合，它允许在相同数据上操作的多个机制结合起来提供联合 DP 保证。因此，向分析管道的不同组件添加 DP 噪声——例如，输入、输出、优化目标（124, 125）或梯度（126, 127）——可以根据分析任务对整体精度产生重大影响。影响噪声量的另一个关键因素是灵敏度，它测量由于数据中单个记录的变化而导致的分析输出的最大变化。已经提出了不同的方法来分析给定函数的灵敏度 [例如，全局、局部或平滑灵敏度（128）]。由于这些考虑，通常需要为特定应用程序精心设计 DP 机制以优化其性能。\n例如，在多方设置中，DP 可以由各个数据提供者本地实施，也可以由聚合分析结果的中央服务器全局实施；这些方法分别称为本地差分隐私（LDP）和集中差分隐私（CDP）。尽管 CDP 通常需要较少的噪声，通过直接向聚合数据添加噪声，但它可能更容易受到隐私泄露的影响，因为它依赖于受信任的第三方进行数据聚合。另一方面，LDP 在单个数据提供者级别提供 DP，同时增加了整体噪声量。实现 LDP 的常见数据扰动技术包括随机响应及其变体（129-131）。\n最近，各种实体已经部署了 DP 来解决私有统计数据的收集和发布隐私化数据集。Erlingsson 等人（132）的 RAPPOR 技术使用随机响应和布隆过滤器从 Chrome 浏览器私密地收集使用统计数据。Apple 已经部署了 LDP 来收集其设备上的 emoji 和搜索查询信息（133），Microsoft 也在 Windows 10 中用于应用程序级遥测（134）。2020 年，美国人口普查局使用 TopDown 算法（135）发布了带有 DP 的人口普查数据，该算法基于地理单位层次地聚合统计数据。\nDP 在生物医学应用中的关键重点之一是发布 GWAS 统计数据。Uhlerop 等人（136）引入了用于发布病例对照 GWAS 的最小等位基因频率和χ2 统计量的 DP 机制。这项工作后来由 Yu 等人（137, 138）扩展到处理更大的队列和逻辑回归。还提出了一种基于指数机制的替代方法（139）。Simmons \u0026amp; Berger（140）引入了一个优化框架，用于私密报告固定数量的最显著关联。在后续工作中，Simmons 等人（141）开发了具有人口分层校正的 GWAS 的 DP 方法。DP 的其他值得注意的应用包括共享基因型数据（142）、临床试验数据（143）和表格医疗记录（144）。DP 也已应用于交互式数据库设置，例如计数或成员资格查询（145, 146）和患者遗传匹配（147）。在公共卫生领域，DP 已被用于支持 COVID-19 实时信息系统的准备工作和流行病响应（148）以及冠状动脉心脏病的移动诊断系统（149）。\n尽管取得了这些进展，DP 的实际采用面临几个技术挑战。与 DP 方法相关的隐私参数（例如，ε）是控制隐私和效用之间权衡的重要因素；然而，没有严格的选择这些参数的可接受值的方法或标准，用于给定任务。由于生物医学数据通常是高维的，需要私密共享的统计数据数量很大。此外，这些数据通常使用许多步骤的复杂算法进行分析，其中可以整合 DP。因此，设计有效的 DP 机制，以最佳方式分配隐私预算可能是困难的。另一个限制是 DP 不能保护每个数据集；例如，小型数据集通常需要大量的噪声进行 DP，需要使用其他策略进行保护。\n4.5. 联邦学习 FL 允许多个参与方以分布式方式协作训练机器学习（ML）模型。参与方在训练期间共享模型参数或更新（例如，梯度），但不直接共享训练数据，从而减轻隐私风险。FL 用例的两个主要类别包括（a）跨数据孤岛，其中少数参与方持有大量数据，以及（b）跨设备，其中大量设备（可能数百万）持有少量数据（150）。前者更类似于传统的 MPC 设置，其中参与方可能代表不同机构，每个机构都从许多个体中收集数据，而后者通常在消费者应用中发现，例如，数百万移动电话可能收集个人用户数据。\n在 FL 中，每个参与方在评估和更新模型时仅限于其本地数据份额；因此，存在几种同步参与方模型状态的方法。联邦平均技术要求每个参与方本地计算模型更新，这些更新被发送到中央服务器进行平均，并全局应用（151）。用于平均这些更新的权重通常根据每个参与方的数据大小和质量选择（152, 153）。更高级的方法，如联邦匹配平均，通过匹配进行层级同步，以应对神经网络中的排列不变性（154）。其他方法避免全局同步，而是迭代地将权重从一个参与方传递到另一个参与方（155）。个性化 FL 是另一种方法，每个参与方学习一个不同的本地模型，该模型结合了来自其他参与方的信息和本地数据特征（156, 157）。\nFL 的鲁棒性在实践中是一个主要挑战。网络连接、通信限制和资源限制等问题可能会阻止某些参与方完全参与协议的每一轮（158, 159）。数据孤岛或设备之间的异质性也可能引入关于训练模型的公平性和泛化性的担忧；例如，简单的平均技术已被证明会导致小亚群组中的不准确结果（160-162）。\n另一个挑战是 FL 可能提供有限的隐私和安全保护。例如，通过检查其他参与方在协议多轮中的模型更新，一个医院可能能够推断出另一个医院患者的特征。这可能揭示有关临床标签分布、特征向量中个别坐标的信息，有时甚至可以揭示整个训练输入（164-166）。此外，恶意对手可能会操纵数据或模型以推进自己的目标，牺牲他人的利益（167）。\n最近关于 FL 的文献引入了广泛的技术来解决这些限制。将 FL 与 DP 结合可以提供有关隐私泄露的严格界限（168, 169），尽管在保持模型准确性的同时这样做可能是具有挑战性的。如果中央聚合器不受信任，参与方可以选择使用 LDP 向其本地梯度添加噪声，然后再聚合（170）。或者，MPC、HE 或 TEE 也可以支持安全聚合模型权重，以便除了聚合结果外不会泄露任何额外信息（163）。还提出了使用加密技术在整个训练过程中保护模型参数的解决方案（96-98）。鲁棒性通常通过根据每个参与方的质量调整协议来解决。例如，一些方法建议检测和删除异常值，从一组可靠的参与方中学习。其他人建议改变平均权重，以产生在每个参与方上表现相当好的更公平的全局模型（162）。尽管现有的 FL 应用主要集中在监督学习上，但最近的工作将 FL 扩展到解决其他 ML 任务，包括半监督、无监督和强化学习（171-173）。\nFL 已经触及了许多生物医学应用。在跨数据孤岛设置中，FL 可以通过结合更广泛的训练数据来改善不同阶段患者的分析和护理，从而提高 ML 模型的性能。值得注意的用途包括罕见疾病分析（174, 175）、多医院合作进行医学图像分析（176-178）以及从临床笔记中自动表型化和风险预测（179-181）。在跨设备设置中，FL 有潜力转变移动健康（182）。例如，FL 可以允许可穿戴设备，如 Fitbits 或 Apple Watches，随着时间的推移适应个人独特的健康和生活方式特征，如静息心率、每天的步数和血氧水平。这些模型可以为个人提供更准确的健康监测，例如步态识别和跌倒检测（183, 184）。\n5. 其他相关技术 涉及私有数据交换的几个工作流程已经引起了隐私和安全社区的特别关注，以开发超出第 4 节中描述的 PETs 范围的针对性方法。在这一节中，我们强调了一些这样的技术。\n5.1. 私密信息检索 在私密信息检索（PIR）中，客户端从一个存储在服务器上的数据库中检索特定的感兴趣项目，而不会透露访问项目的标识（185, 186）。下载整个数据库并本地查询的朴素方法对于大型数据集来说是不切实际的。在基于 HE 的解决方案中，客户端上传一个加密的查询，服务器同态地搜索数据库，然后返回结果供客户端解密。通过实际的基于格的 HE（第 4.2 节）和数据库预处理以及摊销技术（187, 188），最近的 PIR 协议已经显示出可以扩展到包含数十亿条目的数据库（189-192）。其他工作已经将 PIR 扩展到更复杂的查询，如在稀疏数据库中的关键词搜索（193, 194）和批量查询（195, 196）。在生物医学背景下，PIR 可以增强需要用户要么下载整个数据库，要么向服务器披露私有数据（例如，遗传突变或患者记录）才能查询数据库的公共数据资源的效用。例如，已经提出了用于外包基因组数据存储的 PIR 解决方案，这些解决方案支持安全检索感兴趣的变异（197, 198）。\n5.2. 私密集合交集 私密集合交集（PSI）解决了与 PIR 密切相关的问题，其中两个参与方，每个都持有一组项目，希望了解这两组之间的交集，而不向彼此透露任何其他信息。PSI-size 是 PSI 的一个显著变体，其中只揭示交集的大小。PSI 已经得到了广泛的研究，导致实际协议适用于数十亿项目，并提出了几种变体，涉及不同的信任假设、通信和计算之间的权衡，以及参与方的数量（199-201）。几项工作提出了用于计算基因组相似性的 PSI 协议，将每个基因组视为一组变异：Baldi 等人（202）引入了基于 PSI 技术的亲子鉴定（203, 204），Wang 等人（205）开发了一种基于 PSI 的协议，用于安全计算基因组之间的编辑距离。\n5.3. 零知识证明 在涉及敏感生物医学数据时，验证计算的正确性可能具有挑战性。零知识证明（ZKP）（206）是一种密码原语，与 MPC 和数字签名（56, 207）相关，允许在不披露敏感信息的情况下证明关于数据或计算的陈述的真实性。例如，Goldreich 等人（208）表明，通用 ZKP（209）可以用来证明安全计算协议（例如，MPC）在不解密任何中间值的情况下诚实地执行。尽管通用构造通常会产生不切实际的计算开销，但最近的进步已经提高了在各种安全和模型假设下 ZKP 的效率（210-213）。Froelicher 等人（214）展示了用于离散对数的 ZKP（215）可以确保分布式健康分析系统中某些 HE 计算的完整性。Chatel 等人（216）引入了一种基于 MPC-in-the-head 范式（217, 218）的 ZKP 方案，允许直接向消费者分析服务提供商验证用户上传的数据来自受信任的来源，从而防止恶意用户篡改分析结果。\n5.4. 区块链 区块链提供了一个去中心化的框架，用于安全地记录和验证分布式网络中的交易。它使用密码技术创建区块链，即时间戳交易列表，提供数据管理的透明度、不可变性和可问责性。除了在金融领域（例如，比特币）的众所周知的应用外，区块链在生物医学领域也变得越来越相关（219）。一个关键用例是创建一个安全且去中心化的健康信息交换，以改善各种参与者之间的医疗记录和保险索赔的管理（220）。它还可以用来创建一个数据共享平台，以支持生物医学研究，同时提供数据来源和可问责性（221）。通过区块链交换的数据的隐私保护是一个关键挑战，通常需要仔细结合其他加密技术或 PETs。正在进行的另一项研究重点是提高区块链网络的可扩展性和鲁棒性，这对于它们在大型机构网络中的部署是必要的。\n5.5. 合成数据生成 创建与真实数据相似但不直接链接到私有个体的合成数据已成为一种有用的隐私意识数据共享策略（222）。公共共享的合成数据可以支持合作努力，如数据分析竞赛和跨机构的计算模型验证。它还可以支持各种学术和教育活动，例如，通过创建用于培训或公共沟通的真实患者资料。随着 ML 的进步，特别是深度生成模型的发展，生成合成数据的技术也在不断发展。生成对抗网络和扩散模型的引入大大提高了各种类型生物医学数据的合成，包括医学图像（223, 224）和 EHR（225）。然而，合成数据可能泄露用于训练模型的原始数据的私有信息的可能性仍然是一个主要关注点（226）。最近的研究建议，现代生成模型的更大表现力实际上增加了私有训练数据被重建的可能性（227）。虽然将 DP 纳入模型训练可以帮助减轻这些风险（228），但它可能会降低生成数据的质量，特别是对于高维数据，如图像和基因组。未来在合成数据的质量和隐私方面的改进对于扩大它们在需要直接共享数据的环境中的使用至关重要。\n6. 开放性挑战和展望 随着生物医学数据科学的领域扩展到包含更多样化的数据模式、更复杂的统计模型和不断演变的计算环境，我们对隐私风险的理解也必须改变。揭示新兴数据类型（例如，转录组学（84, 229-231）、蛋白质组学（232）和可穿戴设备（233））和计算模型（例如，扩散和大型语言模型（227, 234））中的新隐私风险的研究将特别有价值。将这些发现整合到实际指导方针和政策中将需要深思熟虑地检查潜在对手不断演变的动机和能力（235-237）。\nPETs 的一个关键方面是它们提供的不同程度的隐私保护，以及它们如何与我们的社会价值观和实践者的需求保持一致。虽然承认提供最强大、最正式隐私概念的密码 PETs（即 HE、MPC 和 DP）的价值，我们还必须意识到这些技术在实际实施中的潜在陷阱，例如软件缺陷（238）或违反模型假设（239）。提供较少形式但更广泛适用的隐私增强技术（即 TEE 和 FL）可以在某些设置中作为有用的替代方案。一个有希望的未来方向是探索 PETs 的联合使用，以结合它们的优势，同时减轻它们的弱点，如在标题为“扩展同态加密到协作分析设置：多方同态加密”的侧边栏中所述。未来的政策和法规将在将基于 PETs 的新兴工具的复杂隐私属性转化为生物医学社区的具体指导方针中发挥关键作用。\nPETs 的社会影响涉及另一个关键考虑因素：公平性（240）。许多研究表明，计算工具在新兴的临床应用中存在不平等（241, 242）。解决这些问题需要更大的数据共享，以创建更多样化的数据集，这反过来又引入了新的隐私挑战（243, 244）。另一方面，那些最需要数据以改善生物医学公平性的人（例如，代表性不足的群体）在隐私泄露事件中可能遭受的损失最大（245）。此外，某些 PETs，如 DP，可能会不成比例地降低数据集中代表性有限的人群的 ML 模型的准确性（246）。在隐私和公平之间导航这种复杂的权衡仍然是一个重要的挑战。\n我们期望信任和透明度在将 PETs 与组织环境中利益相关者的利益一致方面发挥关键作用（247）。PETs 可以被视为加强利益相关者之间信任的工具，通过增加透明度和减轻合作伙伴关系中出现的各種隐私和安全风险。这与 PETs 社区通常关注防止恶意行为者破坏系统并获得对敏感数据的访问的焦点形成对比。将诸如信任和以人为中心的设计原则等情境价值整合到 PETs 中，可以促进创建更有效地解决生物医学社区需求的工具。\n随着 PETs 的不断成熟和更广泛的适用性，如本综述所示，将有越来越多的需求定制这些技术，以创建有效的算法和工具，解决多样化的生物医学工作流程。PET 开发者、生物医学从业者、政策制定者以及患者和研究参与者之间的更紧密合作，可以帮助优先考虑解决最紧迫挑战的努力。此外，旨在协助研究人员将 PETs 纳入其现有工作流程的软件开发和部署工具，可以帮助确保这些技术被广泛接受。基础技术的进步、有效的算法设计以及建立社会框架以保障这些技术的使用，将是解锁 PETs 在生物医学数据科学中潜力的关键。\n","date":"October 26, 2024","permalink":"/blog/posts/0001-privacy-enhancing-technologies-in-biomedical-data-science/","summary":"","title":"Privacy-Enhancing Technologies in Biomedical Data Science","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/22 前提 之前的附件配置是基于链接的形式，而附件是单独文件夹存储的。 新安装时，设备上已经移除了所有之前的设置（几之前的所有配置信息都被保存到了云端你自己的账号下）。 设备上只剩下了之前的pdf附件文件夹和其中的pdf附件了。 配置过程 安装最新版的Zotero7的安装包：https://www.zotero.org/download/ 安装后到设置中配置： 先到 同步 选项卡中登录你的Zotero账号密码，将基础的设置同步下来。 再配置 高级 选项卡中的已链接附件的根目录到你的附件文件夹目录。 重启Zotero即可。 常规情况下到这步应该就没问题了。 后续配置 手动安装插件市场：https://github.com/syt2/zotero-addons 安装好后从插件市场中安装附件管理器插件：Zotero Attanger 对附件管理器插件中配置新的附件采用链接形式，配置源路径、附加类型（链接）和靶路径等以适配你现有附件的存储格式。 安装其他的插件，并进行相关的配置。 ","date":"September 1, 2024","permalink":"/blog/posts/0002-zotero7%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE/","summary":"","title":"【Zotero7】重新安装与数据配置","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/21 原始文档：https://code.visualstudio.com/docs/editor/portable#_migrate-to-portable-mode\nVisual Studio Code 支持便携模式 。此模式使 VS Code 创建和维护的所有数据都位于自身附近，因此可以在环境中移动。\n此模式还提供了一种设置 VS Code 扩展的安装文件夹位置的方法，这对于阻止在 Windows AppData 文件夹中安装扩展的企业环境非常有用。\n便携模式支持 Windows 的 ZIP 下载，Linux 的 TAR.GZ 下载，以及 macOS 的常规应用程序下载。请参阅下载页面 以查找适合您平台的正确文件。\n[!note] 请勿尝试在从 Windows User or System installers 进行安装时配置便携模式。便携模式仅在 Windows ZIP 存档上受支持。另请注意，Windows ZIP压缩包不支持自动更新。\n启用便携模式 Windows、Linux 解压缩 VS Code 下载后，在 VS Code 的文件夹中创建一个文件夹：data\n|- VSCode-win32-x64-1.84.2\r| |- Code.exe (or code executable)\r| |- data\r| |- bin\r| | |- code\r| | |- ...\r| |- ... 从那时起，该文件夹将用于包含所有 VS Code 数据，包括会话状态、首选项、扩展等。\n[!note] 该文件夹将覆盖 --user-data-dir 和 --extensions-dir 这两个命令行 选项。\ndata 文件夹可以移动到其他 VS Code 安装中。这对于更新可移植的 VS Code 版本非常有用，在这种情况下，您可以将文件夹移动到较新的 VS Code 提取版本。\nmacOS 在 macOS 上，您需要将数据文件夹作为应用程序本身的同级文件夹。由于该文件夹将与应用程序并列，因此您需要专门命名它，以便 VS Code 可以找到它。默认文件夹名称为：code-portable-data。\n|- Visual Studio Code.app\r|- code-portable-data 如果您的应用程序处于隔离状态 ，则便携模式将不起作用，如果您刚刚下载了 VS Code，则默认情况下会发生这种情况。如果便携模式似乎不起作用，请确保删除隔离属性：\nxattr -dr com.apple.quarantine Visual\\ Studio\\ Code.app [!note] 在预览体验成员上，文件夹应命名为 code-insiders-portable-data。\n更新便携式 VS Code 在 Windows 和 Linux 上，可以通过将 data 文件夹复制到 VS Code 的最新版本来更新 VS Code。\n在 macOS 上，自动更新应一如既往地工作，无需额外工作。\n迁移到便携模式 您还可以将现有安装迁移到便携模式。\nWindows、Linux 下载适用于您的平台的 VS Code （或 VS Code Insiders ）ZIP 分发。\n如上所述创建 data 文件夹。\n将用户数据目录 Code 复制到 data 中，并重命名为 user-data。\nWindows %APPDATA%\\Code Linux $HOME/.config/Code 将扩展目录复制到 data。\nWindows %USERPROFILE%\\.vscode\\extensions Linux ~/.vscode/extensions 例如，以下是 Windows 上的预期结果：\n|- VSCode-win32-x64-1.84.2\r| |- Code.exe (or code executable)\r| |- data\r| | |- user-data\r| | | |- ...\r| | |- extensions\r| | | |- ...\r| |- ... macOS 下载适用于 macOS 的 VS Code （或 VS Code Insiders ）。\n如上所述创建文件夹 code-portable-data。 将用户数据目录 $HOME/Library/Application Support/Code 复制到 code-portable-data 并重命名为 user-data。 将扩展目录 ~/.vscode/extensions 复制到 code-portable-data\r。 TMP 目录 默认情况下，即使在便携模式下，默认的 TMP 目录仍然是系统目录，因为那里没有保留任何状态。如果您还希望将 TMP 目录放在可移植目录中，则可以在 data 文件夹内创建一个空目录 tmp。只要目录存在，它就会用于 TMP 数据。\n","date":"July 30, 2024","permalink":"/blog/posts/0003-%E8%AF%91%E4%BD%BF%E7%94%A8%E4%BE%BF%E6%90%BA%E6%A8%A1%E5%BC%8F%E7%9A%84vscode/","summary":"","title":"【译】使用便携模式的VSCode","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/20 [!IMPORTANT] 原文：https://builtin.com/machine-learning/ensemble-model 有时，一个模型是不够的。在这篇集成模型指南中，我将向您介绍如何（以及何时）对机器学习模型使用集成技术。\n每当我们试图做出重要决定时，我们都会尝试收集尽可能多的信息，并向专家寻求建议。我们可以收集的信息越多，我们（和我们周围的人）就越信任决策过程。\n机器学习预测遵循类似的行为。模型处理给定的输入并产生结果。结果是根据模型在训练过程中看到的模式进行预测。\n在许多情况下，一个模型是不够的，本文阐明了这一点。我们何时以及为何需要多个模型？我们如何训练这些模型？这些模型应该提供什么样的多样性？所以，让我们直接进入。\n如果你想看一个如何构建集成模型的例子，你可以跳到最后！\n[!NOTE] 什么是集成模型？\n集成模型是一种机器学习方法，用于在预测过程中结合多个其他模型。这些模型称为基础估计器。集成模型提供了一种解决方案，可以克服构建单个估计器的技术挑战。\n构建单一估计器的技术挑战包括：\n高方差：模型对为学习特征提供的输入非常敏感。 准确性低：一个模型（或一种算法）无法拟合整个训练数据，可能无法为您提供项目所需的细微差别。 具有噪声和偏差的特征：模型在进行预测时严重依赖于太少的特征。 集成算法 单一算法可能无法对给定的数据集做出完美的预测。机器学习算法有其局限性，生成高精度的模型具有挑战性。如果我们构建并组合多个模型，我们就有机会提高整体准确性。然后，我们通过将每个模型的输出与两个目标聚合来实现模型组合：\n减少模型误差 保持模型的泛化 您可以使用不同的技术（有时称为元算法）来实现此类聚合。\n图 1：使用多种算法使模型预测多样化\n集成学习 当我们构建集成模型时，我们不仅要关注算法的方差。例如，我们可以构建多个 C45 模型，其中每个模型都在学习专门用于预测任何给定事物的特定模式。我们可以用来获得元模型的模型称为弱学习器。在这种集成学习架构中，输入被传递给每个弱学习器，同时还收集他们的预测。我们可以使用组合预测来构建最终的集成模型。\n值得一提的是，弱学习者可以采用不同的方式将特征与变体决策边界进行映射。\n图 2：使用同一算法的多个弱学习器的聚合预测\n[!NOTE] 集成建模技术的类型\nBagging Boosting Stacking Blending 集成技术 Bagging bagging 的想法是基于使训练数据可用于迭代学习过程。每个模型都使用训练数据集的略有不同的子集来学习前一个模型产生的误差。bagging 可减少 方差 并最大程度地减少 过拟合 。这种技术的一个例子是 随机森林 算法。\nBootstrap Aggregation (Bagging) 此技术基于自举采样技术 (bootstrapping sampling technique)。bootstrapping 会创建多组原始训练数据并进行替换。替换允许在一组中复制样本实例。每个子集都具有相同的相等大小，可用于并行训练模型。\n图 3：通过结合来自多个模型的预测来做出最终预测的 Bagging 技术\n随机森林 (Random Forest) 此技术使用训练样本的子集以及特征的子集来构建多个分割树。构建了多个决策树以适应每个训练集。样本/特征的分布通常以随机模式实现。\nExtra-Trees Ensemble 这是另一种集成技术，其中来自许多 决策树 的预测组合在一起。与随机森林类似，它结合了大量的决策树。但是，额外的树在随机选择分割时使用整个样本。\n[!TIP] 相关阅读:\n在 Python 中实现随机森林回归：简介 深入了解在 Python 中实现随机森林分类 Boosting Adaptive Boosting (AdaBoost) 这是一个算法集合，我们在 几个弱学习器 的顶部构建模型。正如我们前面提到的，这些学习器被称为弱学习器，因为它们通常很简单，预测能力有限。AdaBoost 的适应能力使该技术成为最早成功的二元分类器之一。\n顺序决策树 (Sequential Decision Trees) 这些是这种适应性的核心，每棵树都根据先验的精度知识调整其权重。因此，我们以顺序（而不是并行）过程执行这种技术的训练。在这种技术中，对于给定的迭代次数或当错误率没有显着变化时，可以重复训练和测量估计误差的过程。\n图 4：AdaBoost 的顺序学习产生更强的学习模型\n梯度提升 (Gradient Boosting) 梯度提升算法是具有高预测性能的出色技术。 Xgboost 、LightGBM 和 CatBoost 是可用于回归和分类问题的常用提升算法。在他们证明有能力 赢得一些 Kaggle 比赛 后，他们的受欢迎程度显着增加。\nStacking 堆叠类似于提升模型; 它们产生更稳健的预测因子。堆叠是一个学习如何从所有弱学习者的预测中创建这样一个更强大的模型的过程。\n图 5：用于在集成架构中进行最终预测的堆叠技术\n请注意，该算法正在从每个模型（作为特征）中学习预测。\nEnsemble Learners Tutorial | Video: Udacity Blending blending 类似于 stacking 方法，不同之处在于最终模型正在学习验证和测试数据集以及预测。因此，所使用的特征被扩展为包括验证集。\n分类问题 分类只是一个分类过程。如果我们有多个标签，我们需要决定：我们是否应该构建一个单一的多标签分类器？或者我们是否应该构建多个二元分类器？如果我们决定构建多个二元分类器，我们需要解释每个模型预测。例如，如果我们想要识别四个对象，每个模型都会告诉您输入数据是否是该类别的成员。因此，每个模型都提供了隶属的概率。同样，我们可以构建一个结合这些分类器的最终集成模型。\n回归问题 在上一个函数中，我们使用所得概率确定了最佳拟合隶属度。在回归问题中，我们不是在处理“是”或“否”的问题。相反，我们需要找到最佳预测的数值，然后我们可以对收集的预测值进行平均。\n聚合预测 当我们集成多个算法以调整预测过程以组合多个模型时，我们需要一种聚合方法。我们可以使用三种主要技术：\n最大投票：此技术的最终预测是基于对分类问题的多数投票做出的。 平均：此技术通常用于对预测进行平均的回归问题。我们也可以使用概率，例如，通过对最终分类进行平均。 加权平均：有时，在生成最终预测时，我们需要为某些模型/算法赋予权重。 如何构建集成模型：示例 我们将使用以下示例来说明如何构建集成模型。我们将使用泰坦尼克号数据集，并尝试使用不同的技术预测泰坦尼克号的生存情况。图六和图七显示了数据集的样本和目标列对乘客年龄的分布。如果你想跟着做，你可以在我的 GitHub 上找到示例代码。\n泰坦尼克号数据集是需要大量特征工程的分类问题之一。我们将尝试只关注模型构建以及如何将集成模型应用于此用例。\n我们将使用不同的算法和技术; 因此，我们将创建一个模型对象来提高代码的可重用性。\n# Model Class to be used for different ML algorithms class ClassifierModel(object): def __init__(self, clf, params=None): self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): return self.clf.fit(x,y).feature_importances_ def predict(self, x): return self.clf.predict(x) def trainModel(model, x_train, y_train, x_test, n_folds, seed): cv = KFold(n_splits= n_folds, random_state=seed) scores = cross_val_score(model.clf, x_train, y_train, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores Random Forest Classifier # Random Forest parameters rf_params = { \u0026#39;n_estimators\u0026#39;: 400, \u0026#39;max_depth\u0026#39;: 5, \u0026#39;min_samples_leaf\u0026#39;: 3, \u0026#39;max_features\u0026#39; : \u0026#39;sqrt\u0026#39;, } rfc_model = ClassifierModel(clf=RandomForestClassifier, params=rf_params) rfc_scores = trainModel(rfc_model,x_train, y_train, x_test, 5, 0) Extra Trees Classifier # Extra Trees Parameters et_params = { \u0026#39;n_jobs\u0026#39;: -1, \u0026#39;n_estimators\u0026#39;:400, \u0026#39;max_depth\u0026#39;: 5, \u0026#39;min_samples_leaf\u0026#39;: 2, } etc_model = ClassifierModel(clf=ExtraTreesClassifier, params=et_params) etc_scores = trainModel(etc_model,x_train, y_train, x_test, 5, 0) AdaBoost Classifier # AdaBoost parameters ada_params = { \u0026#39;n_estimators\u0026#39;: 400, \u0026#39;learning_rate\u0026#39; : 0.65 } ada_model = ClassifierModel(clf=AdaBoostClassifier, params=ada_params) ada_scores = trainModel(ada_model,x_train, y_train, x_test, 5, 0) XGBoost Classifier Gradient Boosting parameters # Gradient Boosting parameters gb_params = { \u0026#39;n_estimators\u0026#39;: 400, \u0026#39;max_depth\u0026#39;: 6, } gbc_model = ClassifierModel(clf=GradientBoostingClassifier, params=gb_params) gbc_scores = trainModel(gbc_model,x_train, y_train, x_test, 5, 0) 让我们将所有模型交叉验证精度结合在五个折上。\n现在，让我们构建一个堆叠模型，其中一个新的更强的模型从所有这些弱学习器中学习预测。我们用于训练先前模型的标签向量将保持不变。特征是从每个分类器收集的预测。\nx_train = np.column_stack((etc_train_pred, rfc_train_pred, ada_train_pred, gbc_train_pred, svc_train_pred)) 现在，让我们看看构建 XGBoost 模型仅学习生成的预测是否会表现得更好。但首先，我们将快速了解分类器预测之间的相关性。\n图 9：分类器投票标签之间的 Pearson 相关性\n现在，我们将构建一个模型来组合来自多个贡献分类器的预测。\ndef trainStackModel(x_train, y_train, x_test, n_folds, seed): cv = KFold(n_splits=n_folds, random_state=seed) gbm = xgb.XGBClassifier( n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective=\u0026#39;binary:logistic\u0026#39;, scale_pos_weight=1).fit(x_train, y_train) scores = cross_val_score(gbm, x_train, y_train, scoring=\u0026#39;accuracy\u0026#39;, cv=cv) return scores 之前创建的基础分类器表示 Level-0 模型，新的 XGBoost 模型表示 Level-1 模型。该组合说明了在样本数据的预测上训练的元模型。您可以在下面看到新堆叠模型的准确性与基础分类器之间的快速比较：\n仔细考虑 噪声、偏差和方差 ：来自多个模型的决策组合有助于提高整体性能。因此，使用集成模型的关键原因之一是克服噪声、偏差和方差。如果集成模型没有提供集体经验来提高这种情况下的准确性，那么有必要仔细重新考虑是否有必要使用。 简单性和可 解释 性：机器学习模型，尤其是那些投入生产环境的模型，应该易于解释。当您使用集成模型时，您能够解释最终模型决策的机会会大大降低。 泛化：有许多说法认为集成模型具有更强的泛化能力，但其他报告的用例显示出更多的泛化错误。因此，如果没有仔细的训练过程，集成模型可能会快速产生高过拟合模型。 推理时间：尽管我们可能不愿意接受更长的模型训练时间，但推理时间仍然很关键。将集成模型部署到生产环境中时，传递多个模型所需的时间会增加，并且可能会减慢预测任务的吞吐量。 集成模型是机器学习的一种极好的方法，因为它们为分类和回归问题提供了各种技术。现在您了解了集成模型的类型，我们如何构建简单的集成模型，以及它们如何提高模型的准确性。\n","date":"July 25, 2024","permalink":"/blog/posts/0004-%E8%AF%91%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%83%E4%BB%AC%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%BD%95%E6%97%B6%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC/","summary":"","title":"【译】集成模型：它们是什么以及何时应该使用它们？","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/19 Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery - An Experimental Study 论文：https://arxiv.org/pdf/2405.08493 原始文档：https://github.com/lartpang/blog/issues/19 深度学习方法，尤其是卷积神经网络 (CNN) 和视觉变换器 (ViT)，经常用于执行高分辨率遥感图像的语义分割。然而，cnn 受到其有限的接受领域的限制，而 vit 由于其二次复杂性而面临挑战。最近，具有线性复杂性和全局感受野的 mamba 模型在视觉任务中获得了广泛的关注。在此类任务中，需要对图像进行序列化以形成与 mamba 模型兼容的序列。许多研究工作已经探索了扫描策略来序列化图像，旨在增强 Mamba 模型对图像的理解。然而，这些扫描策略的有效性仍然不确定。\n在这项研究中，我们对主流扫描方向及其组合对遥感图像语义分割的影响进行了全面的实验研究。通过在 LoveDA，ISPRS Potsdam 和 ISPRS Vaihingen 数据集上进行的广泛实验，我们证明，**无论其复杂性或所涉及的扫描方向数量如何，都没有单一的扫描策略能胜过其他扫描策略。简单的单个扫描方向被认为足以对高分辨率遥感图像进行语义分割。**还建议了未来研究的相关方向。\n这份工作主要做了两点内容：\n总结了现有视觉 mamba 中常用的 12 种独立的序列化扫描方向，并实验了 22 中不同策略（12 个独立的和 10 个组合变体）。 第一次基于特定设计的实验架构，在三个数据集上定量对比探究了视觉 mamba 不同扫描策略对遥感图像的语义分割准确性的影响。 展示了现有方法的几种不同策略\n**这些努力都是基于这样的假设，即图像补丁的不同扫描方向可以潜在地增强 Mamba 对图像的理解。**但是，在他们的工作中，缺乏对不同扫描方向下的模型性能进行全面和定量的比较。\nVim 和 PlainMamba 缺乏必要的消融研究来验证其扫描方法。 在 VMamba 中，水平扫描组合的结果 (即，D1 和 D2) 没有报告，而四方向扫描 (即，与单向扫描 D1 相比，D1、D2、D3 和 D4) 在 ImageNet 上仅实现了 0.3% 高的精度。考虑到训练过程中模型性能的可能波动，这种边际改进不足以证实多向扫描的有效性。 而在分割任务的研究中，一些工作考虑了不同的扫描策略来测试它们对 mamba 图像理解能力的影响。\nU-Mamba 代表了将 Mamba 与 UNet 架构合并以进行医学图像语义分割的首次尝试。但是，由于其简单的建筑设计，其性能不及当时最先进的分割方法。 随后，出现了几种增强的方法使用 Vim 的双向扫描和/或使用 VMamba 的四向扫描。 在遥感领域，Samba 是第一个将 Mamba 引入遥感图像语义分割的研究，其中图像补丁以与 ViT 相同的方式进行展平，如图 2(a)。 后来，RS3Mamba 使用 VMamba 的四向扫描方法，构造了一个用于语义分割的辅助编码器。 同样，RSMamba 在的编码器 - 解码器体系结构中添加四个额外的对角线方向 (即，D5，D6，D7 和 D8) 。 实验设置 实验所使用的架构形式\n图像划分为 patch，序列送入四个 VMS（Vision Mamba Scan）模块顺次下采样。并使用 UperNet 作为解码器预测分割结果。这里为了使用单个模型兼容所有的扫描组合形式，当所考虑的扫描方向的数目为 1、2 或 4 时，扫描方向分别重复 8、4 和 2 次，以填充八个潜在的扫描方向。（这个重复是否会对最终的性能有影响尚未可知）\n三个数据集的训练设置\nRandom resize, random crop, random flip, and photometric distortion are consistently applied for data augmentation in our experiments. Experiments are performed using two RTX 4090D GPUs. 对 patch 和 stride 的实验 **用于图像裁剪的补丁大小和扫描过程中使用的步幅也会影响实验结果。**目前，大多数基于 mamba 的视觉任务都采用 4×4 大小的 patch 和 4 的步幅。为了探索最适合后续实验的补丁大小和步幅，本研究在三个数据集上进行了各种变体的消融实验。为了进行一致的比较，在消融实验中使用了 D1 扫描方向。\n**步幅也起着至关重要的作用，因为它会影响序列的长度和计算负荷。**这里使用 FLOPs 量化计算负载，这是使用一个随机生成的 512×512 图像作为输入来计算的。加倍步幅将需要计算的像素数减少了四倍，从而使计算减少了大约四倍。\n考虑到实用性，实验中最小步幅设为 4，较小步幅需要过高的计算资源，因此在两个 24G GPU 上进行训练是不切实际的。实验中考虑的 patch 大小包括 4×4、8×8、16×16 和 32×32，每个配对的步幅与相应的 patch 大小相同，用于分割整个图像。此外，还考虑了小于 patch 尺寸的步幅，这可以允许图像的重叠扫描。\n表 II 列出了由各种修补尺寸和步幅组合的分割精度。从三个数据集的性能分析中可以看到一致的发现：当处理 512×512 的图像输入尺寸时，步幅为 4 的 4×4 补丁大小可产生所有三个数据集的最高分割精度。当将步幅依次减小到 4 并保持固定的 patch 大小，mamba 在处理这些长序列时没有出现瓶颈，这表明它有潜力有效处理更小的步幅。因此，mamba 在处理较小步幅的长序列方面显示出希望。固定步幅后，减小 patch 大小可改善分割性能，这表明 mamba 架构更擅长处理更精细的图像补丁。基于这些发现，在随后的实验中始终使用 4x4 的 patch 大小和 4 的步幅 (即，实验 1- 实验 22)。\n对扫描策略的实验 实验中对应的不同扫描组合形式\n在实验中使用的所有三个数据集上观察到一个有趣的现象。22 种扫描策略产生的分割精度似乎相似。考虑到每个数据集内不同扫描策略之间的小性能差异，以及单个扫描策略在所有三个数据集中的性能差异，没有明显迹象表明任何特定的扫描策略都优于其他方法，无论它们的复杂性或涉及单个或多个扫描方向。观察到的任何轻微的性能波动都可能归因于训练过程的随机性。\n总结 对于高分辨率遥感图像的语义分割，利用特定的扫描方向或不同扫描方向的组合，如现有的基于 mamba 的方法所提出的策略，并不能有效提高分割精度。因此，在 vision mamba 框架中，使用类似 ViT 的扁平化方法 (即，D1 扫描) 对于此类图像的语义分割仍然有效。此外，采用单向扫描策略 (例如 D1) 也降低了计算需求，从而允许在有限的计算资源内进行更深的网络堆叠。\n探索语言模型在视觉任务上的泛化能力对于深度学习的发展至关重要。基于循环模型的最新进展 [Mamba, RWKV] 中，对将其集成到视觉任务中的有效方法进行着持续探索。当前的工作重点是设计扫描图像 patch 的策略，以增强模型对图像序列的理解。但是本文中对远程感知图像的语义分割的调查表明，基于 mamba 的模型对不同的扫描策略并不敏感。因此，可以说，应该将努力放到探索更有效的方式，而不是探索不同的扫描策略，以增强 mamba 模型对远程感知图像的理解。\n我们的工作并不是要否定 Vision Mamba 为改进扫描策略所做的大量努力，而是要证明这些改进对遥感图像的语义分割效果有限。这种现象是可以解释的：遥感图像在特征方面与传统图像不同。\n一方面，与人物图片等传统图像相比，遥感图像中代表相同语义的斑块在转换成序列时差异很小。 另一方面，其序列的因果联系也弱于传统图像。 不过，在其他类型的数据集（如 COCO-Stuff 和 Cityscapes）中，不同的扫描策略在序列中的因果关系更为明显，其有效性还有待验证，这也是未来工作的一个有趣领域。\n在对不同 patch 方法的实验中，我们发现了一个有趣的现象：减小步长可以提高分割精度，但代价是增加了计算需求。这表明，在处理 512×512 像素的图像时，Mamba 在使用比实验中使用的最小步长 4 更小的步长时可能会有更好的表现。但是，由于计算资源有限，较小的步长会使序列长度呈指数增长，因此无法使用这些较小的步长进行实验。研究更高效的计算方法以适应更密集的扫描是未来研究的一个有意义的方向。\n","date":"May 17, 2024","permalink":"/blog/posts/0005-arxiv-2405---rethinking-scanning-strategies-with-vision-mamba-in-semantic-segmentation-of-remote-sensing-imagery---an-experimental-study/","summary":"","title":"ArXiv 2405 - Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery - An Experimental Study","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/18 FasterViT: Fast Vision Transformers with Hierarchical Attention 论文：https://arxiv.org/abs//2306.06189 代码：https://github.com/NVlabs/FasterViT 解读：https://mp.weixin.qq.com/s/0AXri6EUrXd6Hwf2HU5Wmg 原始文档：https://github.com/lartpang/blog/issues/18 本文提出了一种 CNN 和 ViT 的混合架构，即 FasterViT。这样的混合架构可以快速生成高质量 token，然后基于 Transformer 块来进一步处理这些 token。其重点在于结合架构组合和高效的注意力模块的设计，从而优化 ViT 模型的计算效率，提高图像的吞吐率，加强对于高分辨率图像的适应能力。\n模型设计 关于模型设计，作者写了一段具有参考价值的经验性分析，并在这些见解的指导下设计了所提架构，所有阶段均可从加速计算硬件受益。\n我们专注于 GPU 等主流现成硬件上的计算机视觉任务的最高吞吐量这在并行计算方面表现出色。在这种情况下，计算涉及一组流式多处理器（SM），以 CUDA 和 Tensor cores 作为计算单元。它需要频繁的数据传输来计算，并可能受到数据移动带宽的影响。因此，受计算限制的操作是计算受限的，而那些受内存传输限制的操作是内存受限的。需要两者之间仔细平衡才能最大化吞吐量。\n在分层视觉模型中，中间表示的空间维度随着推理过程而缩小。\n初始网络层大多具有较大的空间维度和较少的通道（例如 112x112x64)，这使它们是内存受限的。这使得这里更适合计算密集型操作，例如密集卷积而不是会增加额外传输成本的深度/稀疏卷积。也不可用矩阵操作形式表示的操作，例如非线性操作，池化，批归一化。这类操作也是内存受限的，应减少使用。 相反，后面层的操作往往是计算受限的，且运算成本高昂。例如，分层 CNN 具有尺寸为 14x14 的特征图和高维卷积核。这为更具表达性的操作留下了空间，例如层归一化、SE 模块，或注意力，并对吞吐量影响相当小。 stem 层：输入图像利用两个连续的跨步为 2 的 3x3 卷积转换为重叠的 patch。每个卷积层之后 BN 和 ReLU 被插入处理 token。 下采样：遵循封层结构，每个 stage 下采样 1/2。应用 2D LN 并跟一个跨步为 2 的 3xe 卷积来降低特征分辨率以及加倍通道数。 Conv Block：在高分辨率 stage（即 stage 1、2）使用残差卷积块。x = x + BN(Conv3x3(GELU(BN(Conv3x3(x)))))。 Hierarchical Attention：在低分辨率 stage（即 stage 3、4）使用 Transformer 块。对于每个 Transformer 块，论文提出分层注意力块来提取长短距离空间关系，进行有效的跨窗口交互。 分层注意力 HAT 这个模块使用的是粗细 token 组合的方式进行的设计。核心即为每个局部窗口学习专用的 carrier tokens参与到窗口内部和跨窗口的信息交互，基于 carrier token 对窗口之间的交互模式进行建模。由于其通过组合有固定窗口的注局部意力和随区域数量增加而线性增加的窗口 carrier tokens，分层注意力的计算复杂度几乎随输入图像分辨率线性增长。因此，它是捕获高分辨率特征的远距离关系的高效且有效的方法。\n整体模块细节图如图 4 所示。在原始的局部注意力之上额外引入了与每个窗口相对应的 carrier token（CT）。这些 CT 用于总结对应的局部窗口。具体流程如下：\n从原始的特征图中初始化 CT。这些 token 可以看做是一种更高阶的 token。初始化过程使用 卷积位置编码-\u0026gt;平均池化 的组合缩小特征空间维度。每个窗口对应生成 $L$ 个 CT（$L \u0026laquo; k$，k 是窗口的边长），共计 $n^2$ 个窗口，所以一共有 $n^2L$ 个 CT。 CT 通过独立的序列自注意力块（$x2=x1+\\gamma_2(MLP(LN(x1))), x1=x+\\gamma_1 MHSA(LN(x))$，$\\gamma$ 是一个可学习的逐通道放缩量）总结并传播全局信息。这里由于注意力自身对位置的不敏感，所以需要引入位置编码。这里受 SwinV2 启发，利用 2 层的 MLP 嵌入 token 的绝对位置信息到特征维度上加到 CT 上。这里也对图像 token 补充了绝对位置信息。 局部窗口的 token 与对应的 CT 拼接后形成新的序列，计算窗口内部的序列自注意力块（与前面的类似）。实现低成本的局部和全局的信息传播。同时，为了促进图像局部归纳偏置，这里也在注意力操作中引入了来自于 SwinV2 的基于 2 层 MLP 的对数空间相对位置编码。这确保了 token 的相对位置贡献到共享的注意力模式上。这种方法在图像大小（如位置）方面具有灵活性，因为位置编码被由 MLP 插值，因此训练好的模型可以应用于任何输入分辨率。 （CT 自己的全局交互和窗口内部的交互两部分构成了分层注意力） 之后将更新后的序列中局部 token 和 CT 进行拆分，并将 CT 上采样到原始分辨率，同时将图像 token 重组会原始形式。之后二者加和获得更新后的特征图。 HAT 整体计算复杂度为 $k^2H^2d+LH^2d+\\frac{H^4}{k^4}L^2d$ 实验性能 这里是在 ImageNet1K 上初始训练 256 大小（I）300 个 epoch，后面测试了使用不同的尺寸和窗口大小（W）进行微调的结果。\n","date":"May 17, 2024","permalink":"/blog/posts/0006-iclr-2024---fastervit---fast-vision-transformers-with-hierarchical-attention/","summary":"","title":"ICLR 2024 - FasterViT - Fast Vision Transformers with Hierarchical Attention","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/17 【获得邀请函】在 CVPR 官网参会注册后即可申请：https://cvpr.thecvf.com 【申请美签】填写 DS-160：https://ceac.state.gov/genniv 需要准备电子照片，相关要求可见：https://travel.state.gov/content/travel/en/us-visas/visa-information-resources/photos.html 美国签证DS160照片和要携带面试上交的照片必须要同一张吗？ 考虑选择申请 B1 签证。 Passport Book Number（似乎可以不填）：办理美国签证护照本编号是什么 邀请函中提供了填写 DS-160 需要的美国当地的联系人信息。 中文姓名电码查询，注意电码必须四位，如果连续的多个，可以用空格隔开：https://apps.chasedream.com/chinese-commercial-code 会议地址的信息可以从会议中心的官网上找到：https://seattleconventioncenter.com 过程中会用到很多信息和选项，可以参考这些帖子： 各种填写细节： 美国签证办理全流程攻略（上） - 火星大章鱼的文章 - 知乎 赴美签证DS160表格填写说明 如何填写美国B1-B2申请表（DS-160） 美国签证DS160表格填写样本 关于社交账号：如何回答DS-160表格上“你是否有社交网络账号”的问题 【申请面签】页面为 https://www.ustraveldocs.com/cn/zh/nonimmigrant-visa 关于 B1 签证的一些要求：https://www.ustraveldocs.com/cn/zh/business-visa 关于费用支付的一些信息：https://www.ustraveldocs.com/cn/zh/payment-options 【注意事项】 费用支付后，记住支付码，后面取消预约后还可以在新的预约中使用这个码。 预约系统有防止滥用预约的机制，尽量不要反复操作，容易被封号72小时。如果被封了，期间不要再登录系统，容易继续延长。 【面签过程】 美国签证被CHECK怎么办？一定会被拒吗？哪种情况会导致CHECK? 准备好DS-160的确认页和最终预约确认页一起打印出来，面试需要。 到日期准备参加面试吧。建议面试前多看看小红书上关于不同地区签证处的面试情况。提前做好心理准备。 【面前后续】面签之后如果没有当场通过或者被拒绝，那可能就要收走材料面临行政审查了。可以在 https://ceac.state.gov/CEACStatTracker/Status.aspx 中检索你的当前状态。 实锤！美国签证查询系统更新，行政审查的“Refused”不是最终裁决！ - 鱼总美签的文章 - 知乎 等了一个月，终于从refused变为了issued。护照寄出来，但是还没收掉。感觉时间可能不太行了。 一些建议的材料（有点旧了） ：\n","date":"April 17, 2024","permalink":"/blog/posts/0007-cvpr-%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/","summary":"","title":"CVPR 参会记录","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/16 CVPR 2024 - SED - A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation 论文：https://arxiv.org/abs/2311.15537 代码：https://github.com/xb534/SED 这篇文章提出了一种名为 SED 的简单编码器解码器，用于结合 CLIP 的 open-vocabulary 能力实现了开放词汇语义分割。在多个语义分割数据集上的实验证明了 SED 在开放词汇准确性和效率方面的优势。当使用 ConvNeXt-B 时，SED 在 ADE20K 上的 mIoU 得分为 31.6%，并且在单个 A6000 上每张图像只需 82 毫秒。\n[!note] 本文的方法受启发于最近的 CAT-Seg（通过 cost map 微调图像编码器没有损坏 CLIP 的 open-vocabulary 能力），主要差异包括三点：\n本文是一个不需要额外视觉 encoder 的更加简单的框架，同时具有更好的性能和更快的推理速度。 本文利用分层图像编码器生成 cost map 并且执行跳层融合，这显著提升了性能，并且计算成本与图像尺寸呈线性。 本文在解码器中引入了一个简单的大核操作，并逐步融合特征，同时设计了一个 category early rejection strategy 来加速推理同时不损害性能。 模型细节 SED 方法包括一个 hierarchical encoder-based cost map generation 和一个带有 category early rejection strategy 的 gradual fusion decoder。\nGradual Fusion Decoder Category Early Rejection hierarchical encoder：不使用普通的直筒型 ViT 视觉编码器，而是基于分层的 ConvNeXt 视觉编码器，从而帮助更好地捕捉不同层次的空间信息，增强局部性，并且与输入大小成线性复杂度。利用其可以获得多层级特征图 $F_2, F_3, F_4, F_5$。其中的 $F_5$ 利用最后的 MLP 层转换获得图像嵌入 $F_v \\in \\mathbb{R}^{H_v \\times W_v \\times D_t}$，结合文本嵌入集合（对应 $N$ 类，每类 $P$ 个模板，设计与 CAT-Seg 一致） $E \\in \\mathbb{R}^{N \\times P \\times D_t}$ 获得像素级的 4D image-text cost map $F_{cv} \\in \\mathbb{R}^{H_v \\times W_v \\times N \\times P}$ 。 gradual fusion decoder：解码器采用自顶向下的结构将 cost map 和不同级别的 backbone 特征图结合起来进行分割。通过级联特征聚合模块（FAM）和跳层融合模块（SFM）来逐步组合视觉编码器的多层级特征图 $F_2, F_3, F_4$ 从而生成高分辨率特征图 $F_h$，这通过一个输出层转换为不同类别的分割图预测。需要注意的是，CAT-Seg 中观察到，直接对图像编码器回传梯度会破坏 open-vocabulary semantic segmentation 性能。所以这里对 skip-layer fusion 模块到图像编码器的梯度进行了截断。 FAM 分别利用大核深度卷积执行空间融合，利用线性自注意力操作执行类别融合。 SFM 利用层次编码器的浅层特征 $F_2, F_3, F_4$ 增强 FAM 输出特征图的局部细节。并进一步融合原始的 cost map。 category early rejection strategy：因为类别数量增大时，解码器中的推理成本会显著增加。但是实际上大多数图像仅包含数个语义类别。所以本文在解码器中引入的类别早期拒绝方案，可以在早期层有效地预测现有类别并拒绝不存在的类别，从而显著提高推理速度。从而最多提供 4.7 倍的速度提升而不会出现显著的性能下降。 训练期间，如图 4a，添加的辅助卷积分治分别预测分割图，并受真值监督。为了避免对模型训练的负面影响，这里阶段了其与 decoder 主体间的梯度。 推理期间，如图 4b，引入 top-k 策略来预测存在的语义类别。通过对每个像素选择选择 top-k 个最大激活类别，从而生成一个所有像素的并集，移除未选择类别后的特征图（$H\u0026rsquo; \\times W\u0026rsquo; \\times N\u0026rsquo; \\times D$）被送到下一个解码器层中。论文中观察到 k=8 的时候确保了大多数存在的类别被识别。 实验设定 训练集：COCO-Stuff 的训练集，包含大约 118k 密集标注的 171 类目标。 测试集：跨数据集测试。 ADE20K，包含 20K training 和 2K validation =\u0026gt; A-150 和 A-847。 PASCAL VOC，包含 1.5k training 和 1.5k validation =\u0026gt; PAS-20。 PASCAL-Context 来自原始的 PASCAL VOC 数据集 =\u0026gt; PC-59 和 PC-459。 模型设定： 基于 ConvNeXt-B/L 视觉编码器形式的预训练 CLIP。 类别模板数量 $P$ 同 CAT-Seg 一致，均为 80。 文本编码器冻结，只训练图像编码器和解码器。 GPU: 4xA6000 with the mini-batch of 4 images。 图像编码器学习率多乘以一个 0.01 倍的因子。 共 80k 次迭代。 训练时剪裁图像 $768 \\times 768$ 大小，测试时直接放缩图像到 $768 \\times 768$ 大小。 Vision Encoder 形式和微调策略的消融 Decoder 结构的消融 早退结构的消融 ","date":"April 11, 2024","permalink":"/blog/posts/0008-cvpr-2024---sed---a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/","summary":"","title":"CVPR 2024 - SED - A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/15 CVPR 2024 - OVFoodSeg - Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation 论文：https://arxiv.org/abs/2404.01409 主要内容 大量食材之间的类别差异、新食材的出现以及与大型食物分割数据集相关的高注释成本。现有方法主要采用封闭词汇和静态文本嵌入设置，往往无法有效处理食材，特别是新颖和多样化的食材。为此本文提出了一种新的开放词汇食品图像分割（Open-Vocabulary Food Image Segmentation）框架 OVFoodSeg，通过采用图像感知文本表示来提升开放词汇食品图像分割的能力。这一任务和框架旨在解决现有方法在处理新和多样化的食材时的不足。\n在整合视觉语言模型 CLIP 的基础上，为了处理食物配料视觉表征中大的类内方差，该方法集成了两个创新模块，即图像到文本学习器 FoodLearner 和图像感知的文本编码器 Image-Informed Text Encoder，丰富了文本嵌入与图像特定的信息，从而有效地将知识从已知的食材转移到新的食材。\nOVFoodSeg 的训练过程分为两个阶段：\n第一阶段是预训练 FoodLearner，使其具备将视觉信息与特定相关食物的文本表征对齐的能力。利用视觉表征利用交叉注意力层更新可学习的 query token，文本信息联合生成文本表征。\nFoodLearner 使用预训练的 BLIP2 中的 qformer 来初始化。训练时配合 query token 输入的可以使图像嵌入，也可以是文本嵌入，甚至可以只有文本嵌入。多种输入形式实现对于这一组件的联合优化。 使用 Recipe-1M+ 数据集训练。其中的成分过于详细，类似调料等虽然是食谱的组成部分，但由于它们在图像中是不可见的，因此会造成数据噪声。为了提高配料数据的质量，本文通过特定的 prompt 让 ChatGPT 为每个菜谱获取最直观的配料。然后利用清理后的成分列表作为文本信息，与相应的图像配对后对 FoodLearner 模块进行预训练。 第二阶段是用于分割任务的学习阶段，调整 FoodLearner 和 Image-Informed Text Encoder 以适应分割任务。\nFoodLearner 基于可学习的 query token 与图像特征的交互获得文本诱导的视觉表征，这与 CLIP 文本编码器提出的文本表征直接相加，之后直接送入现有的开集分割算法 SAN 的管线中。 训练中使用两个基准数据集，即 FoodSeg103 和 FoodSeg195。对于前者，本文随机选择 20 个类作为新类，剩下的 83 个作为基类。对于后者随机使用 40 类作新类，剩余的用于训练。新类别在训练时不可见，仅用于测试。 通过在大规模食品相关图像文本对数据集上预训练 FoodLearner，OVFoodSeg 成功地将视觉信息与文本表示紧密地联系起来，从而有效地解决了食材图像分割中的大类内变化问题。OVFoodSeg 在两个开放词汇食品图像分割基准测试中都取得了最先进的性能，证明了其有效性和对现有方法的超越。\n食品相关的分割数据集 食品图像分割是食品计算的核心问题，构建具有像素级掩码注释的大规模数据集是解决这一问题的基础。\n菜品级别注释：注释都限制在菜品级别的分割注释上，这使得它们不适合用于食材分割。 Food201【Im2Calories: towards an automated mobile vision food diary】包含约 12,000 张图片，跨越 201 种菜肴类。 UECFoodPix【A new large-scale food image segmentation dataset and its application to food calorie estimation based on grains of rice】和 UEC-FoodPIX Complete【UEC-FoodPIX Complete: A large-scale food image segmentation dataset】，它们包含了大约 10,000 张图片，跨越 102 种菜肴类型用于分割。 食材级注释 FoodSeg103【A large-scale benchmark for food image segmentation】是第一个具有食材级注释的食物图像分割数据集。包含涉及 103 个食材类别的大约 7,000 个图像。 FoodSeg195 由本文基于 FoodSeg103 扩展。额外添加 92 个类创建的。其中包括大约 18k 用于训练的图像和 16k 用于测试的图像，总共 113k 个带注释的掩码。额外补充的数据是通过一个政府机构从一个国家的居民那里收集的，用于食物记录。由于保密协定，当前并不会被公开。 Recipe-1M+【Recipe1m+: a dataset for learning cross-modalembeddings for cooking recipes and food images】是当前最大的图像与食谱成对数据集，有大约 1,000,000 个烹饪食谱。每份食谱都详细介绍了菜肴的名称、烹饪方法、配料表，以及来自不同餐馆的大约 10 张代表这道菜的图片。这个数据集包含了大量的食物照片和配料表。需要注意的是，数据集中的成分列表通常包括“看不见的”成分，如糖、油和盐。 ","date":"April 10, 2024","permalink":"/blog/posts/0009-cvpr-2024---ovfoodseg---elevating-open-vocabulary-food-image-segmentation-via-image-informed-textual-representation/","summary":"","title":"CVPR 2024 - OVFoodSeg - Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/14 CVPR 2024 - Open-Vocabulary Video Anomaly Detection 论文：https://arxiv.org/abs/2311.07042 这篇文章主要研究了开放词汇视频异常检测（openvocabulary video anomaly detection，OVVAD）的问题，这是一个具有挑战性但实际重要的问题。\n传统方法不能处理开放词汇场景下的视频异常检测：\n主要是因为它们通常针对特定类别或已知异常进行训练和检测，缺乏对未知异常（即开放词汇）的泛化能力。 此外，传统方法往往难以充分挖掘和利用视频数据中的时空信息以及外部知识，从而限制了其在开放词汇场景下的性能。 该研究提出了一种基于预训练大型模型的解决方案。利用语言图像预训练模型，如 CLIP 作为基础，得益于其强大的零样本泛化能力。\n具体而言，将开放词汇视频异常检测任务分解为类无关检测（class-agnostic detection）和类特定分类（class-specific categorization）这样两个互补的子任务，以更好地处理开放词汇视频异常检测问题。并引入了几个专用模块来促进对基线和新异常的检测。\n对于类无关的检测：设计了时序适配器模块（TA）和语义知识注入模块（SKI），主要旨在提高检测性能； 时序适配器模块 TA：现有的基于时序 Transformer 的设计面对新的类别时性能衰退严重，可能的原因是其中的额外参数特化于训练集，损害了对于新类别的泛化能力。因此本文设计了一个构建在经典的图卷积网络之上的几乎无权重的时序适配器来处理时序依赖，即 $x_t = \\text{LN}(\\text{softmax}(H)x_f), H_{i,j}=\\frac{-|i-j|}{\\sigma}$ 来对各帧特征进行交互重组，这里仅有的额外参数来自于 LN。 语义知识注入模块 SKI：该模块旨在将额外的语义知识引入视觉检测任务中，以提高模型性能。SKI 引入额外的关于异常场景的短语（包括场景名词和动作动词）作为先验知识输入 CLIP。然后使用视觉特征利用类似于 cross-attention 的操作从文本特征中寻找相关的语义知识 $F_{know} = \\text{sigmoid}(x_t F^{\\top}{text})F^{\\top}{text}/l$。这里使用 sigmoid 函数来促使视觉信号可以组合更多相关的语义概念。模块输出直接送入二值检测器中获得类无关检测的帧异常和视频异常得分。 对于细粒度异常分类：考虑到 CLIP 这样的预训练 VLM 面对视频相关的数据时 zore-shot 能力有限，本文使用新异常合成模块（NAS）基于潜在的异常类别生成可能的伪新异常视频样本，以帮助模型更准确地分类新异常类型。主要由三个关键过程组成： 首先，使用预定义的模板提示生成器（prompt_gen）来提示大型语言模型（LLMs），例如 ChatGPT 和 ERNIE Bot，以生成有关现实世界中“战斗”的十个简短场景描述。 然后，使用 AI 生成模型（AIGC）例如 DALLE mini 和 Gen-2，从生成的文本描述中产生相应的图像（Igen）或短视频（Sgen）。 Igen 会从单个图像转成视频片段，具体使用不同的检测比例中心剪裁图像中的对应区域，然后放缩到原始尺寸后，堆叠形成新的视频片段 Scat。 为了模拟完整长异常视频中真实世界场景，这里将生成的 Scat 或者 Sgen 被随机插入到正常视频中的随机位置上，从而获得最终的伪异常视频样本 Vnas。 利用这些 Vnas 可以微调训练在现有标准数据集（包含正常样本和异常样本）上的模型，从而增强其对于新异常的泛化能力。 实验结果表明，该模型在三个公开基准 UBnormal，UCF-Crime，XD-Violence 上优于现有方法，特别是在处理新类别时表现出明显的优势。\n","date":"April 10, 2024","permalink":"/blog/posts/0010-cvpr-2024---open-vocabulary-video-anomaly-detection/","summary":"","title":"CVPR 2024 - Open-Vocabulary Video Anomaly Detection","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/13 CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection 论文：https://arxiv.org/abs/2404.05687 代码：https://github.com/mlvlab/RALF 本文提出了一种新的开放词汇目标检测方法 Retrieval-Augmented Losses and visual Features (RALF)。RALF 通过从大型词汇库中检索词汇并增强损失函数和视觉特征来提高检测器对新类别的泛化能力。\n该方法由两个部分组成：检索增强损失（RAL）和检索增强视觉特征（RAF）。\nRAL RAF RAL 通过使用与负词汇库的语义相似性的距离来优化嵌入空间。通过从大型词汇库中，按照语义相似性检索与真实类别标签相关的难负词汇和易负词汇。然后，RAL 使用这些词汇和真实框嵌入来定义难负损失和易负损失。 RAF 则利用大型语言模型（LLM）生成关于大型词汇库的描述，并从中提取有关目标的详细信息，以增强视觉特征。RAF 首先在离线阶段从目标提案中生成视觉特征。然后，在推理阶段，RAF 使用概念检索器和增强器从概念存储库中检索相关概念，并使用这些概念来增强视觉特征。 通过实验，作者证明了 RALF 在 COCO 和 LVIS 基准数据集上的有效性。特别是在 COCO 数据集的新类别上，APN50 提高了 3.4%，在 LVIS 数据集的新类别上，mask APr 提高了 3.6%。 未命名\n","date":"April 10, 2024","permalink":"/blog/posts/0011-cvpr-2024---retrieval-augmented-open-vocabulary-object-detection/","summary":"","title":"CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/12 CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts 论文：https://arxiv.org/bas/2404.00741 代码：https://github.com/uncbiag/SegNext 这篇文章主要研究了如何在保持低延迟的同时提高交互式图像分割的质量，并实现多种提示的兼容性。\n研究人员提出了一种名为 SegNext 的方法，它重新引入了专家模型中常用的密集视觉提示的表示和融合方式，以促进高质量的分割。该方法将五种不同的视觉提示（包括点击、框、涂鸦、多边形和遮罩）统一在一个密集图上。\n所提方法也支持文本提示，即使用 CLIP 来将文本提示编码成向量，并通过交叉注意力块将其与图像嵌入融合。尽管文本提示的性能可能具有很大的提升空间，但是配合视觉提示仍然展现出了具有希望的表现。\n实验结果显示，密集表示和融合视觉提示是实现高质量分割的关键设计选择。与现有的专家模型相比，该方法能够在保持低延迟的同时实现更好的分割效果。\n交互式图像分割是一个长期存在的计算机视觉任务，旨在精确地划分特定的图像区域。然而，现有的专家模型和通用模型在实现低延迟、高质量的交互式分割以及支持多种提示方面存在困难。现有方法无法实现低延时高性能的原因主要有以下几点：\n现有方法通常只支持有限的提示，如点击或简单的文本描述，缺乏多样性，这限制了模型的泛化能力。 许多方法采用联合编码的方式处理图像和提示，导致计算复杂度高，延迟较大。 在处理图像分割任务时，现有方法往往需要对整个图像进行重新计算，每次更新提示都需要重新计算，这也增加了延迟。 相比之下，本文提出的方法通过引入密集的视觉提示和优化模型结构，实现了低延时和高性能的图像分割效果。\n与 SAM 这类专家模型的关键差异主要在于表征视觉提示的方式不同。本文采用密集表征来保留空间信息。这样的密集设计导致了高质量的分割效果。\n","date":"April 10, 2024","permalink":"/blog/posts/0012-cvpr-2024---rethinking-interactive-image-segmentationwith-low-latency-high-quality-and-diverse-prompts/","summary":"","title":"CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/11 CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation 论文：https://arxiv.org/abs/2403.00712 代码：https://github.com/baegwangbin/DSINE 该研究重新思考了表面法线估计的归纳偏置，提出了利用逐像素射线方向并学习相邻表面法线之间相对旋转关系的方法。相比于现有采用通用密集预测模型的方法，该方法能生成清晰且平滑的预测，并且在训练数据规模更小的情况下展现出更强的泛化能力。\n归纳偏置的重要性：现有的表面法线估计方法采用了一般目的的密集预测模型，这限制了其预测精度和对不同相机拍摄图像的泛化能力。这篇论文深入讨论了表面法线估计所需的归纳偏置，并提出了三种架构改变来融入这些偏置。 方法介绍：作者首先提出利用逐像素的射线方向作为输入，然后提出了基于射线方向的激活函数，最后将表面法线估计重新解释为旋转估计，其中相邻像素之间的相对旋转以轴角表示法估计。这种方法可以生成细节丰富且在表面交界处清晰的预测。 总的来说，这篇论文通过提出新的归纳偏置和方法，成功地改善了表面法线估计的准确性和平滑性，并提高了模型的泛化能力。\n表面法线估计\n表面法线估计是一种计算机视觉任务，旨在从单个 RGB 图像中估计出每个像素点的表面法线。表面法线是一个三维向量，表示在给定像素点上垂直于表面的方向。这个任务在许多计算机视觉应用中非常重要，如三维重建、场景理解、物体识别等。 其提供了一种理解场景几何结构的方法。通过估计表面法线，我们可以获取到场景中物体表面的局部属性，如平面的法线方向、曲面的光滑程度等。这些信息对于进一步理解场景和进行后续处理（如深度估计、物体识别等）非常有用。 表面法线估计通常通过深度学习方法实现。通过对大量带有标注的表面法线图像进行训练，神经网络可以学习到一种从 RGB 图像中估计表面法线的方法。然而，由于表面法线估计任务的输出空间是三维的（即单位球面），因此存在一些挑战，如输出空间的非一致性、难以收集带有标注的数据等。\n","date":"April 10, 2024","permalink":"/blog/posts/0013-cvpr-2024---rethinking-inductive-biases-for-surface-normal-estimation/","summary":"","title":"CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/10 Rethinking Interactive Image Segmentation Feature Space Annotation 论文：https://arxiv.org/abs/2101.04378 代码：https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation 本文提出了一种新的交互式图像分割方法，通过特征空间注释来同时对多张图像进行分割注释，这与现有的交互式分割方法在图像领域进行注解的方式形成了鲜明对比。研究结果表明，这种特征空间注解的方法在前景分割数据集上可以取得与最先进的方法相媲美的结果。\n交互式图像分割方法：现有的交互式图像分割方法主要依赖于用户的点击、矩形框或者多边形等输入，以实现像素级的注解。然而，这些方法仍然需要大量的用户努力，成为深度学习应用的一个瓶颈。 特征空间注解的新方法：本文提出了一种新的交互式图像分割方法，通过特征空间投影来进行同时图像分割注解。这种方法与现有方法的不同之处在于，它不是在图像领域进行注解，而是在特征空间中进行操作，从而大大减少了用户的注解负担。 实验结果：实验结果表明，这种新方法在前景分割数据集上可以取得与最先进的方法相当的结果。在语义分割的 Cityscapes 数据集上，它达到了 91.5% 的精度，比原始注解程序快了 74.75 倍。 总的来说，本文提出了一种新的交互式图像分割方法，通过特征空间注解，可以在保持结果竞争力的同时，大大提高用户的注解效率。这种方法为交互式图像注解提供了一个新的方向，可以与其他现有方法相结合，实现更有效的注解。\n","date":"April 10, 2024","permalink":"/blog/posts/0014-rethinking-interactive-image-segmentation--feature-space-annotation/","summary":"","title":"Rethinking Interactive Image Segmentation: Feature Space Annotation","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/9 CVPR 2024 - Efficient Deformable ConvNets - Rethinking Dynamic and Sparse Operator for Vision Applications 论文：https://arxiv.org/abs/2401.06197 代码：https://github.com/OpenGVLab/DCNv4 本文提出了高效的 DCNv4，这是一个专为视觉应用设计的高效有效的运算符。\nDCNv4 通过两个关键增强解决了其前身 DCNv3 的限制：\n在空间聚合中去除 softmax 归一化，以增强其动态特性和表达能力； 优化内存访问，最小化冗余操作以加快速度。通过对现有实现进行指令级内核剖析，发现 DCNv3 已经很轻量级，计算成本不到 1%，而内存访问成本占了 99%。因此重新审视运算符实现，并发现许多内存访问在 DCN 的前向过程中是冗余的，可以通过优化来实现更快的 DCNv4 实现。 这些改进使得 DCNv4 与 DCNv3 相比显示出显著更快的收敛速度，并且处理速度大大提高，DCNv4 的速度提高了三倍以上。\n将 DCNv4 集成到其他现代骨干架构中，包括 ConvNeXt 和 ViT，替换深度可分离卷积和密集自注意力层。值得注意的是，在没有进行任何超参数调整的情况下，这些经过精心设计的网络在使用 DCNv4 时表现得相当出色，同时速度快得多，显示了动态、稀疏的 DCNv4 的有效性和效率。\n","date":"April 10, 2024","permalink":"/blog/posts/0015-cvpr-2024---efficient-deformable-convnets--rethinking-dynamic-and-sparse-operator-for-vision-applications/","summary":"","title":"CVPR 2024 - Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/8 CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization 论文：https://arxiv.org/abs/2305.15253 这篇文章主要讨论了领域泛化评估协议的重新思考，特别是如何处理可能存在的测试数据信息泄露风险。作者首先指出，当前的领域泛化评估协议可能存在问题，可能导致测试数据信息泄露，进而影响评估的公平性和准确性。为了解决这个问题，作者提出了两个建议：\n一是建议领域泛化算法在进行比较和评估时应采用自监督预训练权重或随机权重作为初始化； 二是建议对每个训练模型在多个测试域上进行评估。 作者还根据这些建议重新评估了十个代表性的领域泛化算法，并提供了三个新的测试leaderboard。这些更改和新的测试leaderboard将鼓励未来的研究，并促进领域泛化的更准确评估。\n","date":"April 10, 2024","permalink":"/blog/posts/0016-cvpr-2024---rethinking-the-evaluation-protocol-of-domain-generalization/","summary":"","title":"CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/7 CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection 论文：https://arxiv.org/abs/2312.10461 代码：https://github.com/chuangchuangtan/NPR-DeepfakeDetection 本文主要研究了基于 CNN 的生成网络中的上采样操作，以实现通用的深度伪造检测。研究者发现，上采样操作不仅可以产生基于频率的伪造 artifact，还可以产生更广义的伪造 artifact。研究者提出了邻近像素关系（NPR）的概念，用作训练检测模型的 artifact 表示。研究者通过在包含 28 种不同生成模型样本的开放世界数据集上进行的全面分析，验证了所提出 NPR 的有效性。\n上采样操作的研究：研究者发现，上采样操作在常见的生成模型中普遍存在，这些操作不仅在频率域上影响整个图像，还在像素级上留下痕迹。 邻近像素关系（NPR）：研究者提出了邻近像素关系（NPR）的概念，用作训练检测模型的 artifact 表示。NPR 有效地捕捉了与图像细节相关的 artifact，如头发、眼睛和胡子等。NPR（Neighboring Pixel Relationships）是一种用于检测合成图像中本地上采样痕迹的简单而有效的特征表示方法。它通过计算相邻像素之间的关系来揭示图像细节中的上采样痕迹。在实际应用中，可以通过调整窗口大小 l 和指数 j 来优化 NPR 的性能。实验表明，当 l=2 且 j=1 时，NPR 具有较好的性能。此外，NPR 可以应用于不同类型的图像生成技术，如 GAN 和扩散生成的图像，具有较强的泛化能力。 NPR 的计算过程如下： 选择一个大小为 l×l 的窗口，窗口中心位于像素点 (x, y)。 在窗口内，计算窗口中心像素值与窗口内其他像素值的差值，并将这些差值存储在一个向量 vc 中。 对于向量 vc 中的每个元素 vi，计算其与指定参考值的差值，即 vi-vj。 将这些差值组成的向量视为一个特征表示，记作 NPR。 NPR 的有效性验证：研究者通过对包含 28 种不同生成模型样本的开放世界数据集进行的全面分析，验证了所提出 NPR 的有效性。实验结果显示，NPR 在 28 种不同的生成模型上都有很好的泛化能力，相比于现有方法，NPR 带来了显著的 11.6% 的提升。 如何实现检测算法：本文中的检测算法是通过训练一个二元分类器 D 来实现的，该分类器利用训练图像的相邻像素关系（NPR）作为特征提取器 f 的输入，以区分真实图像和合成图像。在测试阶段，分类器 D 根据 NPR 特征来预测测试图像是否为真实图像。为了训练这个分类器，作者们首先将 ProGAN 生成器的训练集作为真实图像，然后使用各种 GAN 和扩散模型生成合成图像。这些合成图像与真实图像混合在一起，用于训练二元分类器 D。在训练过程中，NPR 特征被提取出来并输入到分类器 D 中，以使其学习如何区分真实图像和合成图像。在测试阶段，对于给定的测试图像，NPR 特征被提取出来并输入到训练好的分类器 D 中，以预测该图像是否为真实图像。 ","date":"April 10, 2024","permalink":"/blog/posts/0017-cvpr-2024---rethinking-the-up-sampling-operations-in-cnn-based-generative-network-for-generalizable-deepfake-detection/","summary":"","title":"CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/6 ICLR 2024 | FeatUp: A Model-Agnostic Framework for Features at Any Resolution 论文：https://arxiv.org/abs/2403.10516 代码：https://github.com/mhamilton723/FeatUp 背景动机 深层特征是计算机视觉研究的基石，捕获图像语义并使社区即使在零或少样本情况下也能解决下游任务。然而，这些特征通常缺乏空间分辨率来直接执行分割和深度预测等密集预测任务，因为模型会积极地池化大区域的信息。在这项工作中引入了 FeatUp，这是一个与任务和模型无关的框架，用于恢复深层特征中丢失的空间信息。\n相关工作 Image-adaptive Filtering 自适应滤镜通常用于增强图像，同时保留其底层结构和内容。\n例如，双边滤波器（bilateral filters）【Bilateral filtering for gray and color images，The Guided Bilateral Filter: When the Joint/Cross Bilateral Filter Becomes Robust，Fast image dehazing using guided joint bilateral filter】 将空间滤波器应用于低分辨率信号，将图像亮度滤波器应用于高分辨率引导，以混合来自二者的信息。而其重要的扩展形式，联合双边上采样（Joint Bilateral Upsampling，JBU）【Joint bilateral upsampling】，可以在高分辨率引导下对低分辨率信号进行上采样。已成功用于高效图像增强和其他应用。\n最近一些工作嵌入了双边滤波方法【Guided Upsampling Network for Real-Time Semantic Segmentation】和非局部均值（Nonlocal Means）【A Non-Local Algorithm for Image Denoising】方法到卷积网络【Superpixel Convolutional Networks using Bilateral Inceptions，Non-local neural networks】和视觉 transformer【Blending Anti-Aliasing into Vision Transformer，Transformer for Single Image Super-Resolution】中。\nShape Recipes【Shape recipes: scene representations that refer to the image】学习信号之间的局部关系以创建上采样目标信号。 Pixel-adaptive convolutional (PAC) 【Pixel-Adaptive Convolutional Neural Networks】使卷积运算适应输入数据，并已用于提高分割【Single-Stage Semantic Segmentation from Image Labels】和单目深度估计【Semantically-Guided Representation Learning for Self-Supervised Monocular Depth，SelfDeco: Self-Supervised Monocular Depth Completion in Challenging Indoor Environments】。 Spatially-Adaptive Convolution (SAC)【SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation】中的空间自适应卷积将自适应滤波器分解为注意力图和卷积核。 Bilateral Inception Module【Superpixel Convolutional Networks using Bilateral Inceptions】将双边过滤扩展到超像素，并将此操作嵌入到深层网络中以改进语义分割。 这类方法在各种应用中都有效，直接将空间信息合并到任务中，同时仍然允许学习网络的灵活性。\nImage Super-resolution 最早的深度无监督超分辨率方法之一是零样本超分辨率 ZSSR【Zero-Shot Super-Resolution Using Deep Internal Learning】，它在测试时学习单图像网络。 局部隐式模型 LIIF【Learning Continuous Image Representation with Local Implicit Image Function】使用局部自适应模型来插值信息，并已被证明可以提高超分辨率网络的性能。 深度图像先验 DIP【Deep Image Prior】表明，CNN 可以为零样本图像去噪和超分辨率等逆问题提供归纳偏置。 尽管有大量关于图像超分辨率的文献，但这些方法并不适合处理超低分辨率但高维的深层特征。\nGeneral-purpose feature upsampling 双线性插值是一种广泛使用的对深度特征图进行上采样的方法。虽然有效，但这种方法模糊了信息，并且对原始图像中的内容或高分辨率结构不敏感。最近邻插值法和双三次插值法也有类似的缺点。 在更大的输入上评估网络可以实现更高的分辨率，但计算成本很高。此外，由于相对感受野大小的减小，这通常会降低模型性能和语义。 对于深度卷积网络，一种流行的技术是将最终卷积步长设置为 1【Fully Convolutional Networks for Semantic Segmentation】。然而，这种方法会产生模糊的特征，因为模型的感受野仍然很小（原文是仍然很大，似乎意思不太对）。 最近使用 visual transformer 的工作【Deep vit features as dense visual descriptors】对输入 patch 步幅进行了类似的修改并插入位置编码。虽然简单有效，但分辨率每增加 2 倍，这种方法就会导致计算占用量急剧增加，因此无法在实践中用于更大的上采样因子。由于前面 patch 的感受野是固定的，这种方法也会扭曲特征。 Image-adaptive feature upsampling 反卷积/转置卷积【Learning Deconvolution Network for Semantic Segmentation】使用学到的卷积核将特征转换到具有更大分辨率的新空间。 Resize+Convolution【Deconvolution and Checkerboard Artifacts】将学习的卷积附加到确定性上采样过程中，并减少困扰反卷积的棋盘伪影。这一设定是现在图像解码器的常见组件，并已应用于语义分割和超分辨率。 其他方法，如 IndexNet【Index Networks】和 Affinity-Aware Upsampling (A2U) 【Learning Affinity-Aware Upsampling for Deep Image Matting】在图像抠图方面很有效，但在其他密集预测任务上效果不佳【FADE: Fusing the Assets of Decoder and Encoder for Task-Agnostic Upsampling】。 Pixel-adaptive convolutional (PAC) 【Pixel-Adaptive Convolutional Neural Networks】、CARAFE【Carafe: Content-aware reassembly of features】、SAPA【Sapa: Similarity-aware point affiliation for feature upsampling】和 DGF【Fast end-to-end trainable guided filter】等方法使用可学习的输入自适应算子来转换特征。 - PAC 很灵活，但它并没有忠实地对现有特征图进行上采样，而是用于转换下游任务的特征。 - DGF 通过逐点卷积和线性映射来近似 JBU 操作，但没有完全实现 JBU，因为局部的 query/model 在计算上很难处理。这正是本文通过新的高效 CUDA 核解决的问题。 - FADE 中引入了一种新的半移位算子，并使用解码器特征来生成联合特征上采样模块。 Implicit Feature Alignment function (IFA)【Learning implicit feature alignment function for semantic segmentation】从不同的角度（in a different light）看待特征上采样，重点关注最近邻方法，以将编码器 - 解码器架构中的特征图与 IFA 对齐。虽然 IFA 在特定语义分割基准上表现良好，但它没有利用图像引导，也无法在编码 - 解码器框架之外学习高质量表示。 具体方法 FeatUp 背后的核心直觉是，人们可以通过观察低分辨率特征的多个不同“视角”来计算高分辨率特征。\n与 NeRF 通过在场景的许多 2D 照片之间强制一致性来构建 3D 场景的隐式表示一样，FeatUp 通过在许多低分辨率特征图之间强制一致性来构建上采样器，即认为低分辨率信号的多视图一致性可以监督高分辨率信号的构建。更具体地说，通过聚合多个“抖动”（例如翻转、填充、裁剪）图像的模型输出的低分辨率视图来学习高分辨率信息。通过学习具有多视图一致性损失的上采样网络来聚合这些信息。这一基本思想可以产生多种方法。\n基于联合双边上采样的轻量级前向上采样器：该前馈上采样器是联合双边上采样 (JBU) 滤波器的参数化概括，通过自定义 CUDA 核函数从而比现有实现速度快了几个数量级且占用内存更少。该上采样器可以与几个卷积相当的计算成本产生与对象边缘对齐的高质量特征。 基于隐式网络的上采样器：将隐式模型拟合到单个图像以在任何分辨率下重建特征。通过用深度隐式网络过拟合信号，针对目标图像独立学习，允许任意分辨率特征生成，同时具有较低的存储成本。 在这两种上采样架构的特征可以在下游应用中直接替换使用，因为所提方法不会转换底层特征的语义，即使无需重新训练也能获得分辨率和性能提升。\n文中的实验表明，FeatUp 在类激活图生成、分割和深度预测的迁移学习以及语义分割的端到端训练方面显着优于其他特征上采样和图像超分辨率方法。\n第一步是生成低分辨率特征视图，以细化为单个高分辨率输出。为此用小的 padding、尺寸和水平翻转扰乱输入图像，并将模型应用于每个变换后的图像，以提取低分辨率特征图的集合。这些小的图像抖动可以帮助观察输出特征的微小差异，并提供子特征信息来训练上采样器。 接下来从这些视图构建一致的高分辨率特征图。假设可以学习一个潜在的高分辨率特征图。当下采样时，它可以再现低分辨率的抖动特征。FeatUp 的下采样是光线行进（ray-marching）的直接模拟；正如在此 NeRF 中将 3D 数据渲染为 2D 一样，这里的下采样器将高分辨率特征转换为低分辨率特征。与 NeRF 不同，这里不需要估计生成每个视图的参数。相反，用于 \u0026quot; 抖动 \u0026quot; 每个图像的参数被跟踪，并在下采样之前对学到的高分辨率特征应用相同的转换。然后使用高斯似然损失将下采样特征与真实模型输出进行比较【It Is Likely That Your Loss Should be a Likelihood】。一个好的高分辨率特征图应该重建所有不同视图中观察到的特征。 整体的多视角重建损失可以表示如下：（低分辨率特征层面上的一致性）\n$t \\in T$ 代表这些小变换的集合，$x$ 为输入图像，$f$ 为模型主干。 $\\sigma_{\\downarrow}$ 为学到的下采样器，$\\sigma_{\\uparrow}$ 为学到的上采样器。 $F_{hr} = \\sigma_{\\uparrow}(f(x), x)$ 形成预测的高分辨率特征 $F_{hr}$。这种参数化允许 $\\sigma_{\\uparrow}$ 可以有四种形式： - 引导上采样器（取决于 $x$ 和 $f (x)$）； - 非引导上采样器（仅取决于 $f (x)$）； - 隐式网络（取决于仅 $x$）； - 特征的可学习缓存参数（不依赖任何东西，即一套可学习的参数）。 $|\\cdot|$ 是标准的 $l_2$ 范数。 $s = \\mathcal{N}(f(t(x)))$ 是空间变化的自适应不确定性【It Is Likely That Your Loss Should be a Likelihood】。这将 MSE 损失转化为能够处理不确定性的更合适的似然。这种额外的灵活性允许网络在某些异常特征从根本上无法进行上采样时进行学习。 下采样器设计 这里引入两个选项，即快速且简单的学到的模糊核，更灵活的基于注意力的下采样器。两个提出的模块都不会通过特殊变换来改变特征的 \u0026quot; 空间 \u0026quot; 或 \u0026quot; 语义 \u0026ldquo;，而只是在一个小邻域内插入特征。图 3 中绘制了这两种选择。\n两个下采样器的主要超参数是核大小，对于具有较大感受野的模型（例如卷积网络），内核大小应该更大。\n简单的下采样器 通过学到的模糊核来模糊特征，可以实现为独立应用于每个通道的卷积。学习到的核被归一化为非负且总和为 1，以确保特征保持在同一空间中。尽管这种基于模糊的下采样器非常高效，但它无法捕获动态感受野、对象显着性或其他非线性效应。\n更灵活的注意力下采样器 可以在空间上调整下采样核。该组件使用 1x1 卷积根据高分辨率特征预测显着性图。它将显着性图与学到的空间不变权重和偏置核相结合，并对归一化结果以创建插入特征的空间变化模糊核：\n$\\sigma_\\downarrow(F)_{ij}$ 是结果特征图的第 i, j 个分量。 $F_{hr}[\\Omega_{ij}]$ 是指与下采样特征中的 i, j 位置相对应的高分辨率特征块。 $\\odot$ 和 $\\cdot$ 分别指元素级乘积和向量内积。 $w$ 和 $b$ 是所有补丁共享的学习权重和偏置核。 上采样器设计 图中展示了本文设计的两种方法，二者都使用相同的更广泛的架构和损失进行训练。\n基于堆叠的联合双边上采样器的引导上采样器 该架构学习了一种跨图像语料泛化的上采样策略。\n$\\circ$ 是函数组合。$f(x)$ 是低分辨率特征图。$x$ 是原始图像。\n该架构速度快，直接将输入图像中的高频细节合并到上采样过程中，并且独立于 $f$ 的架构。\n公式将原始 JBU 的实现推广到高维信号，并使该操作可学习。在联合双边上采样中，使用高分辨率信号 $G$ 作为低分辨率特征 $F_{lr}$ 的引导。$\\Omega$ 是每个像素在引导图像中的邻域。\n在实践中，我们使用以每个像素为中心的 3×3 正方形。令 $k(\\cdot,\\cdot)$ 为相似核，用于衡量两个向量的 \u0026quot; 接近 \u0026quot; 程度。然后我们可以形成联合双边过滤器：\n原始的 JBU 形式：\n其中的 $Z$ 是归一化因子，确保核参数加和为 1。spatial 核是向量坐标之间欧氏距离上宽度为 $\\sigma_{spatial}$ 的可学习的高斯核：\n此外，range 核是对引导信号 $G$ 进行操作的多层感知器 (MLP) 输出内积施加温度 $\\sigma_{range}^2$ 的 softmax 的形式：\n原始 JBU 在引导信号 G 上使用固定的高斯核。而这里设计的泛化性能要好得多，因为可以从数据中学习 MLP 以创建更好的上采样器。在实验中使用具有 30 维隐藏向量和输出向量的两层 GeLU MLP。为了评估 $F_{lr}[a, b]$，如果引导像素不直接与低分辨率特征对齐，则遵循原始 JBU 公式使用双线性插值特征。为了与分辨率无关，这里在空间核中使用归一化到 [−1, 1] 的坐标距离。\n这一形式的挑战之一是现有 JBU 实现的速度和内存性能较差。这可以解释为什么这种简单的方法没有得到更广泛的使用。为此，本文为 JBU 中使用的空间自适应内核提供了高效的 CUDA 实现。与使用 torch.nn.Unfold 运算符的简单实现相比，改进操作使用的内存减少了两个数量级，并且推理速度提高了 10 倍。\n基于隐式神经网络的图像特定的上采样器 这在过度拟合单个图像时可以产生非常清晰的特征。通过使用隐式函数 $F_{hr}=MLP(z)$ 参数化单个图像的高分辨率特征。一些现有的上采样解决方案也采用这种推理时训练方法，包括 DIP 和 LIIF。这里使用小型 MLP 将图像坐标和强度映射到给定位置的高维特征。遵循先前工作【Implicit Neural Representations with Periodic Activation Functions】的指导，并使用傅里叶特征来提高隐式表示的空间分辨率。除了标准傅里叶位置特征之外，本文也表明添加傅里叶颜色特征允许网络使用原始图像中的高频颜色信息。这显着加快了收敛速度，并能够优雅地使用高分辨率图像信息，而无需使用条件随机场 (CRF) 等技术。\n$h(z,\\hat{\\omega})$ 表示输入信号 $z$ 的各个成分的离散傅立叶变换，其中频率向量为 $\\hat{\\omega}$。$e_i$ 和 $e_j$ 表示范围在区间 [−1, 1] 内的二维像素坐标场。$:$ 表示沿通道维度的拼接。\n这里的 MLP 是一个小型 3 层 ReLU 网络，具有 dropout（$p = 0.1$）和层归一化。\n这一设定可以使得测试时可以查询像素坐标场以产生任何分辨率下的特征。而隐式表示中的参数数量比一个 224×224 的显式表征小两个数量级以上，同时更具表现力，显着减少收敛时间和存储大小。\n其他细节 使用特征压缩加速训练 为了减少内存占用并进一步加速 FeatUp 隐式网络的训练，本文首先将空间变化的特征压缩到其最大的 $k=128$ 个主成分。此操作几乎是无损的，因为前 128 个分量解释了单个图像特征中 96% 的方差。这将 ResNet-50 的训练时间缩短了 60 倍，减少了内存占用，支持更大的批次，并且对学习的特征质量没有任何明显的影响。 在训练 JBU 上采样器时，在每个批次中对随机投影矩阵进行采样来降维，以避免在内循环中计算 PCA。得益于 Johnson–Lindenstrauss 引理 (让人惊叹的Johnson-Lindenstrauss引理：理论篇 - 科学空间|Scientific Spaces )，这可以达到相同的效果。 总变分先验 为了避免高分辨率特征中的杂散噪声，在隐式特征量值上添加一个小的 ( $\\lambda_{tv}=0.05$) 总变分平滑先验：\n这比正则化完整特征更快，并且避免过度规定各个组件的组织方式。注意不在 JBU 上采样器中使用它，因为此时不会受到过度拟合的影响。\n实验结果 ","date":"April 3, 2024","permalink":"/blog/posts/0018-iclr-2024---featup---a-model-agnostic-framework-for-features-at-any-resolution/","summary":"","title":"ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/5 Arixv 2403 | Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey 论文：https://arxiv.org/abs/2403.14608 语雀文档：https://www.yuque.com/lart/papers/gvqrizgggd22g88n 大型模型代表了多个应用领域的突破性进步，在各种任务中取得了显着的成就。然而，其前所未有的规模伴随着巨大的计算成本。这些模型通常由数十亿个参数组成，需要大量的计算资源来执行。特别是，在为特定下游任务定制它们时，特别是在受计算能力限制的硬件平台上，广泛的规模和计算需求带来了相当大的挑战。\n参数高效微调 (PEFT) 通过在各种下游任务中有效地调整大型模型，提供了实用的解决方案。具体来说，PEFT 是指调整预训练大型模型的参数以使其适应特定任务或领域，同时最大限度地减少引入的附加参数或所需的计算资源的数量的过程。在处理具有高参数量的大规模语言模型时，这种方法尤其重要，因为从头开始微调这些模型可能 computationally expensive 并且 resource-intensive，从而给支撑系统平台的设计带来相当大的挑战。\n这份综述：\n对各种 PEFT 算法进行了全面研究，检查了它们的性能和计算开销。 概述了使用不同 PEFT 算法开发的应用程序，并讨论了用于减轻 PEFT 计算成本的常用技术。 除了算法角度之外还概述了各种现实世界的系统设计，以研究与不同 PEFT 算法相关的实施成本。 这对于旨在了解 PEFT 算法及其系统实现的研究人员来说是不可或缺的资源，提供了对最新进展和实际应用的详细见解。\n大型模型 (LM) 最近引起了公众的极大兴趣。他们理解上下文和细微差别的能力使他们能够熟练地处理跨多个领域的不同任务，包括自然语言处理（NLP）、计算机视觉（CV）等。在 NLP 领域，大型语言模型（LLM）在各种任务上取得了显着的进步，包括文本生成、翻译、个性化聊天机器人和摘要生成，展现出出色的熟练程度。\n早期的研究【Language models are few-shot learners】表明 LLM 表现出高水平的泛化能力，**使他们能够应用所学知识到在原始训练未包含的新任务，这种能力通常称为零样本学习。尽管如此，微调对于在新用户数据集和任务上进一步增强 LLM 性能仍然重要。由于其规模，广泛采用的微调 LLM 的策略主要为参数高效微调（PEFT），这有选择地调整一小部分参数，同时保持其余参数不变。**此外，PEFT 的应用超出了 NLP 领域，并迅速引起了 CV 社区的兴趣，用于微调具有大量参数视觉模型，例如 Vision Transformer 和扩散模型，以及诸如视觉语言模型（VLM）等学科模型。\n这份综述系统回顾和分类 PEFT 算法最新进展，以及不同场景下各种 PEFT 算法相关的系统实施成本。图 1 概述了本文内容。\nLLM 和 PEFT 的一些基本概念 PEFT 的背景知识 微调对于提高未见过的用户数据集和任务上的 LLM 性能仍然至关重要。随着模型规模的不断增长（例如，GPT-2 中的 1.5B 到 GPT-3 中的 175B），标准的完全微调范式需要数千个 GPU 并行工作，这是非常低效且不可持续的。人们提出参数高效微调（PEFT）来调整最小参数，以在下游任务的全面调整上获得更好的性能。\n在并行发展中，视觉和多模态领域的大规模预训练模型也展示了其有效的表征学习能力，能够通过微调实现从大型数据集到较小数据集的适应或跨各种数据模式的适应。因此，这种能力使 PEFT 对更广泛的研究界越来越有吸引力。\n常用的数据集和任务 语言方面：\n通用语言理解评估（GLUE）基准： 它集成了九个句子或句子对语言理解任务（CoLA、SST-2、MRPC、STS-B、QQP、MNLI、QNLI、RTE 和 WNLI），数据集选择过程考虑了数据集大小、文本类型和难度级别的多样性，并建立在现有数据集上。 它还包括专门设计用于评估和分析自然语言中固有的各种语言现象的性能诊断数据集。 它还具有一个公共排行榜来跟踪基准测试的性能，以及一个仪表板来可视化诊断集上的模型性能。 最近的 LLM 论文使用的另一类数据集是常识推理： OpenBookQA 旨在促进高级问答领域的研究，深入研究对主题及其表达语言的深刻理解。 PIQA 主要强调日常场景，表现出对非常规解决方案的偏爱。 Social IQA 作为一种新颖的问答基准而出现，专门用于衡量社会常识智力。 HellaSwag 作为一个数据集，其本质是确定机器恰当地总结句子的能力。 BoolQ 是一个专用于问答的数据集，特别是二元响应（是或否的查询）。 WinoGrande 作为新的集合引入，包含 44,000 个问题。 ARC-easy 是一个新数据集，包含真正的小学水平的多选的科学问题，旨在激发复杂问答的研究。 ARC-challenges 只包含那些基于检索的算法和单词共现算法都不能准确解决的问题。 视觉方面：\n图像识别，例如细粒度视觉分类（FGVC）和视觉任务适应基准（VTAB）。 视频动作识别，涉及 Kinetics-400、SSv2 和 HMDB51 等数据集。 此外 MSCOCO、ADE20K 和 PASCAL VOC 等密集预测任务数据集也被关注。 根据计算流程分类 PEFT 算法 根据操作将 PEFT 算法分为加性、选择性、重新参数化和混合四种类型。\nTransformer 上应用不同类型 PEFT 不同类型的 PEFT 对比 Additive PEFT 通常要么引入额外的权重参数，要么修改激活。在不同的附加可调模块或参数方面彼此不同。\n标准的完全微调需要大量的计算费用，并且还可能损害模型的泛化能力。为了缓解这个问题，一种广泛采用的方法是保持预训练主干不变，并仅引入在模型架构中策略性定位的最少数量的可训练参数。在针对特定下游任务进行微调时，仅更新这些附加模块或参数的权重，这会导致存储、内存和计算资源需求的大幅减少。由于其添加参数的特点，这些技术可以称为附加性调整。\n大体分为三类：\nAdapter 在 Transformer 块内插入小型适配器层。通常适配器层由降维投影矩阵、非线性激活函数和升维投影矩阵，以及残差框架组成。配置适配器时会将内部的维度变化配置设置为超参数。\n顺序形式： Serial Adapter 中最初提出了 NLP 领域适配器概念。每个 Transformer 块都通过添加两个适配器模块来增强，一个适配器模块分别位于自注意力层之后，另一个位于 FFN 层之后。随后的研究旨在解决与适配器层相关的额外计算成本。 AdapterFusion 中提出了一种修改后的框架，其中仅在 FFN 层之后的 \u0026ldquo;Add \u0026amp; Norm\u0026rdquo; 步骤之后插入适配器层，以提高计算效率。 并行形式：这避免前述顺序形式可能会降低模型的并行性，并需要在推理效率和准确性之间进行权衡的问题。 **Parallel Adapter (PA)**将传统的顺序适配器层重新组织为与每个 Transformer 子层并行的网络。 CIAT、CoDA 和 KronA也采用并行适配器设计。 稀疏激活： CoDA还采用稀疏激活机制来提高推理效率。其使用软 top-k 选择过程识别每层中的 k 个重要标记，这些标记将由冻结的预训练 Transformer 层和适配器分支进行处理，以保持模型的准确性。那些不重要的标记仅由适配器分支处理，同时跳过繁重的预训练层，因此在不影响整体性能的情况下优化了推理效率。 为了提高适配器的性能和泛化能力，各种研究已经实现了多任务学习策略。\nAdapterFusion 将所有预训练的适配器保留在模型中，并采用融合模块来合并多任务信息。 *MerA 通过基于权重和激活的最佳传输将预训练的适配器合并为一个适配器。这种方法避免了引入任何额外的可训练参数，从而提高了计算效率。 Hyperformer 将多任务信息存储在共享的超网络中，该超网络根据任务和层 ID 嵌入生成特定于任务和层的适配器参数。给定一个新任务，只需要学习一个额外的任务嵌入，因此减少了训练参数的数量。 Soft Prompt 人们普遍认为，软提示的连续嵌入空间本质上包含更多信息，而不是通过上下文学习来优化离散 token 表示【When do prompting and prefix-tuning work? a theory of capabilities and limitations】。受到这个概念的启发，研究人员直接将可调整向量（称为软提示）附加到输入序列的开头。这可以表示如下：\nPrefix-tuning 将可学习向量添加到所有 Transformer 层中的 k 和 v 序列之前。为了确保优化过程中的稳定性，Prefix-tuning 采用了重参数化策略，利用 MLP 层来生成这些前缀向量，而不是直接对其进行优化。微调后，仅保存前缀向量以供推理。该技术在多项研究中得到了调整和改进。 P-tuning v2删除了重参数化并将其用途扩展到更广泛的模型规模和 NLP 任务。 APT (Adaptive Prefix Tuning) 通过引入自适应门机制来控制每层中的前缀重要性来增强前缀调整。 同期的工作 P-tuning 和 Prompt-tuning 仅在初始词嵌入层,而不是所有层应用可学习向量，以提高训练和推理效率。需要强调的是，后者主要在大型模型中证明了其有效性，特别是那些拥有超过 110 亿个参数的模型。 Xprompt 作为补充，通过分层结构修剪消除了负面提示标记，从而缩小了较小模型规模的性能差距。 并且 Universality and limitations of prompt tuning 中提供了一些针对即时调整的理论分析，证明了其在有限深度 Transformer 中的普遍性和局限性。 IDPG (InstanceDependent Prompt Generation) 通过使用轻量级提示生成器根据每个输入句子生成提示来改进提示调整。 LPT (Late Prompt Tuning) 还利用提示生成器来获取实例感知提示。与之前的工作不同，LPT 仅在中间层之后添加这些提示，而不是在初始层或所有层。这种策略性的布局消除了中间层以下的梯度计算，从而显着加快了训练速度。同时，LPT 可以提高整体性能，因为较短的反向传播路径保留了更多的任务相关信息。 SPT (Selective Prompt Tuning) 受 LPT 的启发，更深入地研究了提示插入策略的重要性。它在每一层中引入一个可学习的概率门控，来确定是使用从前一层传播的提示还是注入新生成的提示。 APrompt 采用了另一种提示插入策略。除了在每个 Transformer 层的输入序列开头插入输入提示之外，APrompt 还为自注意力模块中的相应的 Q、K、V 矩阵添加额外的可学习的 prompt，以学习新的注意力模式。此外，APrompt 还结合了任务特定头的学习。 软提示的概念已被用于各种下游任务，尽管它们的训练可能容易不稳定且收敛缓慢。\nSPoT 为了解决这个问题，使用从一个或多个任务中学习到的源提示来初始化新任务的提示。 TPT (transferable prompt tuning) 提出将 Soft Prompt 从一个任务迁移来初始化另一个任务，这表明更好的提示初始化会带来很大的训练收敛加速。 InfoPrompt 开发了两种基于互信息的损失函数，即头损失和表示损失，以找到更好的提示初始化并学习足够的任务相关信息，从而也加速收敛。 PTP 深入研究了训练不稳定的根本原因。它识别了传统即时调整中损失情况的陡峭性质，其中输入数据的微小变化可能导致显着的损失波动。为了缓解这个问题，PTP 引入了基于扰动的正则化器来平滑损失情况，从而稳定训练过程。 DePT 将软提示分解为带有一对低秩矩阵的较短的软提示，并使用两种不同的学习率进行优化。该策略不仅提高了性能，还提高了训练和推理效率。 SMoP (Sparse Mixture-of-Prompts) 通过利用简短的软提示来减少训练和推理成本。在训练过程中，训练多个简短的软提示，每个提示都针对数据集的特定子集进行定制。在推理过程中，SMoP 集成了一种门控机制，将每个输入实例路由到适当的简短提示。该技术不仅提高了训练和推理阶段的效率，而且还保持了与较长软提示所实现的性能相当的性能。 IPT (Intrinsic Prompt Tuning) 为了进一步减少软提示参数的数量，通过在多个任务上训练自动编码器来识别内在任务子空间。然后，调整新任务只需调整该子空间内的几个参数，从而显着减少训练参数的数量。 其他的一些方案 除了上面提到的方法之外，还出现了其他在微调过程中策略性地合并附加参数的方法。\n(IA)$^3$ 引入了三个一维的可学习重缩放向量，分别重新缩放 K、V 和 FFN 的激活。而 Q 和 K 的尺度向量可以无缝地集成到 Q 和 K 的权重矩阵中。这种集成有效地消除了推理过程中的额外计算成本。 SSF 与之类似，也对模型激活执行线性变换。在预训练模型中的每个操作（即 MSA、FFN 和层归一化）之后，注入 SSF-ADA 层，该层对操作生成的特征执行缩放和移位。在微调过程中，只有 SSF-ADA 层可以更新。而在推理过程中，与 (IA)3 类似，这些 SSF-ADA 层可以合并到模型权重中，因此不会产生额外的推理开销。 IPA (InferenceTime Policy Adapters) 提供了一种新颖的方法来使 LLM（例如 GPT-4）与用户特定的要求保持一致，而无需修改基本模型的参数。当处理参数非常大且通常无法直接访问的模型时，这一点尤其重要。IPA 通过在解码阶段将基本 LLM（基本策略）的输出分布与较小规模模型（适配器策略）的输出分布相结合（通过乘法和归一化）来实现这一目标。在训练过程中，策略适配器的参数使用强化学习进行微调，而基本策略的参数保持固定。在推理过程中，IPA 使用基本模型和经过训练的策略适配器的组合分布进行解码，对其进行定制以满足特定的用户定义标准。 Selective PEFT 专门需要对现有参数进行微调的算法。不需要任何额外的参数，它从主干模型中选择一小部分参数（就像是乘上了了一个二值掩码一样），仅使它们可学习，同时在下游任务的微调过程中保持大多数参数不变。根据所选参数的分组可以分类为非结构化掩码和结构化掩码。\n非结构化掩码 Diff pruning 是一项代表性工作，它在微调过程中将可学习的二进制掩码应用于模型权重。为了实现参数效率，掩模通过 L0 范数惩罚的可微近似进行正则化。 PaFi 只需选择具有最小绝对量级的模型参数作为可训练的。 FishMask 使用近似 Fisher 信息确定参数重要性。然后它根据此信息选择前 k 个参数以形成掩码。 Fish-Dip 也使用 Fisher 信息来计算掩码，但掩码将在每个训练周期动态重新计算。 LT-SFT 引入了另一种技术来确定参数重要性，该技术受到彩票假设的启发，其中选择在初始微调阶段变化最大的参数子集来形成掩码。 SAM【On the effectiveness of parameter-efficient fine-tuning】提出了一种二阶近似方法，该方法用可分析求解的优化函数来近似原始问题，以帮助确定参数掩码。 Child-tuning 提出了两种在每次训练迭代期间选择子网络的方法，其中仅可以更新该子网络内的参数。 结构化掩码：上述非结构化参数掩码会导致非零掩码分布不均匀，并在实现 PEFT 时降低硬件效率。以规则模式组织参数掩码可以提高训练期间的计算和硬件效率。因此各种结构化选择性 PEFT 技术得到了广泛的研究。 Diff pruning提出了一种结构化剪枝策略，将权重参数划分为局部组并策略性地将它们一起消除。 FAR 通过将 Transformer 块中 FFN 的权重分组为节点来微调 BERT 模型，然后使用 L1 范数对学习器节点进行排序和选择。为了进一步降低内存访问频率，他们还通过将学习器节点分组在一起来重新配置 FFN。 Bitfit 仅仅调整每个 DNN 层的 bias 参数，并在小模型上取得有竞争力的结果。然而，这种方法无法处理大型模型。 S-BitFit【Neural architecture search for parameter-efficient fine-tuning of large pre-trained language models】将 NAS 应用到 Bitfit，保留了 Bitfit 中的结构化性质，限制 NAS 算法必须为每个 bias 模块选择是否倒数为的。 Xattn Tuning 其仅微调交叉注意层。 SPT (sensitivity-aware visual parameter-efficient fine-tuning) 首先识别调整时通过损失的减少测量的敏感参数。该灵敏度是使用一阶泰勒展开式计算的，该展开式是在单次微调之前从单个前向和后向传递中导出的。接下来，SPT 找到敏感参数数量超过预定义阈值的权重矩阵，然后将 PEFT（例如 LoRA 和 Adapter）应用于这些目标权重，以实现结构调整。 LayerNorm Tuning 只调整每个注意力块中的 LayerNorm 的权重。这种简单的技术可以实现与微调相当甚至更好的性能，同时提供比 LoRA 高约 10 倍的参数效率。 Reparameterized PEFT 这通常意味着构建低秩参数化以在训练期间实现参数效率的目标。重新参数化微调在训练期间引入了额外的低秩可训练参数，然后将其与原始模型集成来推理以保持推理速度。该方法分为低秩分解和 LoRA 衍生形式两大类。\n早期的研究Intrinsic SAID【Intrinsic dimensionality explains the effectiveness of language model finetuning】表明，常见的预训练模型表现出极低的内在维度。换句话说，可以找到对整个参数空间进行有效微调的低维重新参数化。 Intrinsic SAID 是在 LLM 微调过程中研究内在维度特征的开创性工作。然而，最广泛认可的重新参数化技术是LoRA (Low Rank Adaptation)。\n对于给定的预训练权重矩阵，如图 8(a)，LoRA 引入了两个可训练的权重矩阵（其中秩 $r \\ll min(d,k)$）与预训练权重并行操作，从而实现结果对预训练组件输出的增量更新，从而封装了特定于任务的知识。在训练开始时，使用随机高斯分布对降维矩阵进行初始化，而升维矩阵初始化为零，确保增量最初保持为零值。微调完成后，LoRA 的自适应权重将与预先训练的主干权重无缝集成。这种集成确保 LoRA 保持模型的效率，在推理过程中不会增加额外的负担。\n在 LoRA 训练中，选择合适的 rank 一直是一个具有挑战性的问题。为了解决这个问题，出现了一些工作：\nDyLoRA在预定义的训练预算内训练了一系列的具有不同秩的 LoRA 模块。DyLoRA 在训练过程的每次迭代中动态选择一个秩。则两个矩阵针对所选的秩定制，从而产生两个空间截取的版本，并且本次迭代期间后续的前向和反向传播将被限制在选定位置，而不是完整权重。通过这种动态且免搜索的方法，DyLoRA 显着减少了为特定任务找到最佳且固定的 LoRA 秩所需的训练时间。 AdaLoRA使用奇异值分解 (SVD) 重新表述整个增量变换，表示为 $\\Delta W = P \\Lambda Q$，三个矩阵都设置可学习，其中 $P \\in \\mathbb{R}^{d \\times r}$ 和 $Q \\in \\mathbb{R}^{r \\times k}$ 正交，$\\Lambda \\in \\mathbb{R}^{r \\times r}$ 是包含奇异值的对角矩阵。训练过程中，奇异值被根据其重要性分数进行迭代修剪，这些重要性分数是根据梯度权重乘积大小的移动平均值构建的。并构建额外的正则化项确保 $P,Q$ 之间的正交性。这种自适应方法使模型能够动态调整每个 LoRA 模块内的秩，根据权重矩阵的重要性有效管理其参数计数。 SoRA中认为 AdaLoRA 中使用的重要性分数是启发式构建的，缺乏严格的理论动机。此外，移动平均运算和额外的正则项的计算在训练期间引入了额外的计算成本。为了解决这个问题，SoRA 消除正交性前提，直接应用和优化在降维输出后插入的门控向量 $g \\in \\mathbb{R}^{r}$，其通过 L1 损失的近端梯度迭代的变体进行更新，这具有明确的数学意义并且不需要启发式前提。训练后，通过删除降维和升维权重中对应于门向量中 0 的列和行。 随后的几项研究旨在提高 LoRA 各方面的性能。\nLaplace-LoRA 注意到经过微调的 LLM 常常表现出过度自信。为了增强微调 LLM 的校准，Laplace-LoRA 采用贝叶斯方法，特别是对 LoRA 参数的后验拉普拉斯近似（post-hoc Laplace approximation）。 LoRA+ 建议为 LoRA 的两个权重矩阵设置不同的学习率，以至于升维的学习率大于降维的。 LoRAHub 聚合了针对不同任务进行训练的各种 LoRA 模块。给定一些新任务的示例，LoRAHub 可以通过无梯度方法 Shiwa 自主构建兼容的 LoRA 模块，而无需人工干预。 MOELoRA 采用专家混合 (MOE) 方法在多任务设置中训练 LoRA，从而产生多个专家 LoRA 模块。为了检索特定任务的参数，MOELoRA 利用任务驱动的门函数，根据任务 ID 为每个专家分配贡献权重，并通过所有专家的加权和计算最终参数。 除了 LoRA 之外，其他几种具有巨大潜力的重新参数化技术正在兴起。\nCompacter 将降维和升维权重进一步简化为 $W = \\sum^n_{i=1}A_i \\otimes B_i$，这里 $A_i \\in \\mathbb{R}^{n \\times n}, B_i \\in \\mathbb{R}^{\\frac{r}{n} \\times \\frac{d}{n}}$，并二者使用 Kronecker 积组合。通过将 $A_i$ 指定为共享参数并使用两个低秩矩阵的乘积重新参数化 $B_i$ 来进一步减少参数数量，从而有效地将参数复杂度从 $O(rd)$ 降低到 $O(r+d)$。 KronA 和KAdaptation 也采用 Kronecker 乘积来重新参数化适配器权重，旨在实现参数减少。 HiWi 提出了一种适配器微调方法，该方法将适配器直接应用于预训练参数，而不是隐藏表征上 $W\u0026rsquo; = W + \\sigma(WW_{down})W_{up}$，这里的 $W$ 表示 Transformer 块 FFN 的权重或者偏置参数。推理期间，$W\u0026rsquo;$ 会先计算，确保模型位置推理延时不变。 DoRA (Weight-Decomposed Low-Rank Adaptation) 将模型权重分解为幅度向量 m 和方向矩阵 V 的组合。随后 DoRA 对 m 和 V 采用独特的微调策略。虽然两者都是可调的，但实际只有 V 进行 LoRA 形式的重参数化，对其加上基于降维升维组合的附加权重。DoRA 在各种任务和模型中始终优于 LoRA。 Hybrid PEFT 各种 PEFT 方法的功效在不同的任务中可能存在显着差异。因此，许多研究旨在结合不同 PEFT 方法的优点，或通过分析这些方法之间的相似性来寻求建立统一的视角。\nUniPELT将 LoRA, prefix-tuning, 和 adapters 集成到每个 Transformer 块中。为了控制哪些 PEFT 子模块应该被激活，他们还引入了门控机制。该机制由三个小 FFN 组成，每个 FFN 产生一个 0~1 的标量值 G，然后分别应用于 LoRA、前缀和适配器矩阵。在各种设置中，UniPELT 始终显示出准确度的提高，范围为 1% 到 4%。 S4探索了几种 PEFT 方法（即 Adapter (A), Prefix (P), BitFit (B), LoRA (L)）的设计空间，以揭示底层设计模式。经过一系列实验，他们发现，对 Transformer 层应用主轴分组划分后得到的四个层组具有如下特点，由此产生的具有最佳性能的设计空间是（G1 : AL, G2 : AP, G3 : APB, G4 : PBL）： 组中的层具有相似的行为，这意味着应该应用相似的 PEFT 策略。 将可训练参数的数量均匀分配到各层。 调整所有组。 在不同组中分配不同的 PEFT 策略。 MAM Adapter探索了三种附加性 PEFT 方法之间的内在相似性：adapters, prefix-tuning, LoRA，这导致了如下三种变体的开发，广泛实验表明最有效的配置包含在 SA 层中使用 prefix-tuning，并在 FFN 层中使用 Scaled Parallel Adapter，称为 MAM Adapter。 Parallel Adapter 将适配器层放置在特定层（SA 或 FFN）旁边而不是他们后面； Multi-head Parallel Adapter 将并行适配器分为多个头，都会影响 SA 中的头注意力输出； Scaled Parallel Adapter 在并行适配器层之后添加了一个缩放项，类似于 LoRA。 LLM-Adapters 构建了一个易于使用的框架，将各种 PEFT 技术合并到 LLM 中。通过跨多个数据集的全面基准测试，该研究揭示了几个关键见解： series adapters, parallel adapters, LoRA 最有效的位置分别是在 MLP 层之后、MLP 层旁边，和同时跟随注意力层和 MLP 层。 与规模较大的 LLM 相比，利用 PEFT 的规模较小的 LLM 可以在某些任务上取得有竞争力甚至更好的结果。 通过适当的分布内数据微调，较小的模型能够在特定任务的性能上超越较大的模型。 NOAH 发现不同的 PEFT 配置是专门针对不同的任务而定制的。为了解决这个问题，NOAH 使用 NAS 来识别每个数据集最有效的 PEFT 配置。具体来说，NOAH 的搜索空间包含三种 PEFT 方法：Adapter、LoRA 和 Visual Prompt Tuning (VPT)。它利用 AutoFormer （一种 one-shot NAS 算法）来有效发现最佳 prompt 模块。 AUTOPEFT 首先建立一个搜索空间，其中包括 serial adapters, parallel adapters, prefix tuning。之后提出了一种基于高维多维贝叶斯优化的有效 NAS 方法。 进一步降低 PEFT 算法计算复杂性的策略 从计算的角度来看，处理延迟 latency 和峰值内存开销 peak memory overhead 是需要考虑的关键因素（pivotal factor）。以下几种策略都旨在最小化模型资源消耗的同时增强模型性能。由于量化方法的独特性，将其从内存高效的 PEFT 内容中独立出来单独讨论。\nKV 缓存管理 LLM 模型的核心在于自回归 Transformer 模型。自回归特性成为设计推理系统的一个主要挑战。因为每次生成一个新的令牌时，整个 LLM 模型必须将所有权重从不同的内存转移到图形处理器的内存中，这对于单用户任务调度或多用户工作负载平衡非常不友好。\n服务自回归范式的挑战性部分是所有先前的序列都必须被缓存并保存以供下一次迭代使用，由先前序列生成的缓存激活被存储为键值缓存（KV-cache）。KV-cache 的存储将消耗内存空间和 IO 性能，导致工作负载内存有限 (workload memory-bounded)，并且系统的计算能力利用率不足 (under-utilizing the computation power)。\n之前的工作提出了一系列解决方案，例如 KV-cache 控制管理【Efficient Memory Management for Large Language Model Serving with PagedAttention】或 KV-cache 压缩【High-throughput Generative Inference of Large Language Model with a Single GPU】，以提高吞吐量或减少延迟。\n在设计 PEFT 方法时，考虑 KV-cache 的特性以补充其功能至关重要。例如，当在推理阶段应用 soft prompt 时，有效利用 KV-cache 来处理这些附加输入可以通过确保 prompt 相关的数据易于访问从而帮助加快响应时间。\n剪枝 这可以大大提高 PEFT 方法的效率。特\nAdapterDrop探索了从 AdapterFusion 中删除浅层 Transformer 层适配器和多任务适配器，这表明剪枝可以提高训练和推理效率，同时性能下降最小。 SparseAdapter研究了不同的修剪方法，发现高稀疏率（80％）可以优于标准适配器。此外，大稀疏配置在增加瓶颈维度的同时保持恒定的参数预算（例如，以 50% 的稀疏度加倍维度），大大增强了模型的容量，从而提高了性能。 SPLoRA对 LoRA 中降维和升维的权重采用基于通道的剪枝。这种修剪不仅影响源权重，而且影响 LoRA 的这两个权重。 LoRAPruning 不仅对预训练模型权重采用结构化剪枝，而且对 LoRA 权重也采用结构化剪枝。非结构化 LoRA 剪枝方法主要侧重于稀疏模型权重，同时保持 LoRA 权重的密集性，从而使权重合并难以实现，与此相反，LoRAPruning 可以轻松合并权重。此外，这项工作还引入了一种新颖的标准，利用 LoRA 梯度作为预训练权重梯度的近似值，从而能够估计权重重要性。 ProPETL跨层和任务构建单个共享原型（例如 adapter, prefix, LoRA）。此外，ProPETL 学习二进制掩码来修剪不同层和任务中的不同子网络。因此，参数可以在跨层和任务中重用，大大提高了参数效率。 量化 这是另一种提高计算效率和减少内存使用的流行技术。\nBI-Adapter通过调查适配器的损耗情况，发现适配器对参数空间中的噪声具有抵抗力。基于这一见解，作者引入了一种基于聚类的量化方法。值得注意的是，他们证明了适配器的 1-bit 量化不仅可以最大限度地减少存储需求，而且可以在所有精度设置中实现卓越的性能。 PEQA (ParameterEfficient and Quantization-aware Adaptation) 使用两阶段管道来实现参数高效和量化感知的微调。第一阶段，预训练的 FFN 权重矩阵被量化为一个通道放缩量构成的向量与量化权重之间的乘积。第二阶段，量化权重保持固定，仅对通道放缩向量微调。这种方法不仅保证了内存效率，而且有利于参数效率。 QLoRA 提出了几种新技术，包括 4-bit NormalFloat、双量化和分页优化器，将 4-bit 量化预训练语言模型反向传播到 LoRA 中。这些技术可以在单个 48GB GPU 上对 65B 语言模型进行微调，同时保持与完整 16-bit 微调类似的性能。与原始 LoRA 类似，QLoRA 将固定的零初始化 LoRA 权重附加到量化的预训练模型作为训练起点。 然而，当应用极低 bit（例如 2-bit）量化时，巨大的量化误差会对 LoRA 微调的初始化产生负面影响，即 $\\text{quantization}(W_0)+W_{down}W_{up} \\ne W_0$（$W_{down}=0$），如【Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning】中对初始化的研究所示（“在初始化 PEFT 方法时保持 PLM 的起点是至关重要的”）。为了解决这个问题，提出了几种量化策略来消除量化误差。\nLoftQ (LoRA-Fine-Tuningaware Quantization) 提出了一种创新框架，为后续的 LoRA 微调提供了量化主干权重和 LoRA 权重的优越初始化点。该方法通过在网络初始化期间优化 Frobenius 范数目标来解决由量化引起的差异，该目标考虑了 LoRA 权重和量化的预训练骨干网络。LoftQ 在 2-bit 量化方面表现出优于 QLoRA 的性能，并且对下游任务具有更强的泛化能力。 LQ-LoRA 使用一种受稳健主成分分析启发的迭代算法，该算法近似分解预训练权重 $W_0$ 为 $Q + L_1L_2$ 来解决由量化误差引起的不准确性，其中 $Q$ 是保持固定的量化分量，$L_1, L_2$ 是可训练的低秩分量。此外，该方法利用整数线性规划来确定混合量化策略，从而为每个权重矩阵启用动态量化配置，同时遵守预定的总比特率限制。 QA-LoRA 解决了 QLoRA 的另一个限制，即在微调后难以保留其量化属性。在 QLoRA 中，量化的预训练权重（NF4）必须恢复为 FP16，以在权重合并期间匹配 LoRA 权重精度（FP16）。相反，QA-LoRA 使用 INT4 量化并引入分组运算符以在推理阶段实现量化，因此与 QLoRA 相比提高了效率和准确性。 BitDelta 引入了一种新颖的 1-bit 训练后量化方法，该方法作用于微调模型与底层预训练模型之间的权重增量。具体来说，给定分别来自微调模型和基础模型的权重矩阵，二者之间的权重增量 $\\Delta = W_{fine} - W_{base}$ 被二值化为 $\\alpha \\odot \\text{Sign}(\\Delta)$。这里高精度缩放标量 $\\alpha$ 根据增量的平均绝对增量值初始化。BitDelta 通过在紧凑校准数据集上的蒸馏进一步校准缩放因子，而二进制矩阵保持不变。这种方法通过利用单个全精度基本模型以及高效批处理的 1-bit 增量，显着简化了共享服务器上多个微调模型的部署。 内存优化 由于其规模相当大，微调完整的 LLM 需要大量的训练内存。虽然大多数 PEFT 方法主要以参数效率为目标，但它们在训练期间仍然会产生大量内存开销，因为梯度计算和反向传播对于这些方法仍然是必要的。例如，根据一些文献【LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning，Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients】，与完整模型微调相比，流行的 PEFT 技术（例如适配器和 LoRA）只能将内存使用量减少到大约 70%。从计算的角度来看，内存效率仍然是一个不容忽视的关键因素。\n为了提高内存效率，人们开发了各种技术来最大限度地减少微调期间整个 LLM 缓存梯度的需要，从而减少内存使用。\nSide-Tuning和LST(Ladder-Side Tuning) 都引入了与主干模型并行的可学习网络分支。通过专门通过该并行分支引导反向传播，它避免了存储主模型权重的梯度信息的需要，从而显着减少了训练期间的内存需求。 Res-Tuning将 PEFT 调整组件（例如，prompt tuning, adapter）与骨干模型解耦，提出了一种名为 Res-Tuning-Bypass 的内存高效微调框架，该框架通过删除从解耦的调整组件到主干的数据流，来生成与主干模型并行的旁路网络。这消除了反向传播期间骨干模型内梯度缓存的要求。 MEFT(Memory-efficient fine-tuning) 是一种受可逆模型【The Reversible Residual Network: Backpropagation Without Storing Activations】启发的方法。在可逆模型的训练过程中，中间激活不需要缓存在前向传递中。在反向传播期间，可以根据最终输出重新计算它们。为了在微调过程中节省内存，MEFT 研究了如何将 LLM 转换为其可逆的对应项，而无需额外的预训练。这种转换的一个关键方面是在预训练模型中仔细初始化新引入的参数。 MEFT 论证了参数初始化的重要性，并建议这些参数必须以保留预训练模型起点的方式进行初始化，确保修改后模型的微调达到与完全微调模型相当的性能方法。考虑到这一关键因素，MEFT 引入了三种不同的方法，每种方法都显着减少了传统上存储激活的内存需求。 LoRA-FA解决了 LoRA 微调中内存开销的限制。在训练过程中，LoRA 模块仍然需要较高的激活内存消耗。这是因为，在反向传播期间，必须在前向传播期间存储大量输入激活来计算梯度。 LoRA-FA 通过冻结预训练权重和 LoRA 中降维权重并仅更新 LoRA 中升维权重来解决这个问题。因此，不再需要存储输入激活，因为降维权重输出的中间激活足以用于升维权重的梯度计算。所以 LoRA-FA 中激活的内存需求可以显着降低。 为了进一步减少微调期间的内存使用，一些方法尝试规避 LLM 内的反向传播来解决此问题。\nHyperTuning 采用超模型仅使用少数样本来生成 PEFT 参数。这种方法展示的结果与通过完整模型微调获得的结果相当。 PEFT Plug-in 首先在小型语言模型上训练 PEFT 模块，与在大型语言模型上训练相比，它的内存效率更高。随后，该研究引入了一套技术，用于在推理过程中将这些经过训练的 PEFT 模块无缝集成到 LLM 中。这种策略有效地避免了直接在较大模型上进行基于梯度的优化的必要性，从而节省了大量的内存。但需要注意的是，HyperModel 和 PEFT Plug-in 仍然需要额外的模型训练，而且这个训练成本不能完全忽视。 MeZO 为 LLM 引入了一种内存高效的零阶 (zeroth-order, ZO) 优化器。传统的 PEFT 技术依靠反向传播来计算梯度来更新模型参数，而 MeZO 与此不同，MeZO 仅通过前向传递来微调 LLM。它通过使用 ZO 梯度估计器来计算梯度来实现这一点。值得注意的是，MeZO 为经典 ZO 梯度估计器实现了 in-place 解决方案，有效减轻了内存占用推理执行期间的消耗。这种创新方法允许在具有 80GB 内存的单个 GPU 上对包含 300 亿个参数的 LLM 进行高效微调，同时保持与使用反向传播进行微调相当的性能。此外，与 LoRA 和 Adapter 等传统 PEFT 方法相比，它可以大大减少存储需求。 应用 PEFT 到不同模型架构 主要包括 LLM、Vision Transformer、VLM、扩散模型，以适用于各种下游任务，强调 PEFT 在各种场景中的多功能性和适用性。\n针对 LLM 的应用 Visual Instruct Following VL-Adapter 直接应用了几种 PEFT 方法（Adapter、Hyperformer 和 Compacter），在 VL-BART 基准上几个图像文本和视频文本任务上对它们进行测试。结果表明，简单的适配器最好，它可以实现与完全微调相当的性能。然而，考虑到 VL-BART 中编码器和解码器之间的功能差异，直接分配相同的模块修改将导致性能不佳。 VL-PET选择性地将 PEFT 模块集成到编码器和解码器的不同组件中。还引入了粒度控制机制以实现更细粒度的控制。 LLaMA-Adapter 在 LLaMA 更高的 Transformer 层中的输入标记前添加一组可学习的提示（类似于 prefix tuning）。为了避免在早期训练阶段出现较大损失值的不稳定微调，LLaMA-Adapter 采用零初始化注意力机制，而不是其他 PEFT 方法的随机初始化权重，该机制学习零初始化的门控因子来自适应地调整权重，控制提示对单词标记的贡献。这可以保持微调起点与原始模型相同，并逐步向模型注入新的知识，类似的想法可以在前面讨论的 MEFT 和 LoftQ 中找到。为了表示视觉信息，LLaMA-Adapter 使用 CLIP 图像编码器提取多尺度全局图像特征，然后将它们投影到语言嵌入空间。之后，该功能会按元素添加到所有插入的 Transformer 层的 adaptation prompt 中。仅在 LLaMA-7B 中引入了 120 万个可学习参数，在 8 个 A100GPU 上进行微调的成本不到一小时。 LLaMA-Adapter V2 表明 LLaMA-Adapter 中的简单多模态融合不能推广到更具挑战性的开放式多模态推理任务，其中视觉提示往往比语言指令数据更能主导 adaptation prompt。为了解决这个问题，LLaMA-Adapter V2 将指令遵循（instruction-following）能力的学习（以生成长语言响应）和视觉语言对齐解耦，以避免视觉和语言微调之间的干扰。具体来说，LLaMA-AdapterV2 设置了不相交的参数组，这些参数组分别从图像文本对和语言指令数据中学习。视觉适应提示是在 LLM 的早期阶段插入的，而语言适应提示则保留在与 LLaMA-Adapter 类似的更高 Transformer 层。此外，LLaMA-Adapter V2 引入了更多可学习参数和多个专家系统（例如生成描述、检测和 OCR）来增强多模式性能。 Continual Learning (CL) CL 的目标是在一个模型中随着时间的推移学习一系列新任务，该模型在对话系统、信息提取系统和问答系统等场景中具有广泛应用。CL 的主要挑战是灾难性遗忘。\n一种流行的做法，称为基于架构的方法，通过维护特定于任务的参数来解决 CL 模型中每个新任务的输入。因此，很自然地利用 PEFT 方法来执行 CL 任务。\nAdapterCL 使用 residual adapter 参数化每个新任务。在测试过程中，由于未提供任务 ID，AdapterCL 使用基于熵的分类器来选择使用哪个适配器来完成特定任务。 CPT (Continual Prompt Tuning) 为每个任务训练软提示。CPT 没有从头开始训练软提示，而是提出了一系列技术（continual prompt initialization, query fusion, memory replay, a memory-guided technique）来实现前后任务的知识迁移。 O-LoRA (orthogonal lowrank adaptation) 采用一种在单独的低秩向量子空间内学习不同任务的策略，这些子空间保持彼此正交，以最大限度地减少干扰。这种方法可以有效减少习得新任务过程中的灾难性遗忘。 Context Window Extension LLM 通常使用预定义的上下文大小进行训练。如 LLaMA 和 LLaMA2 的预定义上下文大小分别为 2048 和 4096 个 token。位置编码 RoPE 具有较弱的外推属性，这意味着在输入长度超过预定义上下文长度的情况下，性能明显下降。为了解决这个问题，一个简单的解决方案是将预训练的 LLM 微调到更长的上下文。然而，这会导致计算成本随上下文大小呈二次方上升，从而导致内存和处理资源紧张。\n为了解决这个问题：\nLongLoRA 建议使用 LoRA 微调预训练的 LLM 以扩大上下文大小。为了减少 LoRA 调优和完全微调之间的困惑度差距，LongLoRA 还放开了嵌入层和归一化层进行训练。为了进一步提高长上下文场景下的训练效率，LongLoRA 进一步引入了一种新颖的转移稀疏注意力（S2-Attn）作为训练期间标准自注意力的有效替代品。 LongQLoRA 将 LongLoRA 与 QLoRA 和 Position Interpolation【Extending Context Window of Large Language Models via Positional Interpolation】的优点结合起来，以节省 GPU 内存。这项工作成功地将 LLaMA2-13B 在具有 32GB 内存的单个 V100 上的上下文长度从 4096 扩展到 8192。 除了有限的训练阶段序列长度之外，现实世界的系统内存限制还给上下文窗口带来了另一个关键瓶颈。具体来说，KV 缓存的容量受到可用系统内存的限制。例如，以输入长度 1024 和批量大小 128 运行的 30B 参数 LLM 可能需要高达 180GB 的 KV 缓存【H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models】，从而限制了上下文窗口的可行大小。针对这一点，一些策略诉诸于量化 KV 缓存，但量化肯定会损害性能。为了在不造成重大损失的情况下有效地解决这个问题，最近一些工作展开了探索：\nGEAR提出了一种新颖的方法，通过采用低秩矩阵来捕获量化误差的大部分相干基，并辅以稀疏矩阵来解决异常条目（outlier entry）的误差，从而有效地最小化近似误差。 针对 Vision Transformer（ViT）的应用 图像分类 视觉数据集上的图像分类是一种非常常见的需求，并且具有广泛的应用，而预训练然后微调范式是一种广泛的策略。多种方法利用 PEFT 技术来实现高效的模型调整。\nAdaptFormer 将适配器模块与原始 ViT 模型的 FFN 并行插入，以执行视觉识别任务。 VPT (Visual Prompt Tuning) 将少量特定于任务的参数添加到每个 Transformer 层的输入序列中。当将 ViT 应用于下游任务时，只有这些添加的参数和分类头被设置为可训练。 GatedPromptTuning【Improving Visual Prompt Tuning for Self-supervised Vision Transformers】注意到，与有监督的 ViT 相比，VPT 在自监督 ViT 中的表现通常较差。进一步的分析表明，不同的预训练方法和下游任务对不同位置的变压器块有不同程度的依赖。为了解决这个问题，研究引入了 ViT 块的自适应门控。这些门动态地调节提示 token 对 ViT 块的贡献，从而使模型更有针对性地适应手头的任务。 视频识别 一些工作考虑了更具挑战性的适应问题，将 ViT 转移到具有更大域隔阂的下游任务。\nST-Adapter (Spatio-Temporal Adapter) 和 AIM都将适配器层插入到预训练的 ViT 块中。主要目标是对时空信息进行建模，从而实现从图像模型到视频任务的 ViT 的高效适应。值得注意的是，这两种方法都表现出了超越传统全模型微调方法的性能。 针对 Vision-Language Alignment 模型（VLA）的应用 VLA 如 CLIP、ALIGN、DeCLIP 和 FLAVA，旨在学习可以在统一表示中对齐的良好图像和文本特征空间。每个 VLA 通常由提取各自特征的单独图像和文本编码器组成。这些模型利用对比学习来有效地对齐图像和文本特征。利用微调来提高 VLA 在特定数据集或任务上的性能。\n但微调整个模型需要大量计算。例如，微调 CLIP-RN50x64 需要 32,768 的批大小以及在 592 V100 GPU 上进行 18 天的训练【Learning Transferable Visual Models From Natural Language Supervision】。 此外，对较小数据集的全面微调通常会导致灾难性的遗忘。 为了应对这些挑战，并从 PEFT 技术在 NLP 中的成功中汲取灵感，一系列 PEFT 策略被提出并在 VLA 模型中实现，例如语义分割、点云理解、视频理解、视觉推理、时间动作检测等。本节将重点介绍一项使用 VLA 的常见任务：开放词汇图像分类。\n开放词汇图像分类 早期工作为每个类别设计特定于类别的提示，例如 a photo of a [CLASS]，并根据图像与这些文本描述的相似度对图像进行排序。\nCoOp (Context Optimization) 用可学习的向量替换手工制作的文本提示，同时在训练期间保留整个 VLA 固定。 CoCoOp (Conditional Context Optimization) 在此基础上解决了 CoOp 在泛化到未见过的类方面的局限性。它引入了一个轻量级神经网络，可以生成特定于输入的上下文标记，根据每个图像动态调整提示，从而增强通用性，但代价是由于实例感知操作而增加了计算需求。 ProGrad 通过正则化梯度与一般知识对齐的 soft prompt 的更新，仅更新梯度与所提供的一般知识对齐（或不冲突）的提示按原来的提示。 MaPLe 指出，现有方法要么在 CLIP 的语言中要么在视觉分支中学习提示，这不能有效地利用 VLA 的多模态性质。为了解决这个问题，MaPLe 提出了分支感知分层提示，可以同时调整语言和视觉分支，并实现卓越的性能。 TPT (test-time prompt tuning) 研究动态提示调整，无需额外训练样本。具体来说，在推理过程中，TPT 首先将输入图像增强到各种视图中，然后利用这些视图来调整可学习的提示。主要训练目标是确保 VLA 在面对这些不同观点时能够做出一致的反应。 DiffTPT 通过扩散模型进一步增强了测试样本的数据多样性。 另一方面，一些研究探索了 VLA 中 adapter 的使用。\nCLIP-Adapter 在 CLIP 文本和视觉编码器之后集成了残差式适配器。因此，与 CoOp 和 CoCoOp 不同，CLIP-Adapter 避免了通过 CLIP 编码器进行梯度反向传播，从而减少了训练内存和时间方面的计算要求。 Tip-Adapter 采用与 CLIP-Adapter 相同的设计。与 CLIP-Adapter 不同，adapter 权重是从以非参数方式从少样本监督构建的 query-key cache 模型中以免训练的方式获得的。结果，Tip-Adapter 相较于 CLIP-Adapter 的 SGD 训练过程表现出了更高的效率。 针对扩散模型的应用 这是一类生成模型，通过渐进式去噪过程将随机噪声转换为结构化输出来学习生成数据。在训练过程中，扩散模型学习使用去噪网络反转添加到训练数据中的噪声，而在推理过程中，它们从噪声开始，使用去噪网络迭代创建反映与训练示例相同分布的数据。扩散模型各种应用中最值得注意的是 stable diffusion，它以其强大的生成能力弥合了文本和图像之间的差距。直接从文本描述中获得连贯且上下文相关的图像。许多研究利用 PEFT 技术来适应下游任务的预训练扩散模型，包括加快采样速度、文本到视频的适应、文本到 3D 适应等。这里主要关注两种场景：集成超出单纯基于文本的条件的附加输入模式，以及基于预训练扩散模型的定制化内容生成。\nAdditional Input Control GLIGEN 为了合并附加输入模式（例如布局、关键点），同时保留预训练模型中的广泛知识，引入了一种新颖方法，保持原始模型权重的完整，并集成新的、可训练的接受新的 grounding 输入门控 Transformer 层【Flamingo: a Visual Language Model for Few-Shot Learning】。所得模型不仅可以准确地表示 grounding 条件，而且可以生成高质量的图像。值得注意的是，该模型还可以在推理过程中很好地推广到未见过的对象。 ControlNet 微调来自 stabel diffusion 的编码层的可训练副本，同时锁定其预训练的参数权重。固定的原始模型和可训练的副本通过零卷积层桥接。这些层从零初始化权重开始，旨在在训练过程中逐步适配，确保有害噪声不会影响训练开始时稳定扩散的预训练特征。这个细化的模型能够使用各种输入作为条件，例如 Canny edges, Hough lines, user scribbles, human key points, segmentation maps, shape normals, depths 等。 Concept Sliders 引入了一种即插即用的 LoRA 适配器来在扩散模型中精确编辑概念（例如年龄、微笑）。 T2I-Adapter 引入了一种轻量级适配器模型来对齐外部控制信号与文本到图像扩散模型的内部知识。该适配器可以通过 structural control (e.g., sketch, depth map, semantic segmentation map, and keypose), color control (e.g., hue and color distribution) 以及通过组合多个适配器来集成各种控制来，从而实现精确操作。 Customized Generation 文本到图像扩散模型的有效性受到用户通过文本阐明所需目标的能力的限制。 例如，很难描述一个大规模模型训练中不会遇到的创新玩具车的精确特征。因此，定制生成的目标是使模型能够从用户提供的最小图像集中掌握新概念。\nTextual Inversion 通过找到一个新的伪词（类似于 soft prompt）来解决这个问题，该伪词代表预训练文本到图像扩散的文本嵌入空间中的新的特定概念。伪词通过扩散模型中的原始优化目标进行优化，给定描述概念的小图像集（通常为 3-5 个图像），并且预训练模型保持不变（untouched）。在推理过程中，该伪词可以像任何其他单词一样对待，并与其他文本查询组成（例如，\u0026quot;\u0026lt;伪词\u0026gt;在海滩上的照片 \u0026ldquo;）。 Custom Diffusion 解决了更具挑战性的设置：多个概念的组合微调。它仅微调从文本到注意层中的 K 和 Q 的映射权重，这在多概念学习场景中产生了卓越的性能。此外，在微调过程中，自定义扩散通过引入一小组带有与目标类似描述的真实图像来防止模型遗忘，同时采用增强来加快收敛速度并改进结果。 IP-Adapter 确定了当前方法（例如 ControlNet 和 T2I-Adapter）的局限性，这些方法将条件信号投射到交叉注意模块中。当处理旨在控制内容的图像条件时，这些方法无法生成忠实于提示图像的图像。该问题源于在交叉注意层中合并图像特征和文本特征会丢失图像特定信息，导致仅产生粗粒度的可控生成，例如图像风格而不是图像内容。 为了克服这个问题，IP-Adapter 引入了一种新颖的解耦交叉注意力机制来区分文本和图像特征。IP-Adapter 在每个交叉注意层中添加了专门针对图像特征的额外交叉注意层，并且仅训练新交叉注意层的参数。 PEFT 方法的系统设计挑战 这里主要涉及到系统设计，偏工程化的内容。所以简短摘录一下。\n这里提出了三种预期的使用 ：\nCentralized PEFT Query Serving：云提供商通过 API 提供用户应用程序，从而有助于将许多 ML 功能无缝集成到应用程序中。通过 API 收到针对一项特定下游任务的 query 后，云服务器会使用特化的 LLM 模型处理。此时提出的用于处理多个 PEFT 查询的云解决方案涉及仅存储 LLM 的单个副本和多个 PEFT 模块。该单个副本维护 PEFT 模块的多个分支，每个分支与不同的 PEFT 查询相关联。图 10b 中说明了多 query 的 PEFT 推理计算模式。此时需要关注与四个性能指标：System throughput、Run-time memory footprint during query serving, the memory utilization comes from both model parameters and KV-cache、Accuracy performance with variation context length、Quality of services associated with latency requirements and deadline missing rates。 Distributed System for PEFT：当代的 LLM 预训练模型并不完全支持个性化任务，因此需要使用前面提到的方法进行额外微调。然而，当考虑将私人数据集提供给云提供商时，会出现一个很大的问题，因为这些数据集是个性化的。出于这个考虑，可以假设计算遵循模型集中式和 PEFT 分布式范式，即主干 LLM 存储在云设备中，而个人 PEFT 权重以及数据集存储在用户自己的设备中。如图 10(a) 所示。性能评估主要关注与三个指标：Accuracy performance over the downstream tasks、Compute cost on edge devices、Communication cost between the edge device and the cloud. Multi-PEFT Training：与多 PEFT 服务不同，多个定制 PEFT 的调整总是涉及不同的骨干 LLM。当考虑 LLM 在各种下游任务中使用时，预训练模型通常表现出低于标准的性能。使 LLM 适应不同任务的一种流行方法是精心微调 PEFT。然而，同时调整多个 PEFT 可能会带来相当大的挑战。如何管理内存梯度和模型权重存储，以及如何设计用于批处理 PEFT 训练的高效内核等挑战仍未解决。PEFT 将根据其 PEFT 算法和骨干 LLM 模型进行分类。设计挑战涉及如何同时整合具有相同 LLM 主干和多个不同 LLM 主干的多个 PEFT。 总结 在当前以大型模型和大型数据集为主导的时代，PEFT 作为一种非常有吸引力的方法脱颖而出，可以有效地使模型适应下游任务。该技术通过解决传统全模型微调带来的重大挑战而获得吸引力，传统全模型微调通常对普通用户提出难以满足的计算和数据需求。对于 LEFT 的进一步研究，这里从算法和系统的角度提出了一系列可能的方向，希望能够启发更多的研究人员在这些领域进行进一步的研究：\n简化超参数调整：PEFT 的有效性通常对其超参数敏感，例如适配器瓶颈尺寸、LoRA 秩以及不同附加性 PEFT 层的放置。手动调整这些超参数将花费大量精力。因此，未来的努力可以集中在开发更少依赖手动调整这些参数的方法，或者自动找到最佳的超参数设置。多项研究已经开始解决这个问题，但需要更简单、更有效的解决方案来优化这些超参数。 建立统一的基准：尽管存在像 HuggingFace 的 PEFT 和 AdapterHub 这样的库，但仍然缺乏全面的 PEFT 基准。这种差距阻碍了公平比较不同 PEFT 方法的性能和效率的能力。类似于 MMDetection 的广泛接受的最新目标检测基准将使研究人员能够根据任务和指标的标准集来验证他们的方法，促进社区内的创新和协作。 提升训练效率：**PEFT 的假定参数效率并不总是与训练期间的计算和内存节省一致。**鉴于可训练参数在预训练模型架构中交织在一起，因此在微调期间通常需要计算和存储完整模型的梯度。这种疏忽要求重新思考什么构成了效率。潜在的解决方案在于集成模型压缩技术，例如修剪和量化，以及专门设计用于在 PEFT 调整期间优化内存的创新。进一步提高 PEFT 方法的计算效率的研究势在必行。 探索缩放定律：最初为较小的 Transformer 模型开发的 PEFT 方法的设计和有效性不一定适用于较大的模型。 随着基础模型规模的增加，识别和调整保持有效的 PEFT 策略至关重要。这一探索将有助于根据大型模型结构的发展前景（evolving landscape）定制（tailor）PEFT 方法。 服务更多模型和任务：跨领域大型基础模型的兴起为 PEFT 带来了新的机遇。根据模型的独特特征设计定制的 PEFT 方法，例如 Sora、Mamba 和 LVM，可以解锁新的应用场景和机会。 加强数据隐私：信任中心化系统来服务或微调个性化 PEFT 模块是系统开发人员面临的另一个问题。 信道侧攻击者已成功部署通过劫持中间结果来重建用户数据。未来值得信赖的 LLM 系统设计的一个视角涉及为个人数据以及中间训练和推理结果开发加密协议。 带模型压缩的 PEFT：模型压缩是使 LLM 在资源有限的设备上可执行的最有效方法之一。然而，模型压缩技术对硬件上运行的 PEFT 算法性能的影响仍然是另一个系统性挑战。 量化和剪枝等常见的压缩技术需要专用的硬件平台来加快这一过程，而为压缩模型构建这样的硬件平台是研究人员的另一个方向。 ","date":"April 3, 2024","permalink":"/blog/posts/0019-arixv-2403---parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey/","summary":"","title":"Arixv 2403 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/4 Tips for Qt Set a proper mirror for MaintenanceTool.exe From: https://mirrors.tuna.tsinghua.edu.cn/help/qt/ .\\MaintenanceTool.exe --mirror https://mirrors.tuna.tsinghua.edu.cn/qt ","date":"March 25, 2024","permalink":"/blog/posts/0020-tips-for-qt/","summary":"","title":"Tips for Qt","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/3 Six methods of indexing pixels of Mat in OpenCV .at\u0026lt;\u0026gt;() // modify the pixel directly for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols; ++w) { image.at\u0026lt;Vec3b\u0026gt;(h, w)[0] = 255; image.at\u0026lt;Vec3b\u0026gt;(h, w)[1] = 0; image.at\u0026lt;Vec3b\u0026gt;(h, w)[2] = 0; } } // modify the pixel by the reference for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols; ++w) { Vec3b\u0026amp; bgr = image.at\u0026lt;Vec3b\u0026gt;(h, w); bgr.val[0] = 0; bgr.val[1] = 255; bgr.val[2] = 0; } } // the image has one channel for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols / 2; ++w) { image.at\u0026lt;uchar\u0026gt;(h, w) = 128; } } .ptr\u0026lt;\u0026gt;() // use uchar type for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols / 2; ++w) { uchar* ptr = image.ptr\u0026lt;uchar\u0026gt;(h, w); ptr[0] = 255; ptr[1] = 0; ptr[2] = 0; } } // use cv::Vec3b type for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols / 2; ++w) { Vec3b* ptr = image.ptr\u0026lt;Vec3b\u0026gt;(h, w); ptr-\u0026gt;val[0] = 0; ptr-\u0026gt;val[1] = 255; ptr-\u0026gt;val[2] = 0; } } // use the row pointer and the image has one channel for (int h = 0; h \u0026lt; image.rows; ++h) { uchar* ptr = image.ptr(h); for (int w = 0; w \u0026lt; image.cols / 2; ++w) { ptr[w] = 128; } } // use the pixel pointer and the image has one channel for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols / 2; ++w) { uchar* ptr = image.ptr\u0026lt;uchar\u0026gt;(h, w); *ptr = 255; } } iterator // the image has three channels Mat_\u0026lt;Vec3b\u0026gt;::iterator it = image.begin\u0026lt;Vec3b\u0026gt;(); Mat_\u0026lt;Vec3b\u0026gt;::iterator itend = image.end\u0026lt;Vec3b\u0026gt;(); for (; it != itend; ++it) { (*it)[0] = 255; (*it)[1] = 0; (*it)[2] = 0; } // the image has one channel Mat_\u0026lt;uchar\u0026gt;::iterator it1 = image.begin\u0026lt;uchar\u0026gt;(); Mat_\u0026lt;uchar\u0026gt;::iterator itend1 = image.end\u0026lt;uchar\u0026gt;(); for (; it1 != itend1; ++it1) { (*it1) = 128; } .data pointer // 3 channels uchar* data = image.data; for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols / 2; ++w) { *data++ = 128; *data++ = 128; *data++ = 128; } } // 1 channel uchar* data = image.data; for (int h = 0; h \u0026lt; image.rows; ++h) { for (int w = 0; w \u0026lt; image.cols / 2; ++w) { *data++ = 128; } } .row() and .col() for (int i = 0; i \u0026lt; 100; ++i) { image.row(i).setTo(Scalar(0, 0, 0)); // modify the i th row data image.col(i).setTo(Scalar(0, 0, 0)); // modify the i th column data } when isContinuous() is true Mat image = imread(\u0026#34;...\u0026#34;); int nRows = image.rows; int nCols = image.cols * image.channels(); if (image.isContinuous()) { nCols = nRows * nCols; nRows = 1; } for (int h = 0; h \u0026lt; nRows; ++h) { uchar* ptr = image.ptr\u0026lt;uchar\u0026gt;(h); for (int w = 0; w \u0026lt; nCols; ++w) { // ptr[w] = 128 ; *ptr++ = 128; } } Reference http://t.csdn.cn/bSDNn ","date":"March 25, 2024","permalink":"/blog/posts/0021-six-methods-of-indexing-pixels-of-mat-in-opencv/","summary":"","title":"Six methods of indexing pixels of Mat in OpenCV","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/2 Snippets of OpenVINO-CPP for Model Inference Header File #include \u0026lt;openvino/openvino.hpp\u0026gt; Create Infer Request void preprocessing(std::shared_ptr\u0026lt;ov::Model\u0026gt; model) { ov::preprocess::PrePostProcessor ppp(model); ppp.input().tensor().set_layout(\u0026#34;NHWC\u0026#34;); // input data is NHWC from OpenCV Mat ppp.input().model().set_layout(\u0026#34;NCHW\u0026#34;); // In the model, the layout is NCHW model = ppp.build(); } ov::Core core; auto model = core.read_model(model_path); # can use onnx or openvino\u0026#39;s xml file preprocessing(model); auto compiled_model = core.compile_model(model, \u0026#34;CPU\u0026#34;); // Or without `\u0026#34;CPU\u0026#34;` auto input_port = compiled_model.input(); auto infer_request = compiled_model.create_infer_request(); Input and Output single input infer_request.set_input_tensor(blob); infer_request.crop_net.infer(); single output ov::Tensor single_output = this-\u0026gt;point_net.get_output_tensor(0); multiple outputs ov::Tensor multi_outputs0 = this-\u0026gt;point_net.get_output_tensor(0); ov::Tensor multi_outputs1 = this-\u0026gt;point_net.get_output_tensor(1); OpenCV cv::Mat \u0026lt;-\u0026gt; OpenVINO ov::Tensor The key to these steps is the alignment of the data layout.\ncv::Mat -\u0026gt; ov::Tensor // converting the uint8 3-channels image mat to a float32 tensor image.convertTo(image, CV_32FC3, 1.0 / 255); // NHWC layout as mentioned above. (N=1, C=3) ov::Tensor blob(input_port.get_element_type(), input_port.get_shape(), (float *)image.data); ov::Tensor -\u0026gt; cv::Mat // tensor follows the NCHW layout, so tensor_shape is (N,C,H,W) ov::Shape tensor_shape = tensor.get_shape(); // Due to N=1 and C=1, we can directly assign all data to a new mat. cv::Mat mat(tensor_shape[2], tensor_shape[3], CV_32F, tensor.data()); Reference https://github.com/OpenVINO-dev-contest/YOLOv7_OpenVINO_cpp-python/blob/main/cpp/main_preprocessing.cpp https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/main.cpp ","date":"March 25, 2024","permalink":"/blog/posts/0022-snippets-of-openvino-cpp-for-model-inference/","summary":"","title":"Snippets of OpenVINO-CPP for Model Inference","type":"posts"},{"content":" Author: lartpang Link: https://github.com/lartpang/blog/issues/1 Build OpenCV and OpenVINO for Windows 10 with VS 2022 In this guide, I will build the two powerful open-source libraries, i.e., OpenCV and OpenVINO for running my deeplearning model on windows 10. Interestingly, both libraries are closely associated with Intel 🖥️.\nOpenCV 😮 First of all, we must download the related code projects (opencv and opencv_contrib containing some plugins for opencv) into our computer from this links:\nhttps://github.com/opencv/opencv/releases https://github.com/opencv/opencv_contrib/tags Make sure the selected versions of the two libararies are the same. Here, I choice the latest version 4.7.0. Because we will recompiling them by ourselves, we can just download the source code zip files. Put the two unpacked libraries into the same parent folder opencv_dir as follows:\n-opencv_dir\r-opencv-4.7.0\r-...\r-opencv_contrib-4.7.0\r-modules\r-... NOTE: To avoid the network issue that may be encountered during using CMake, we need to add the url proxy prefix https://ghproxy.com/ before the urls of some setting of the relevant modules like https://ghproxy.com/https://raw.github***:\n.cmake in opencv-4.7.0/3rdparty/ippicv .cmake in opencv-4.7.0/3rdparty/ffmpeg CMakeLists.txt in opencv_contrib-4.7.0/modules/face Files in cmake of opencv_contrib-4.7.0/modules/xfeatures2d CMakeLists.txt in opencv_contrib-4.7.0/modules/wechat_qrcode CMakeLists.txt in opencv_contrib-4.7.0/modules/cudaoptflow Next, start compiling OpenCV.\nCreate the build folder: cd opencv_dir \u0026amp;\u0026amp; mkdir opencv-build-vs2022 Configure and generate the VS solution by CMake with some config items: General: source folder: \u0026lt;opencv-4.7.0\u0026gt; build folder: \u0026lt;opencv-build-vs2022\u0026gt; BUILD_OPENCV_WORLD=ON CMAKE_BUILD_TYPE=RELEASE OPENCV_ENABLE_NONFREE=ON BUILD_opencv_dnn=ON OPENCV_EXTRA_MODULES_PATH=\u0026lt;opencv_contrib-4.7.0/modules\u0026gt; CUDA: WITH_CUDA=ON WITH_CUDNN=ON WITH_CUBLAS=ON WITH_CUFFT=ON CUDA_FAST_MATH=ON CUDA_ARCH_BIN=7.5 (We can fill the single value corresponding to the real GPU for accelerating the compilation process.) OPENCV_DNN_CUDA=ON Go to the build directory: cd \u0026lt;opencv-build-vs2022\u0026gt; Start build by cmake and msvc compiler: cmake --build . --config Release --verbose -j8 Install the built opencv into the install folder in the current path: cmake --install . --prefix install Add the bin directory into the user environment: \u0026lt;path\u0026gt;\\install\\x64\\vc17\\bin In VS: add the \u0026lt;path\u0026gt;\\install\\include directory into \u0026ldquo;解决方案资源管理器-\u0026gt;右键点击属性-\u0026gt;VC++目录-\u0026gt;外部包含目录\u0026rdquo; add the \u0026lt;path\u0026gt;\\install\\x64\\vc17\\lib directory into \u0026ldquo;解决方案资源管理器-\u0026gt;右键点击属性-\u0026gt;VC++目录-\u0026gt;库目录\u0026rdquo; add the opencv_world470.lib into \u0026ldquo;解决方案资源管理器-\u0026gt;右键点击属性-\u0026gt;链接器-\u0026gt;输入-\u0026gt;附加依赖项\u0026rdquo; OpenVINO 🍰 The document of OpenVINO is intuitive and the readability is better than OpenCV. The relevant content about building and installing the libirary is listed in these links:\nhttps://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/cmake_options_for_custom_comiplation.md https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md After building and install the OpenCV library, it\u0026rsquo;s time to move on to OpenVINO.\nWe need clone the project and the sub modules. git clone https://github.com/openvinotoolkit/openvino.git\rcd openvino\rgit submodule update --init --recursive Create the build folder: mkdir build \u0026amp;\u0026amp; cd build Configure and generate the VS solution by CMake: ENABLE_INTEL_GPU=OFF (We only use the Intel CPU.) Disable some frontend items: ENABLE_OV_PDPD_FRONTEND=OFF ENABLE_OV_TF_FRONTEND=OFF ENABLE_OV_TF_LITE_FRONTEND=OFF ENABLE_OV_PYTORCH_FRONTEND=OFF For Python: ENABLE_PYTHON=ON It seems that openvino-dev needs to be installed first in the detected environment, otherwise a warning message will be thrown in the cmake-gui window. PYTHON_EXECUTABLE=\u0026lt;python.exe\u0026gt; PYTHON_INCLUDE_DIR=\u0026lt;incude directory\u0026gt; PYTHON_LIBIRARY=\u0026lt;pythonxx.lib in libs directory\u0026gt; For OpenCV: ENABLE_OPENCV=ON OpenCV_DIR=\u0026lt;opencv-build-vs2022/install\u0026gt; Build the library: cmake --build . --config Release --verbose -j8 Install the library into the install directory: cmake --install . --prefix install Add the bin directory into the environment: \u0026lt;path\u0026gt;\\install\\runtime\\bin\\intel64\\Release \u0026lt;path\u0026gt;\\install\\runtime\\3rdparty\\tbb\\bin In VS: add the \u0026lt;path\u0026gt;\\install\\runtime\\include directory into \u0026ldquo;解决方案资源管理器-\u0026gt;右键点击属性-\u0026gt;VC++目录-\u0026gt;外部包含目录\u0026rdquo; add the \u0026lt;path\u0026gt;\\install\\runtime\\lib\\intel64\\Release directory into \u0026ldquo;解决方案资源管理器-\u0026gt;右键点击属性-\u0026gt;VC++目录-\u0026gt;库目录\u0026rdquo; add the 🌟 openvino.lib, 🌟 openvino_onnx_frontend.lib, openvino_c.lib into \u0026ldquo;解决方案资源管理器-\u0026gt;右键点击属性-\u0026gt;链接器-\u0026gt;输入-\u0026gt;附加依赖项\u0026rdquo; Set DLL path in IDE VS: \u0026ldquo;right click on solution -\u0026gt; Properties -\u0026gt; Debugging -\u0026gt; Environment -\u0026gt; PATH=\u0026lt;path\u0026gt;\\install\\x64\\vc17\\bin;%PATH%\u0026rdquo; Qt Creator: \u0026ldquo;Projects -\u0026gt; Build \u0026amp; Run -\u0026gt; Build/Run -\u0026gt; Environment -\u0026gt; Details -\u0026gt; Eidt %PATH% -\u0026gt; Add \u0026lt;path\u0026gt;\\install\\x64\\vc17\\bin\u0026rdquo; ","date":"March 25, 2024","permalink":"/blog/posts/0023-build-opencv-and-openvino-for-windows-10-with-vs-2022/","summary":"","title":"Build OpenCV and OpenVINO for Windows 10 with VS 2022","type":"posts"}]