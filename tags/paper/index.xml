<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Paper on 啊，哈！</title><link>https://lartpang.github.io/blog/tags/paper/</link><description>Recent content in Paper on 啊，哈！</description><generator>Hugo -- 0.145.0</generator><language>en-us</language><lastBuildDate>Sat, 26 Oct 2024 13:48:07 +0000</lastBuildDate><atom:link href="https://lartpang.github.io/blog/tags/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>Privacy-Enhancing Technologies in Biomedical Data Science</title><link>https://lartpang.github.io/blog/posts/0001-privacy-enhancing-technologies-in-biomedical-data-science/</link><pubDate>Sat, 26 Oct 2024 13:48:07 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0001-privacy-enhancing-technologies-in-biomedical-data-science/</guid><description> 1. 引言 数据共享是生物医学创新的重要推动力。公共数据仓库和生物银行使不同组织的研究人员能够分析他们自己可能无法收集的大量人类主体数据。许多学术实验室、商业企业和医院已经联合起来形成合作联盟，共享生物医学数据，希望从数据集中提取出由于数据集规模有限而无法被单个实体访问的洞察。政府实体（例如，美国国立卫生研究院数据管理和共享政策；https://sharing.nih.gov ）和国际标准制定组织如全球基因组学和健康联盟（1）制定的政策和指南在维护生物医学界的数据共享文化中发挥了关键作用，这一传统根植于人类基因组计划等里程碑式的合作努力中。</description></item><item><title>ArXiv 2405 - Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery - An Experimental Study</title><link>https://lartpang.github.io/blog/posts/0005-arxiv-2405---rethinking-scanning-strategies-with-vision-mamba-in-semantic-segmentation-of-remote-sensing-imagery---an-experimental-study/</link><pubDate>Fri, 17 May 2024 11:08:54 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0005-arxiv-2405---rethinking-scanning-strategies-with-vision-mamba-in-semantic-segmentation-of-remote-sensing-imagery---an-experimental-study/</guid><description> Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery - An Experimental Study</description></item><item><title>ICLR 2024 - FasterViT - Fast Vision Transformers with Hierarchical Attention</title><link>https://lartpang.github.io/blog/posts/0006-iclr-2024---fastervit---fast-vision-transformers-with-hierarchical-attention/</link><pubDate>Fri, 17 May 2024 09:21:14 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0006-iclr-2024---fastervit---fast-vision-transformers-with-hierarchical-attention/</guid><description> FasterViT: Fast Vision Transformers with Hierarchical Attention 论文：https://arxiv.org/abs//2306.06189 代码：https://github.com/NVlabs/FasterViT 解读：https://mp.weixin.qq.com/s/0AXri6EUrXd6Hwf2HU5Wmg 原始文档：https://github.com/lartpang/blog/issues/18</description></item><item><title>CVPR 2024 - SED - A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation</title><link>https://lartpang.github.io/blog/posts/0008-cvpr-2024---sed---a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/</link><pubDate>Thu, 11 Apr 2024 06:37:48 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0008-cvpr-2024---sed---a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/</guid><description> CVPR 2024 - SED - A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation 论文：https://arxiv.org/abs/2311.15537 代码：https://github.com/xb534/SED 这篇文章提出了一种名为 SED 的简单编码器解码器，用于结合 CLIP 的 open-vocabulary 能力实现了开放词汇语义分割。在多个语义分割数据集上的实验证明了 SED 在开放词汇准确性和效率方面的优势。当使用 ConvNeXt-B 时，SED 在 ADE20K 上的 mIoU 得分为 31.6%，并且在单个 A6000 上每张图像只需 82 毫秒。</description></item><item><title>CVPR 2024 - OVFoodSeg - Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation</title><link>https://lartpang.github.io/blog/posts/0009-cvpr-2024---ovfoodseg---elevating-open-vocabulary-food-image-segmentation-via-image-informed-textual-representation/</link><pubDate>Wed, 10 Apr 2024 11:12:23 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0009-cvpr-2024---ovfoodseg---elevating-open-vocabulary-food-image-segmentation-via-image-informed-textual-representation/</guid><description> CVPR 2024 - OVFoodSeg - Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation 论文：https://arxiv.org/abs/2404.01409 主要内容 大量食材之间的类别差异、新食材的出现以及与大型食物分割数据集相关的高注释成本。现有方法主要采用封闭词汇和静态文本嵌入设置，往往无法有效处理食材，特别是新颖和多样化的食材。为此本文提出了一种新的开放词汇食品图像分割（Open-Vocabulary Food Image Segmentation）框架 OVFoodSeg，通过采用图像感知文本表示来提升开放词汇食品图像分割的能力。这一任务和框架旨在解决现有方法在处理新和多样化的食材时的不足。</description></item><item><title>CVPR 2024 - Open-Vocabulary Video Anomaly Detection</title><link>https://lartpang.github.io/blog/posts/0010-cvpr-2024---open-vocabulary-video-anomaly-detection/</link><pubDate>Wed, 10 Apr 2024 09:39:46 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0010-cvpr-2024---open-vocabulary-video-anomaly-detection/</guid><description> CVPR 2024 - Open-Vocabulary Video Anomaly Detection 论文：https://arxiv.org/abs/2311.07042</description></item><item><title>CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection</title><link>https://lartpang.github.io/blog/posts/0011-cvpr-2024---retrieval-augmented-open-vocabulary-object-detection/</link><pubDate>Wed, 10 Apr 2024 07:50:42 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0011-cvpr-2024---retrieval-augmented-open-vocabulary-object-detection/</guid><description> CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection 论文：https://arxiv.org/abs/2404.05687 代码：https://github.com/mlvlab/RALF 本文提出了一种新的开放词汇目标检测方法 Retrieval-Augmented Losses and visual Features (RALF)。RALF 通过从大型词汇库中检索词汇并增强损失函数和视觉特征来提高检测器对新类别的泛化能力。</description></item><item><title>CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts</title><link>https://lartpang.github.io/blog/posts/0012-cvpr-2024---rethinking-interactive-image-segmentationwith-low-latency-high-quality-and-diverse-prompts/</link><pubDate>Wed, 10 Apr 2024 06:51:40 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0012-cvpr-2024---rethinking-interactive-image-segmentationwith-low-latency-high-quality-and-diverse-prompts/</guid><description> CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts 论文：https://arxiv.org/bas/2404.00741 代码：https://github.com/uncbiag/SegNext</description></item><item><title>CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation</title><link>https://lartpang.github.io/blog/posts/0013-cvpr-2024---rethinking-inductive-biases-for-surface-normal-estimation/</link><pubDate>Wed, 10 Apr 2024 06:27:49 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0013-cvpr-2024---rethinking-inductive-biases-for-surface-normal-estimation/</guid><description> CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation 论文：https://arxiv.org/abs/2403.00712 代码：https://github.com/baegwangbin/DSINE 该研究重新思考了表面法线估计的归纳偏置，提出了利用逐像素射线方向并学习相邻表面法线之间相对旋转关系的方法。相比于现有采用通用密集预测模型的方法，该方法能生成清晰且平滑的预测，并且在训练数据规模更小的情况下展现出更强的泛化能力。</description></item><item><title>Rethinking Interactive Image Segmentation: Feature Space Annotation</title><link>https://lartpang.github.io/blog/posts/0014-rethinking-interactive-image-segmentation--feature-space-annotation/</link><pubDate>Wed, 10 Apr 2024 06:26:16 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0014-rethinking-interactive-image-segmentation--feature-space-annotation/</guid><description> Rethinking Interactive Image Segmentation Feature Space Annotation 论文：https://arxiv.org/abs/2101.04378 代码：https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation 本文提出了一种新的交互式图像分割方法，通过特征空间注释来同时对多张图像进行分割注释，这与现有的交互式分割方法在图像领域进行注解的方式形成了鲜明对比。研究结果表明，这种特征空间注解的方法在前景分割数据集上可以取得与最先进的方法相媲美的结果。</description></item><item><title>CVPR 2024 - Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications</title><link>https://lartpang.github.io/blog/posts/0015-cvpr-2024---efficient-deformable-convnets--rethinking-dynamic-and-sparse-operator-for-vision-applications/</link><pubDate>Wed, 10 Apr 2024 06:24:56 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0015-cvpr-2024---efficient-deformable-convnets--rethinking-dynamic-and-sparse-operator-for-vision-applications/</guid><description> CVPR 2024 - Efficient Deformable ConvNets - Rethinking Dynamic and Sparse Operator for Vision Applications</description></item><item><title>CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization</title><link>https://lartpang.github.io/blog/posts/0016-cvpr-2024---rethinking-the-evaluation-protocol-of-domain-generalization/</link><pubDate>Wed, 10 Apr 2024 06:23:52 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0016-cvpr-2024---rethinking-the-evaluation-protocol-of-domain-generalization/</guid><description> CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization</description></item><item><title>CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection</title><link>https://lartpang.github.io/blog/posts/0017-cvpr-2024---rethinking-the-up-sampling-operations-in-cnn-based-generative-network-for-generalizable-deepfake-detection/</link><pubDate>Wed, 10 Apr 2024 06:22:51 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0017-cvpr-2024---rethinking-the-up-sampling-operations-in-cnn-based-generative-network-for-generalizable-deepfake-detection/</guid><description> CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection 论文：https://arxiv.org/abs/2312.10461 代码：https://github.com/chuangchuangtan/NPR-DeepfakeDetection</description></item><item><title>ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution</title><link>https://lartpang.github.io/blog/posts/0018-iclr-2024---featup---a-model-agnostic-framework-for-features-at-any-resolution/</link><pubDate>Wed, 03 Apr 2024 05:02:42 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0018-iclr-2024---featup---a-model-agnostic-framework-for-features-at-any-resolution/</guid><description> ICLR 2024 | FeatUp: A Model-Agnostic Framework for Features at Any Resolution</description></item><item><title>Arixv 2403 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey</title><link>https://lartpang.github.io/blog/posts/0019-arixv-2403---parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey/</link><pubDate>Wed, 03 Apr 2024 04:46:25 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0019-arixv-2403---parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey/</guid><description> Arixv 2403 | Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</description></item></channel></rss>