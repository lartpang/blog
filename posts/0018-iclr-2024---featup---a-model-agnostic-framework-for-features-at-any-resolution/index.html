<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta http-equiv=content-language content="en-us"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution &#183; 啊，哈！</title>
<meta name=title content="ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution &#183; 啊，哈！"><meta name=description content="Just for fun."><meta name=keywords content="paper,"><link rel=canonical href=https://lartpang.github.io/blog/posts/0018-iclr-2024---featup---a-model-agnostic-framework-for-features-at-any-resolution/><link type=text/css rel=stylesheet href=/blog/css/main.bundle.min.01dde6ff86a3d23e6258a134912503c50666955af42c932ac894e1b1bd4644de893c078daf8e692391173d9351000cbc001451e378e0a8ccc240295b9ed5a0f8.css integrity="sha512-Ad3m/4aj0j5iWKE0kSUDxQZmlVr0LJMqyJThsb1GRN6JPAeNr45pI5EXPZNRAAy8ABRR43jgqMzCQClbntWg+A=="><link rel=apple-touch-icon sizes=180x180 href=/blog/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/blog/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/blog/favicon-16x16.png><link rel=manifest href=/blog/site.webmanifest><meta property="og:url" content="https://lartpang.github.io/blog/posts/0018-iclr-2024---featup---a-model-agnostic-framework-for-features-at-any-resolution/"><meta property="og:site_name" content="啊，哈！"><meta property="og:title" content="ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution"><meta property="og:description" content="Just for fun."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-03T05:02:42+00:00"><meta property="article:modified_time" content="2024-04-03T05:02:42+00:00"><meta property="article:tag" content="Paper"><meta name=twitter:card content="summary"><meta name=twitter:title content="ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution"><meta name=twitter:description content="Just for fun."><script defer type=text/javascript src=/blog/lib/fuse/fuse.min.1f56d60a7738743270762a2aa4e0a453be99c4476f06f3b2f51f4377d12a7ed973420f76a308d7e4855d88ec34d25940c9015464934faf30e7599fe566d7f6e4.js integrity="sha512-H1bWCnc4dDJwdioqpOCkU76ZxEdvBvOy9R9Dd9EqftlzQg92owjX5IVdiOw00llAyQFUZJNPrzDnWZ/lZtf25A=="></script><link rel=preload href=https://lartpang.github.io/blog/fonts/Ubuntu-Regular.woff2 as=font type=font/woff2 crossorigin><script>const siteTheme="auto";let savedTheme=localStorage.getItem("theme")||siteTheme;savedTheme=="auto"&&window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches&&(savedTheme="dark"),savedTheme==="dark"&&(document.documentElement.classList.add("dark"),localStorage.setItem("theme","dark"))</script></head><body><div class=content><header><a class=title href=/blog/><img loading=lazy src=https://avatars.githubusercontent.com/u/26847524 alt="Site Logo"></a><div class=header-cntr><a class=title href=/blog/><span>啊，哈！</span></a><div class=menu><nav id=main-menu class=mm-normal><ul><li><button id=mob-x-icon class=menu-btn arial-label=x-icon><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></button></li><li><a href=/blog/posts/ aria-label=posts>Posts</li></a><li><a href=/blog/tags/ aria-label=tags>Tags</li></a><li><a href=/blog/about/ aria-label=about>About</li></a><li><a href=https://github.com/lartpang aria-label=github><span><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></li></a></ul></nav><div class=side-menu><button id=search-open class=menu-btn aria-label="Search button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button><div id=search-container data-url=https://lartpang.github.io/blog/><div class=search><div class=panel><form><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
<input type=search id=search-query placeholder=Search tabindex=0 autocomplete=off>
<button id=search-close title="Cancel (ESC)"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></button></form></div><ul id=search-results></ul><div id=search-overlay></div></div></div><button id=theme-switcher class=menu-btn aria-label="Theme switcher"><div id=moon><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></div><div id=sun><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></div></button></div><button id=mob-hb-icon class=menu-btn aria-label="Hamburger icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></button></div><script>(function(){document.addEventListener("DOMContentLoaded",function(){const s=document.getElementById("mob-hb-icon"),e=document.getElementById("mob-x-icon"),t=document.getElementById("main-menu"),n=document.body;s.addEventListener("click",function(){t.classList.replace("mm-normal","mm-mobile-open"),e.style.display="block",n.style.overflow="hidden"}),e.addEventListener("click",function(){t.classList.replace("mm-mobile-open","mm-normal"),e.style.display="none",n.style.overflow=""})})})()</script><script>(function(){var t,n,e=document.getElementById("main-menu");if(!e)return;t=window.location.pathname,n=e.querySelectorAll("a"),n.forEach(function(e){e.getAttribute("href")===t&&e.classList.add("active")})})()</script></div></header><main><ul class=breadcrumbs><li><a href=https://lartpang.github.io/blog/><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1v16.2c0 22.1-17.9 40-40 40h-16c-1.1.0-2.2.0-3.3-.1-1.4.1-2.8.1-4.2.1L416 512h-24c-22.1.0-40-17.9-40-40v-24-64c0-17.7-14.3-32-32-32h-64c-17.7.0-32 14.3-32 32v64 24c0 22.1-17.9 40-40 40h-24-31.9c-1.5.0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1.0-40-17.9-40-40V360c0-.9.0-1.9.1-2.8v-69.7h-32c-18 0-32-14-32-32.1.0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7L564.8 231.5c8 7 12 15 11 24z"/></svg></a></li><li><a href=https://lartpang.github.io/blog/posts/>Posts</a></li></ul><h1 class=pg-title>ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution</h1><div class=meta><p><span class=meta-icon><svg fill="currentcolor" width="800" height="800" viewBox="0 0 24 24"><path d="M19 4H17V3a1 1 0 00-2 0V4H9V3A1 1 0 007 3V4H5A3 3 0 002 7V19a3 3 0 003 3H19a3 3 0 003-3V7A3 3 0 0019 4zm1 15a1 1 0 01-1 1H5a1 1 0 01-1-1V12H20zm0-9H4V7A1 1 0 015 6H7V7A1 1 0 009 7V6h6V7a1 1 0 002 0V6h2a1 1 0 011 1z"/></svg>
</span>Posted on <time datetime=2024-04-03T05:02:42+00:00>Apr 3, 2024</time>
<span class=meta-icon><svg width="800" height="800" viewBox="0 0 24 24" fill="currentcolor"><path d="M23 12c0 6.0751-4.9249 11-11 11C5.92487 23 1 18.0751 1 12 1 5.92487 5.92487 1 12 1c6.0751.0 11 4.92487 11 11zM3.00683 12c0 4.9668 4.02638 8.9932 8.99317 8.9932 4.9668.0 8.9932-4.0264 8.9932-8.9932.0-4.96679-4.0264-8.99317-8.9932-8.99317-4.96679.0-8.99317 4.02638-8.99317 8.99317z" fill="currentcolor"/><path d="M12 5C11.4477 5 11 5.44771 11 6v6.4667S11 12.7274 11.1267 12.9235C11.2115 13.0898 11.3437 13.2343 11.5174 13.3346l4.6198 2.6673C16.6155 16.278 17.2271 16.1141 17.5032 15.6358 17.7793 15.1575 17.6155 14.5459 17.1372 14.2698L13 11.8812V6C13 5.44772 12.5523 5 12 5z" fill="currentcolor"/></svg>
</span>3 mins</p><p><span class=meta-icon><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M48 32H197.5c17 0 33.2 6.74 45.2 18.75l176 175.95c25 25 25 65.6.0 90.6L285.3 450.7c-25 25-65.6 25-90.6.0L18.75 274.7C6.743 262.7.0 246.5.0 229.5V80C0 53.49 21.49 32 48 32zm64 144c17.7.0 32-14.3 32-32 0-17.7-14.3-32-32-32-17.67.0-32 14.3-32 32s14.33 32 32 32z"/></svg>
</span><a class=tag href=/blog/tags/paper/>Paper</a></p></div><div class=toc><details><summary accesskey=c title=(Alt+C)>Table of Contents</summary><div class=toc-innr><nav id=TableOfContents><ul><li><a href=#背景动机>背景动机</a></li><li><a href=#相关工作>相关工作</a><ul><li><a href=#image-adaptive-filtering>Image-adaptive Filtering</a></li><li><a href=#image-super-resolution>Image Super-resolution</a></li><li><a href=#general-purpose-feature-upsampling>General-purpose feature upsampling</a></li><li><a href=#image-adaptive-feature-upsampling>Image-adaptive feature upsampling</a></li></ul></li><li><a href=#具体方法>具体方法</a><ul><li><a href=#下采样器设计>下采样器设计</a></li><li><a href=#上采样器设计>上采样器设计</a></li><li><a href=#其他细节>其他细节</a></li></ul></li><li><a href=#实验结果>实验结果</a></li></ul></nav></div></details></div><ul><li>Author: lartpang</li><li>Link: <a href=https://github.com/lartpang/blog/issues/6 target=_blank>https://github.com/lartpang/blog/issues/6</a></li></ul><h1 id=iclr-2024--featup-a-model-agnostic-framework-for-features-at-any-resolution>ICLR 2024 | FeatUp: A Model-Agnostic Framework for Features at Any Resolution
<a hidden class=anchor href=#iclr-2024--featup-a-model-agnostic-framework-for-features-at-any-resolution><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h1><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711687661665-4150b85f-8f4c-4849-aecf-9031439eb241.png#averageHue=%23f7f5f3&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=176&amp;id=u5a79a7c9&amp;originHeight=220&amp;originWidth=727&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=32319&amp;status=done&amp;style=none&amp;taskId=u5ffc5add-597a-4447-8790-5275da0e6fe&amp;title=&amp;width=581.6" alt=image.png></p><ul><li>论文：<a href=https://arxiv.org/abs/2403.10516 target=_blank>https://arxiv.org/abs/2403.10516</a></li><li>代码：<a href=https://github.com/mhamilton723/FeatUp target=_blank>https://github.com/mhamilton723/FeatUp</a></li></ul><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711687763372-51de525a-0b9f-402c-b70d-431f85752bc6.png#averageHue=%239ec668&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=297&amp;id=uc56f3b36&amp;originHeight=371&amp;originWidth=1042&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=409622&amp;status=done&amp;style=none&amp;taskId=u6b457815-6715-45e7-80e0-73492fdcc6b&amp;title=&amp;width=833.6" alt=image.png></p><h2 id=背景动机>背景动机
<a hidden class=anchor href=#%e8%83%8c%e6%99%af%e5%8a%a8%e6%9c%ba><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>深层特征是计算机视觉研究的基石，捕获图像语义并使社区即使在零或少样本情况下也能解决下游任务。然而，这些特征通常缺乏空间分辨率来直接执行分割和深度预测等密集预测任务，因为模型会积极地池化大区域的信息。在这项工作中引入了 FeatUp，这是一个与任务和模型无关的框架，用于恢复深层特征中丢失的空间信息。</p><h2 id=相关工作>相关工作
<a hidden class=anchor href=#%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><h3 id=image-adaptive-filtering>Image-adaptive Filtering
<a hidden class=anchor href=#image-adaptive-filtering><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p>自适应滤镜通常用于增强图像，同时保留其底层结构和内容。</p><p>例如，双边滤波器（bilateral filters）【Bilateral filtering for gray and color images，The Guided Bilateral Filter: When the Joint/Cross Bilateral Filter Becomes Robust，Fast image dehazing using guided joint bilateral filter】 将空间滤波器应用于低分辨率信号，将图像亮度滤波器应用于高分辨率引导，以混合来自二者的信息。而其重要的扩展形式，联合双边上采样（Joint Bilateral Upsampling，JBU）【Joint bilateral upsampling】，可以在高分辨率引导下对低分辨率信号进行上采样。已成功用于高效图像增强和其他应用。</p><p>最近一些工作嵌入了双边滤波方法【Guided Upsampling Network for Real-Time Semantic Segmentation】和非局部均值（Nonlocal Means）【A Non-Local Algorithm for Image Denoising】方法到卷积网络【Superpixel Convolutional Networks using Bilateral Inceptions，Non-local neural networks】和视觉 transformer【Blending Anti-Aliasing into Vision Transformer，Transformer for Single Image Super-Resolution】中。</p><ul><li>Shape Recipes【Shape recipes: scene representations that refer to the image】学习信号之间的局部关系以创建上采样目标信号。</li><li>Pixel-adaptive convolutional (PAC) 【Pixel-Adaptive Convolutional Neural Networks】使卷积运算适应输入数据，并已用于提高分割【Single-Stage Semantic Segmentation from Image Labels】和单目深度估计【Semantically-Guided Representation Learning for Self-Supervised Monocular Depth，SelfDeco: Self-Supervised Monocular Depth Completion in Challenging Indoor Environments】。</li><li>Spatially-Adaptive Convolution (SAC)【SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation】中的空间自适应卷积将自适应滤波器分解为注意力图和卷积核。</li><li>Bilateral Inception Module【Superpixel Convolutional Networks using Bilateral Inceptions】将双边过滤扩展到超像素，并将此操作嵌入到深层网络中以改进语义分割。</li></ul><p>这类方法在各种应用中都有效，直接将空间信息合并到任务中，同时仍然允许学习网络的灵活性。</p><h3 id=image-super-resolution>Image Super-resolution
<a hidden class=anchor href=#image-super-resolution><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><ul><li>最早的深度无监督超分辨率方法之一是零样本超分辨率 ZSSR【Zero-Shot Super-Resolution Using Deep Internal Learning】，它在测试时学习单图像网络。</li><li>局部隐式模型 LIIF【Learning Continuous Image Representation with Local Implicit Image Function】使用局部自适应模型来插值信息，并已被证明可以提高超分辨率网络的性能。</li><li>深度图像先验 DIP【Deep Image Prior】表明，CNN 可以为零样本图像去噪和超分辨率等逆问题提供归纳偏置。</li></ul><p>尽管有大量关于图像超分辨率的文献，但这些方法并不适合处理超低分辨率但高维的深层特征。</p><h3 id=general-purpose-feature-upsampling>General-purpose feature upsampling
<a hidden class=anchor href=#general-purpose-feature-upsampling><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><ul><li>双线性插值是一种广泛使用的对深度特征图进行上采样的方法。虽然有效，但这种方法模糊了信息，并且对原始图像中的内容或高分辨率结构不敏感。最近邻插值法和双三次插值法也有类似的缺点。</li><li>在更大的输入上评估网络可以实现更高的分辨率，但计算成本很高。此外，由于相对感受野大小的减小，这通常会降低模型性能和语义。</li><li>对于深度卷积网络，一种流行的技术是将最终卷积步长设置为 1【Fully Convolutional Networks for Semantic Segmentation】。然而，这种方法会产生模糊的特征，因为模型的感受野仍然很小（原文是仍然很大，似乎意思不太对）。</li><li>最近使用 visual transformer 的工作【Deep vit features as dense visual descriptors】对输入 patch 步幅进行了类似的修改并插入位置编码。虽然简单有效，但分辨率每增加 2 倍，这种方法就会导致计算占用量急剧增加，因此无法在实践中用于更大的上采样因子。由于前面 patch 的感受野是固定的，这种方法也会扭曲特征。</li></ul><h3 id=image-adaptive-feature-upsampling>Image-adaptive feature upsampling
<a hidden class=anchor href=#image-adaptive-feature-upsampling><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><ul><li>反卷积/转置卷积【Learning Deconvolution Network for Semantic Segmentation】使用学到的卷积核将特征转换到具有更大分辨率的新空间。</li><li>Resize+Convolution【Deconvolution and Checkerboard Artifacts】将学习的卷积附加到确定性上采样过程中，并减少困扰反卷积的棋盘伪影。这一设定是现在图像解码器的常见组件，并已应用于语义分割和超分辨率。</li><li>其他方法，如 IndexNet【Index Networks】和 Affinity-Aware Upsampling (A2U) 【Learning Affinity-Aware Upsampling for Deep Image Matting】在图像抠图方面很有效，但在其他密集预测任务上效果不佳【FADE: Fusing the Assets of Decoder and Encoder for Task-Agnostic Upsampling】。</li><li>Pixel-adaptive convolutional (PAC) 【Pixel-Adaptive Convolutional Neural Networks】、CARAFE【Carafe: Content-aware reassembly of features】、SAPA【Sapa: Similarity-aware point affiliation for feature upsampling】和 DGF【Fast end-to-end trainable guided filter】等方法使用可学习的输入自适应算子来转换特征。
   - PAC 很灵活，但它并没有忠实地对现有特征图进行上采样，而是用于转换下游任务的特征。
   - DGF 通过逐点卷积和线性映射来近似 JBU 操作，但没有完全实现 JBU，因为局部的 query/model 在计算上很难处理。这正是本文通过新的高效 CUDA 核解决的问题。
   - FADE 中引入了一种新的半移位算子，并使用解码器特征来生成联合特征上采样模块。</li><li>Implicit Feature Alignment function (IFA)【Learning implicit feature alignment function for semantic segmentation】从不同的角度（in a different light）看待特征上采样，重点关注最近邻方法，以将编码器 - 解码器架构中的特征图与 IFA 对齐。虽然 IFA 在特定语义分割基准上表现良好，但它没有利用图像引导，也无法在编码 - 解码器框架之外学习高质量表示。</li></ul><h2 id=具体方法>具体方法
<a hidden class=anchor href=#%e5%85%b7%e4%bd%93%e6%96%b9%e6%b3%95><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>FeatUp 背后的核心直觉是，人们可以通过观察低分辨率特征的多个不同“视角”来计算高分辨率特征。</p><p>与 NeRF 通过在场景的许多 2D 照片之间强制一致性来构建 3D 场景的隐式表示一样，FeatUp 通过在许多低分辨率特征图之间强制一致性来构建上采样器，即认为低分辨率信号的多视图一致性可以监督高分辨率信号的构建。更具体地说，通过聚合多个“抖动”（例如翻转、填充、裁剪）图像的模型输出的低分辨率视图来学习高分辨率信息。通过学习具有多视图一致性损失的上采样网络来聚合这些信息。这一基本思想可以产生多种方法。</p><ol><li><strong>基于联合双边上采样的轻量级前向上采样器</strong>：该前馈上采样器是联合双边上采样 (JBU) 滤波器的参数化概括，通过自定义 CUDA 核函数从而比现有实现速度快了几个数量级且占用内存更少。该上采样器可以与几个卷积相当的计算成本产生与对象边缘对齐的高质量特征。</li><li><strong>基于隐式网络的上采样器</strong>：将隐式模型拟合到单个图像以在任何分辨率下重建特征。通过用深度隐式网络过拟合信号，针对目标图像独立学习，允许任意分辨率特征生成，同时具有较低的存储成本。</li></ol><p>在这两种上采样架构的特征可以在下游应用中直接替换使用，因为所提方法不会转换底层特征的语义，即使无需重新训练也能获得分辨率和性能提升。</p><p>文中的实验表明，FeatUp 在类激活图生成、分割和深度预测的迁移学习以及语义分割的端到端训练方面显着优于其他特征上采样和图像超分辨率方法。</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711691913734-48acb9ad-7698-42df-b1d0-f2656a29c3a1.png#averageHue=%23baad74&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=364&amp;id=u22ef2b55&amp;originHeight=455&amp;originWidth=1057&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=464499&amp;status=done&amp;style=none&amp;taskId=ub07dbb88-1510-43c1-baad-aee2100545b&amp;title=&amp;width=845.6" alt=image.png></p><ul><li>第一步是生成低分辨率特征视图，以细化为单个高分辨率输出。为此用小的 padding、尺寸和水平翻转扰乱输入图像，并将模型应用于每个变换后的图像，以提取低分辨率特征图的集合。这些小的图像抖动可以帮助观察输出特征的微小差异，并提供子特征信息来训练上采样器。</li><li>接下来从这些视图构建一致的高分辨率特征图。假设可以学习一个潜在的高分辨率特征图。当下采样时，它可以再现低分辨率的抖动特征。FeatUp 的下采样是光线行进（ray-marching）的直接模拟；正如在此 NeRF 中将 3D 数据渲染为 2D 一样，这里的下采样器将高分辨率特征转换为低分辨率特征。与 NeRF 不同，这里不需要估计生成每个视图的参数。相反，用于 " 抖动 " 每个图像的参数被跟踪，并在下采样之前对学到的高分辨率特征应用相同的转换。然后使用高斯似然损失将下采样特征与真实模型输出进行比较【It Is Likely That Your Loss Should be a Likelihood】。<strong>一个好的高分辨率特征图应该重建所有不同视图中观察到的特征。</strong></li></ul><p>整体的多视角重建损失可以表示如下：（低分辨率特征层面上的一致性）</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711692562385-67c10440-a4ad-4730-85cd-fff407b3e1ea.png#averageHue=%23fbf9f8&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=67&amp;id=u97b2f5ce&amp;originHeight=84&amp;originWidth=672&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=9994&amp;status=done&amp;style=none&amp;taskId=u3a06e6f6-fd82-4762-8490-a4a8f7639d6&amp;title=&amp;width=537.6" alt=image.png></p><ul><li>$t \in T$ 代表这些小变换的集合，$x$ 为输入图像，$f$ 为模型主干。</li><li>$\sigma_{\downarrow}$ 为学到的下采样器，$\sigma_{\uparrow}$ 为学到的上采样器。</li><li>$F_{hr} = \sigma_{\uparrow}(f(x), x)$ 形成预测的高分辨率特征 $F_{hr}$。这种参数化允许 $\sigma_{\uparrow}$ 可以有四种形式：
   - 引导上采样器（取决于 $x$ 和 $f (x)$）；
   - 非引导上采样器（仅取决于 $f (x)$）；
   - 隐式网络（取决于仅 $x$）；
   - 特征的可学习缓存参数（不依赖任何东西，即一套可学习的参数）。</li><li>$|\cdot|$ 是标准的 $l_2$ 范数。</li><li>$s = \mathcal{N}(f(t(x)))$ 是空间变化的自适应不确定性【It Is Likely That Your Loss Should be a Likelihood】。这将 MSE 损失转化为能够处理不确定性的更合适的似然。这种额外的灵活性允许网络在某些异常特征从根本上无法进行上采样时进行学习。</li></ul><h3 id=下采样器设计>下采样器设计
<a hidden class=anchor href=#%e4%b8%8b%e9%87%87%e6%a0%b7%e5%99%a8%e8%ae%be%e8%ae%a1><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711692000614-fac1a2b3-1130-4475-a132-c2e58b6488b2.png#averageHue=%23dad393&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=260&amp;id=u0338e6cf&amp;originHeight=325&amp;originWidth=1071&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=327337&amp;status=done&amp;style=none&amp;taskId=u8100abeb-9284-4eb8-bf57-35afdc00ca3&amp;title=&amp;width=856.8" alt=image.png></p><p>这里引入两个选项，即快速且简单的<strong>学到的模糊核</strong>，更灵活的<strong>基于注意力的下采样器</strong>。两个提出的模块都不会通过特殊变换来改变特征的 " 空间 " 或 " 语义 &ldquo;，而只是在一个小邻域内插入特征。图 3 中绘制了这两种选择。</p><p>两个下采样器的主要超参数是核大小，对于具有较大感受野的模型（例如卷积网络），内核大小应该更大。</p><h4 id=简单的下采样器>简单的下采样器
<a hidden class=anchor href=#%e7%ae%80%e5%8d%95%e7%9a%84%e4%b8%8b%e9%87%87%e6%a0%b7%e5%99%a8><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h4><p>通过学到的模糊核来模糊特征，可以实现为独立应用于每个通道的卷积。学习到的核被归一化为非负且总和为 1，以确保特征保持在同一空间中。尽管这种基于模糊的下采样器非常高效，但它无法捕获动态感受野、对象显着性或其他非线性效应。</p><h4 id=更灵活的注意力下采样器>更灵活的注意力下采样器
<a hidden class=anchor href=#%e6%9b%b4%e7%81%b5%e6%b4%bb%e7%9a%84%e6%b3%a8%e6%84%8f%e5%8a%9b%e4%b8%8b%e9%87%87%e6%a0%b7%e5%99%a8><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h4><p>可以在空间上调整下采样核。该组件使用 1x1 卷积根据高分辨率特征预测显着性图。它将显着性图与学到的空间不变权重和偏置核相结合，并对归一化结果以创建插入特征的空间变化模糊核：</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711693555180-e6847e68-0722-4544-bea3-2133e794bc26.png#averageHue=%23f8f5f2&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=38&amp;id=u00f3e09d&amp;originHeight=47&amp;originWidth=630&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=8683&amp;status=done&amp;style=none&amp;taskId=u27db642e-6505-4ceb-979e-0329d63cefc&amp;title=&amp;width=504" alt=image.png></p><ul><li>$\sigma_\downarrow(F)_{ij}$ 是结果特征图的第 i, j 个分量。</li><li>$F_{hr}[\Omega_{ij}]$ 是指与下采样特征中的 i, j 位置相对应的高分辨率特征块。</li><li>$\odot$ 和 $\cdot$ 分别指元素级乘积和向量内积。</li><li>$w$ 和 $b$ 是所有补丁共享的学习权重和偏置核。</li></ul><h3 id=上采样器设计>上采样器设计
<a hidden class=anchor href=#%e4%b8%8a%e9%87%87%e6%a0%b7%e5%99%a8%e8%ae%be%e8%ae%a1><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711691984643-44342823-5264-4c8b-90e7-125d9a99f978.png#averageHue=%23ada775&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=200&amp;id=u76a38696&amp;originHeight=250&amp;originWidth=1065&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=263580&amp;status=done&amp;style=none&amp;taskId=u6cf8cd91-6ab8-46dd-93b7-6ce5703ac8f&amp;title=&amp;width=852" alt=image.png></p><p>图中展示了本文设计的两种方法，二者都使用相同的更广泛的架构和损失进行训练。</p><h4 id=基于堆叠的联合双边上采样器的引导上采样器>基于堆叠的联合双边上采样器的引导上采样器
<a hidden class=anchor href=#%e5%9f%ba%e4%ba%8e%e5%a0%86%e5%8f%a0%e7%9a%84%e8%81%94%e5%90%88%e5%8f%8c%e8%be%b9%e4%b8%8a%e9%87%87%e6%a0%b7%e5%99%a8%e7%9a%84%e5%bc%95%e5%af%bc%e4%b8%8a%e9%87%87%e6%a0%b7%e5%99%a8><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h4><p>该架构学习了一种跨图像语料泛化的上采样策略。</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711696798085-7d76143a-2d12-4df4-96e2-763754a28658.png#averageHue=%23f6f3ef&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=28&amp;id=uc719991e&amp;originHeight=35&amp;originWidth=450&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=5734&amp;status=done&amp;style=none&amp;taskId=u562690fb-2867-4f40-893c-50b3f724deb&amp;title=&amp;width=360" alt=image.png></p><p>$\circ$ 是函数组合。$f(x)$ 是低分辨率特征图。$x$ 是原始图像。</p><p>该架构速度快，直接将输入图像中的高频细节合并到上采样过程中，并且独立于 $f$ 的架构。</p><p>公式将原始 JBU 的实现推广到高维信号，并使该操作可学习。在联合双边上采样中，使用高分辨率信号 $G$ 作为低分辨率特征 $F_{lr}$ 的引导。$\Omega$ 是每个像素在引导图像中的邻域。</p><p>在实践中，我们使用以每个像素为中心的 3×3 正方形。令 $k(\cdot,\cdot)$ 为相似核，用于衡量两个向量的 " 接近 " 程度。然后我们可以形成联合双边过滤器：</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711698212400-0db76b8a-b619-4929-b5d1-c8e59335236c.png#averageHue=%23faf9f6&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=62&amp;id=ue6bce8f0&amp;originHeight=77&amp;originWidth=854&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=16963&amp;status=done&amp;style=none&amp;taskId=u3d3c82ba-4dce-4793-8396-790c8b648c4&amp;title=&amp;width=683.2" alt=image.png></p><blockquote><p>原始的 JBU 形式：</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711699170852-ae331600-0152-4865-b2a2-9607190155d2.png#averageHue=%23f3f3f3&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=68&amp;id=uad1f55af&amp;originHeight=85&amp;originWidth=439&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=6087&amp;status=done&amp;style=none&amp;taskId=u837338dd-148c-40d1-b9b9-9b912b47947&amp;title=&amp;width=351.2" alt=image.png></p></blockquote><p>其中的 $Z$ 是归一化因子，确保核参数加和为 1。spatial 核是向量坐标之间欧氏距离上宽度为 $\sigma_{spatial}$ 的可学习的高斯核：</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711698320299-3f77a4d6-1bdc-4709-a75e-c961aa580593.png#averageHue=%23f9f7f5&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=72&amp;id=u50691ab7&amp;originHeight=90&amp;originWidth=396&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=8181&amp;status=done&amp;style=none&amp;taskId=u39ab9490-c1c6-4ef8-9a2d-1daba33e935&amp;title=&amp;width=316.8" alt=image.png></p><p>此外，range 核是对引导信号 $G$ 进行操作的多层感知器 (MLP) 输出内积施加温度 $\sigma_{range}^2$ 的 softmax 的形式：</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711698464067-3f7bb00a-b9e4-4171-a9ff-5bc7af841670.png#averageHue=%23fbf9f8&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=68&amp;id=u5204aea2&amp;originHeight=85&amp;originWidth=791&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=13784&amp;status=done&amp;style=none&amp;taskId=ufd2d1836-1256-4577-97f4-8eca31551f3&amp;title=&amp;width=632.8" alt=image.png></p><p>原始 JBU 在引导信号 G 上使用固定的高斯核。而这里设计的泛化性能要好得多，因为可以从数据中学习 MLP 以创建更好的上采样器。在实验中使用具有 30 维隐藏向量和输出向量的两层 GeLU MLP。为了评估 $F_{lr}[a, b]$，如果引导像素不直接与低分辨率特征对齐，则遵循原始 JBU 公式使用双线性插值特征。为了与分辨率无关，这里在空间核中使用归一化到 [−1, 1] 的坐标距离。</p><p>这一形式的挑战之一是现有 JBU 实现的速度和内存性能较差。这可以解释为什么这种简单的方法没有得到更广泛的使用。为此，本文为 JBU 中使用的空间自适应内核提供了高效的 CUDA 实现。与使用 <code>torch.nn.Unfold</code> 运算符的简单实现相比，改进操作使用的内存减少了两个数量级，并且推理速度提高了 10 倍。</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711699584968-f271b07d-2aa0-4268-a2cb-63d7c7468554.png#averageHue=%23f4f2f0&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=381&amp;id=ubbb9ac9c&amp;originHeight=476&amp;originWidth=600&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=76405&amp;status=done&amp;style=none&amp;taskId=uaa109fe1-8383-45f4-918d-68b16337c1d&amp;title=&amp;width=480" alt=image.png></p><h4 id=基于隐式神经网络的图像特定的上采样器>基于隐式神经网络的图像特定的上采样器
<a hidden class=anchor href=#%e5%9f%ba%e4%ba%8e%e9%9a%90%e5%bc%8f%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%9b%be%e5%83%8f%e7%89%b9%e5%ae%9a%e7%9a%84%e4%b8%8a%e9%87%87%e6%a0%b7%e5%99%a8><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h4><p>这在过度拟合单个图像时可以产生非常清晰的特征。通过使用隐式函数 $F_{hr}=MLP(z)$ 参数化单个图像的高分辨率特征。一些现有的上采样解决方案也采用这种推理时训练方法，包括 DIP 和 LIIF。这里使用小型 MLP 将图像坐标和强度映射到给定位置的高维特征。遵循先前工作【Implicit Neural Representations with Periodic Activation Functions】的指导，并使用傅里叶特征来提高隐式表示的空间分辨率。除了标准傅里叶位置特征之外，本文也表明添加傅里叶颜色特征允许网络使用原始图像中的高频颜色信息。这显着加快了收敛速度，并能够优雅地使用高分辨率图像信息，而无需使用条件随机场 (CRF) 等技术。</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711701321483-4a112faa-9482-4c70-8437-94e3159865a0.png#averageHue=%23f8f6f4&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=40&amp;id=u37e5dca9&amp;originHeight=50&amp;originWidth=396&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=5540&amp;status=done&amp;style=none&amp;taskId=u7d6cbc87-634a-4741-9f86-4ab65db9079&amp;title=&amp;width=316.8" alt=image.png></p><p>$h(z,\hat{\omega})$ 表示输入信号 $z$ 的各个成分的离散傅立叶变换，其中频率向量为 $\hat{\omega}$。$e_i$ 和 $e_j$ 表示范围在区间 [−1, 1] 内的二维像素坐标场。$:$ 表示沿通道维度的拼接。</p><p>这里的 MLP 是一个小型 3 层 ReLU 网络，具有 dropout（$p = 0.1$）和层归一化。</p><p>这一设定可以使得测试时可以查询像素坐标场以产生任何分辨率下的特征。而隐式表示中的参数数量比一个 224×224 的显式表征小两个数量级以上，同时更具表现力，显着减少收敛时间和存储大小。</p><h3 id=其他细节>其他细节
<a hidden class=anchor href=#%e5%85%b6%e4%bb%96%e7%bb%86%e8%8a%82><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><h4 id=使用特征压缩加速训练>使用特征压缩加速训练
<a hidden class=anchor href=#%e4%bd%bf%e7%94%a8%e7%89%b9%e5%be%81%e5%8e%8b%e7%bc%a9%e5%8a%a0%e9%80%9f%e8%ae%ad%e7%bb%83><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h4><ul><li>为了减少内存占用并进一步加速 FeatUp 隐式网络的训练，本文首先将空间变化的特征压缩到其最大的 $k=128$ 个主成分。此操作几乎是无损的，因为前 128 个分量解释了单个图像特征中 96% 的方差。这将 ResNet-50 的训练时间缩短了 60 倍，减少了内存占用，支持更大的批次，并且对学习的特征质量没有任何明显的影响。</li><li>在训练 JBU 上采样器时，在每个批次中对随机投影矩阵进行采样来降维，以避免在内循环中计算 PCA。得益于 Johnson–Lindenstrauss 引理 (<a href=https://spaces.ac.cn/archives/8679 target=_blank>让人惊叹的Johnson-Lindenstrauss引理：理论篇 - 科学空间|Scientific Spaces</a>
)，这可以达到相同的效果。</li></ul><h4 id=总变分先验>总变分先验
<a hidden class=anchor href=#%e6%80%bb%e5%8f%98%e5%88%86%e5%85%88%e9%aa%8c><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h4><p>为了避免高分辨率特征中的杂散噪声，在隐式特征量值上添加一个小的 ( $\lambda_{tv}=0.05$) 总变分平滑先验：</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711702121526-d7d9ad8c-8202-42bb-95b2-37dae3721697.png#averageHue=%23fbf9f8&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=76&amp;id=uefd7d0a9&amp;originHeight=95&amp;originWidth=1019&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=13623&amp;status=done&amp;style=none&amp;taskId=u064a9655-1c13-41d4-be00-440bd7761ff&amp;title=&amp;width=815.2" alt=image.png></p><p>这比正则化完整特征更快，并且避免过度规定各个组件的组织方式。注意不在 JBU 上采样器中使用它，因为此时不会受到过度拟合的影响。</p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711702259391-ce8ad37b-33cb-4d9f-92c0-ec6aa20d2515.png#averageHue=%23a9ba53&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=312&amp;id=uf19a696f&amp;originHeight=390&amp;originWidth=739&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=404532&amp;status=done&amp;style=none&amp;taskId=ub2e0f49e-89df-424d-b7fe-63e59efacb0&amp;title=&amp;width=591.2" alt=image.png></p><h2 id=实验结果>实验结果
<a hidden class=anchor href=#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711702380147-7d7235a7-3315-45fc-bc59-9f9e642b6755.png#averageHue=%23f3f1ee&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=362&amp;id=u30ec7daa&amp;originHeight=452&amp;originWidth=1002&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=118588&amp;status=done&amp;style=none&amp;taskId=u9cc0d9e4-76f8-42a0-8bb3-9738232f7f7&amp;title=&amp;width=801.6" alt=image.png></p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711702468645-ada1f314-0f5a-450f-a802-eba4724d2eac.png#averageHue=%23f1eeeb&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=300&amp;id=ud325a84a&amp;originHeight=375&amp;originWidth=996&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=102432&amp;status=done&amp;style=none&amp;taskId=u27a06620-4618-49d6-927a-0e18b71d855&amp;title=&amp;width=796.8" alt=image.png></p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711702420331-3aafe050-a2b3-46b5-8481-f9f1076702a8.png#averageHue=%235ba863&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=502&amp;id=u1e7c3327&amp;originHeight=627&amp;originWidth=1000&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=1213218&amp;status=done&amp;style=none&amp;taskId=u2481f5be-5816-4402-be17-c3ff74c108c&amp;title=&amp;width=800" alt=image.png></p><p><img loading=lazy src="https://cdn.nlark.com/yuque/0/2024/png/192314/1711702446378-5db872df-a350-40c4-9cbe-6a055f78f668.png#averageHue=%235ac04e&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=281&amp;id=ue7517094&amp;originHeight=351&amp;originWidth=999&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=537466&amp;status=done&amp;style=none&amp;taskId=u800d01d7-1be5-4e58-90e9-249c9cd2c7b&amp;title=&amp;width=799.2" alt=image.png></p><nav class=pagenav><a class=prev href=/blog/posts/0017-cvpr-2024---rethinking-the-up-sampling-operations-in-cnn-based-generative-network-for-generalizable-deepfake-detection/><span class=direction>« Prev</span><br><span>CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection </span></a><a class=next href=/blog/posts/0019-arixv-2403---parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey/><span class=direction>Next »</span><br><span>Arixv 2403 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey</span></a></nav></main><footer><div class=copyright>© 2025 Lart Pang</div><div class=attribution>Built with <a href=https://gohugo.io target=_blank>Hugo</a> & <a href=https://github.com/mnjm/kayal target=_blank>Kayal</a></div><div>Last update Jul 17, 2025</div></footer></div><script defer type=text/javascript src=/blog/js/theme.min.eaccfb587dc18aa6c89ddcfe908efce320a265a23826cb3af2032a3f425a36f9b12c233ec2743b2c1017217c7699a7cc7619483ea2257f88b2b63b7b51bbaaa8.js integrity="sha512-6sz7WH3BiqbIndz+kI784yCiZaI4Jss68gMqP0JaNvmxLCM+wnQ7LBAXIXx2mafMdhlIPqIlf4iytjt7UbuqqA=="></script><script defer type=text/javascript src=/blog/js/codecopy.min.6319d0ad3eae498a0591b4ec1d52cc0156f5c3cb95390089d7d0ac7fe6dfb183161c14f935d25b5ad896840e2a9d6a23f555a7976954d0a263c24a11272aa747.js integrity="sha512-YxnQrT6uSYoFkbTsHVLMAVb1w8uVOQCJ19Csf+bfsYMWHBT5NdJbWtiWhA4qnWoj9VWnl2lU0KJjwkoRJyqnRw=="></script><script defer type=text/javascript src=/blog/js/search.min.ad22883b43082f62bc508d1f4e145eb2c6192213144784eadc12e3144ab7af50f826fde65c4a45a68d381b1335f3ff2c189221533436acdffe2c5abd589ce6b8.js integrity="sha512-rSKIO0MIL2K8UI0fThRessYZIhMUR4Tq3BLjFEq3r1D4Jv3mXEpFpo04GxM18/8sGJIhUzQ2rN/+LFq9WJzmuA=="></script></body></html>