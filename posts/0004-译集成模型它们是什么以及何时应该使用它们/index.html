<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta http-equiv=content-language content="en-us"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>【译】集成模型：它们是什么以及何时应该使用它们？ &#183; 啊，哈！</title>
<meta name=title content="【译】集成模型：它们是什么以及何时应该使用它们？ &#183; 啊，哈！"><meta name=description content="Just for fun."><meta name=keywords content="machine learning,"><link rel=canonical href=https://lartpang.github.io/blog/posts/0004-%E8%AF%91%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%83%E4%BB%AC%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%BD%95%E6%97%B6%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC/><link type=text/css rel=stylesheet href=/blog/css/main.bundle.min.01dde6ff86a3d23e6258a134912503c50666955af42c932ac894e1b1bd4644de893c078daf8e692391173d9351000cbc001451e378e0a8ccc240295b9ed5a0f8.css integrity="sha512-Ad3m/4aj0j5iWKE0kSUDxQZmlVr0LJMqyJThsb1GRN6JPAeNr45pI5EXPZNRAAy8ABRR43jgqMzCQClbntWg+A=="><link rel=apple-touch-icon sizes=180x180 href=/blog/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/blog/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/blog/favicon-16x16.png><link rel=manifest href=/blog/site.webmanifest><meta property="og:url" content="https://lartpang.github.io/blog/posts/0004-%E8%AF%91%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%83%E4%BB%AC%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%BD%95%E6%97%B6%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC/"><meta property="og:site_name" content="啊，哈！"><meta property="og:title" content="【译】集成模型：它们是什么以及何时应该使用它们？"><meta property="og:description" content="Just for fun."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-25T08:32:08+00:00"><meta property="article:modified_time" content="2024-07-25T08:32:08+00:00"><meta property="article:tag" content="Machine Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="【译】集成模型：它们是什么以及何时应该使用它们？"><meta name=twitter:description content="Just for fun."><script defer type=text/javascript src=/blog/lib/fuse/fuse.min.1f56d60a7738743270762a2aa4e0a453be99c4476f06f3b2f51f4377d12a7ed973420f76a308d7e4855d88ec34d25940c9015464934faf30e7599fe566d7f6e4.js integrity="sha512-H1bWCnc4dDJwdioqpOCkU76ZxEdvBvOy9R9Dd9EqftlzQg92owjX5IVdiOw00llAyQFUZJNPrzDnWZ/lZtf25A=="></script><link rel=preload href=https://lartpang.github.io/blog/fonts/Ubuntu-Regular.woff2 as=font type=font/woff2 crossorigin><script>const siteTheme="auto";let savedTheme=localStorage.getItem("theme")||siteTheme;savedTheme=="auto"&&window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches&&(savedTheme="dark"),savedTheme==="dark"&&(document.documentElement.classList.add("dark"),localStorage.setItem("theme","dark"))</script></head><body><div class=content><header><a class=title href=/blog/><img loading=lazy src=https://avatars.githubusercontent.com/u/26847524 alt="Site Logo"></a><div class=header-cntr><a class=title href=/blog/><span>啊，哈！</span></a><div class=menu><nav id=main-menu class=mm-normal><ul><li><button id=mob-x-icon class=menu-btn arial-label=x-icon><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></button></li><li><a href=/blog/posts/ aria-label=posts>Posts</li></a><li><a href=/blog/tags/ aria-label=tags>Tags</li></a><li><a href=/blog/about/ aria-label=about>About</li></a><li><a href=https://github.com/lartpang aria-label=github><span><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></li></a></ul></nav><div class=side-menu><button id=search-open class=menu-btn aria-label="Search button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button><div id=search-container data-url=https://lartpang.github.io/blog/><div class=search><div class=panel><form><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
<input type=search id=search-query placeholder=Search tabindex=0 autocomplete=off>
<button id=search-close title="Cancel (ESC)"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></button></form></div><ul id=search-results></ul><div id=search-overlay></div></div></div><button id=theme-switcher class=menu-btn aria-label="Theme switcher"><div id=moon><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></div><div id=sun><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></div></button></div><button id=mob-hb-icon class=menu-btn aria-label="Hamburger icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></button></div><script>(function(){document.addEventListener("DOMContentLoaded",function(){const s=document.getElementById("mob-hb-icon"),e=document.getElementById("mob-x-icon"),t=document.getElementById("main-menu"),n=document.body;s.addEventListener("click",function(){t.classList.replace("mm-normal","mm-mobile-open"),e.style.display="block",n.style.overflow="hidden"}),e.addEventListener("click",function(){t.classList.replace("mm-mobile-open","mm-normal"),e.style.display="none",n.style.overflow=""})})})()</script><script>(function(){var t,n,e=document.getElementById("main-menu");if(!e)return;t=window.location.pathname,n=e.querySelectorAll("a"),n.forEach(function(e){e.getAttribute("href")===t&&e.classList.add("active")})})()</script></div></header><main><ul class=breadcrumbs><li><a href=https://lartpang.github.io/blog/><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1v16.2c0 22.1-17.9 40-40 40h-16c-1.1.0-2.2.0-3.3-.1-1.4.1-2.8.1-4.2.1L416 512h-24c-22.1.0-40-17.9-40-40v-24-64c0-17.7-14.3-32-32-32h-64c-17.7.0-32 14.3-32 32v64 24c0 22.1-17.9 40-40 40h-24-31.9c-1.5.0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1.0-40-17.9-40-40V360c0-.9.0-1.9.1-2.8v-69.7h-32c-18 0-32-14-32-32.1.0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7L564.8 231.5c8 7 12 15 11 24z"/></svg></a></li><li><a href=https://lartpang.github.io/blog/posts/>Posts</a></li></ul><h1 class=pg-title>【译】集成模型：它们是什么以及何时应该使用它们？</h1><div class=meta><p><span class=meta-icon><svg fill="currentcolor" width="800" height="800" viewBox="0 0 24 24"><path d="M19 4H17V3a1 1 0 00-2 0V4H9V3A1 1 0 007 3V4H5A3 3 0 002 7V19a3 3 0 003 3H19a3 3 0 003-3V7A3 3 0 0019 4zm1 15a1 1 0 01-1 1H5a1 1 0 01-1-1V12H20zm0-9H4V7A1 1 0 015 6H7V7A1 1 0 009 7V6h6V7a1 1 0 002 0V6h2a1 1 0 011 1z"/></svg>
</span>Posted on <time datetime=2024-07-25T08:32:08+00:00>Jul 25, 2024</time>
<span class=meta-icon><svg width="800" height="800" viewBox="0 0 24 24" fill="currentcolor"><path d="M23 12c0 6.0751-4.9249 11-11 11C5.92487 23 1 18.0751 1 12 1 5.92487 5.92487 1 12 1c6.0751.0 11 4.92487 11 11zM3.00683 12c0 4.9668 4.02638 8.9932 8.99317 8.9932 4.9668.0 8.9932-4.0264 8.9932-8.9932.0-4.96679-4.0264-8.99317-8.9932-8.99317-4.96679.0-8.99317 4.02638-8.99317 8.99317z" fill="currentcolor"/><path d="M12 5C11.4477 5 11 5.44771 11 6v6.4667S11 12.7274 11.1267 12.9235C11.2115 13.0898 11.3437 13.2343 11.5174 13.3346l4.6198 2.6673C16.6155 16.278 17.2271 16.1141 17.5032 15.6358 17.7793 15.1575 17.6155 14.5459 17.1372 14.2698L13 11.8812V6C13 5.44772 12.5523 5 12 5z" fill="currentcolor"/></svg>
</span>2 mins</p><p><span class=meta-icon><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M48 32H197.5c17 0 33.2 6.74 45.2 18.75l176 175.95c25 25 25 65.6.0 90.6L285.3 450.7c-25 25-65.6 25-90.6.0L18.75 274.7C6.743 262.7.0 246.5.0 229.5V80C0 53.49 21.49 32 48 32zm64 144c17.7.0 32-14.3 32-32 0-17.7-14.3-32-32-32-17.67.0-32 14.3-32 32s14.33 32 32 32z"/></svg>
</span><a class=tag href=/blog/tags/machine-learning/>Machine Learning</a></p></div><div class=toc><details><summary accesskey=c title=(Alt+C)>Table of Contents</summary><div class=toc-innr><nav id=TableOfContents><ul><li><a href=#bagging>Bagging</a><ul><li><a href=#bootstrap-aggregation-bagging>Bootstrap Aggregation (Bagging)</a></li><li><a href=#随机森林-random-forest>随机森林 (Random Forest)</a></li><li><a href=#extra-trees-ensemble>Extra-Trees Ensemble</a></li></ul></li><li><a href=#boosting>Boosting</a><ul><li><a href=#adaptive-boosting-adaboost>Adaptive Boosting (AdaBoost)</a></li><li><a href=#顺序决策树-sequential-decision-trees>顺序决策树 (Sequential Decision Trees)</a></li><li><a href=#梯度提升-gradient-boosting>梯度提升 (Gradient Boosting)</a></li></ul></li><li><a href=#stacking>Stacking</a></li><li><a href=#blending>Blending</a></li><li><a href=#分类问题>分类问题</a></li><li><a href=#回归问题>回归问题</a></li><li><a href=#聚合预测>聚合预测</a></li><li><a href=#如何构建集成模型示例>如何构建集成模型：示例</a><ul><li><a href=#random-forest-classifier>Random Forest Classifier</a></li><li><a href=#extra-trees-classifier>Extra Trees Classifier</a></li><li><a href=#adaboost-classifier>AdaBoost Classifier</a></li><li><a href=#xgboost-classifier>XGBoost Classifier</a></li></ul></li></ul><ul><li><a href=#仔细考虑>仔细考虑</a></li></ul></nav></div></details></div><ul><li>Author: lartpang</li><li>Link: <a href=https://github.com/lartpang/blog/issues/20 target=_blank>https://github.com/lartpang/blog/issues/20</a></li></ul><blockquote><p>[!IMPORTANT]
原文：<a href=https://builtin.com/machine-learning/ensemble-model target=_blank>https://builtin.com/machine-learning/ensemble-model</a></p></blockquote><blockquote><p><em>有时，一个模型是不够的。在这篇集成模型指南中，我将向您介绍如何（以及何时）对机器学习模型使用集成技术。</em></p></blockquote><p>每当我们试图做出重要决定时，我们都会尝试收集尽可能多的信息，并向专家寻求建议。我们可以收集的信息越多，我们（和我们周围的人）就越信任决策过程。</p><p>机器学习预测遵循类似的行为。模型处理给定的输入并产生结果。结果是根据模型在训练过程中看到的模式进行预测。</p><p>在许多情况下，一个模型是不够的，本文阐明了这一点。我们何时以及为何需要多个模型？我们如何训练这些模型？这些模型应该提供什么样的多样性？所以，让我们直接进入。</p><p><em>如果你想看一个如何构建集成模型的例子，你可以跳到最后！</em></p><blockquote><p>[!NOTE]
<strong>什么是集成模型？</strong></p><p>集成模型是一种机器学习方法，用于在预测过程中结合多个其他模型。这些模型称为基础估计器。集成模型提供了一种解决方案，可以克服构建单个估计器的技术挑战。</p></blockquote><p>构建单一估计器的技术挑战包括：</p><ul><li>高方差：模型对为学习特征提供的输入非常敏感。</li><li>准确性低：一个模型（或一种算法）无法拟合整个训练数据，可能无法为您提供项目所需的细微差别。</li><li>具有噪声和偏差的特征：模型在进行预测时严重依赖于太少的特征。</li></ul><h1 id=集成算法>集成算法
<a hidden class=anchor href=#%e9%9b%86%e6%88%90%e7%ae%97%e6%b3%95><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h1><p>单一算法可能无法对给定的数据集做出完美的预测。机器学习算法有其局限性，生成高精度的模型具有挑战性。如果我们构建并组合多个模型，我们就有机会提高整体准确性。然后，我们通过将每个模型的输出与两个目标聚合来实现模型组合：</p><ul><li>减少模型误差</li><li>保持模型的泛化</li></ul><p>您可以使用不同的技术（有时称为元算法）来实现此类聚合。</p><p><img loading=lazy src=http://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_ensemble-model.png alt=集成模型></p><p><em>图 1：使用多种算法使模型预测多样化</em></p><h1 id=集成学习>集成学习
<a hidden class=anchor href=#%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h1><p>当我们构建集成模型时，我们不仅要关注算法的方差。例如，我们可以构建多个 C45 模型，其中每个模型都在学习专门用于预测任何给定事物的特定模式。我们可以用来获得元模型的模型称为弱学习器。在这种集成学习架构中，输入被传递给每个弱学习器，同时还收集他们的预测。我们可以使用组合预测来构建最终的集成模型。</p><p>值得一提的是，弱学习者可以采用不同的方式将特征与变体决策边界进行映射。</p><p><img loading=lazy src=http://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/2_ensemble-model.png alt=集成模型></p><p><em>图 2：使用同一算法的多个弱学习器的聚合预测</em></p><blockquote><p>[!NOTE]
集成建模技术的类型</p><ul><li>Bagging</li><li>Boosting</li><li>Stacking</li><li>Blending</li></ul></blockquote><h1 id=集成技术>集成技术
<a hidden class=anchor href=#%e9%9b%86%e6%88%90%e6%8a%80%e6%9c%af><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h1><h2 id=bagging>Bagging
<a hidden class=anchor href=#bagging><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>bagging 的想法是基于使训练数据可用于迭代学习过程。每个模型都使用训练数据集的略有不同的子集来学习前一个模型产生的误差。bagging 可减少 <a href=https://builtin.com/data-science/bias-variance-tradeoff target=_blank>方差</a>
并最大程度地减少 <a href=https://builtin.com/data-science/model-fit target=_blank>过拟合</a>
。这种技术的一个例子是 <a href=https://builtin.com/data-science/random-forest-python target=_blank>随机森林</a>
算法。</p><h3 id=bootstrap-aggregation-bagging>Bootstrap Aggregation (Bagging)
<a hidden class=anchor href=#bootstrap-aggregation-bagging><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p>此技术基于自举采样技术 (bootstrapping sampling technique)。bootstrapping 会创建多组原始训练数据并进行替换。替换允许在一组中复制样本实例。每个子集都具有相同的相等大小，可用于并行训练模型。</p><p><img loading=lazy src=http://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/3_ensemble-model.png alt=集成模型></p><p><em>图 3：通过结合来自多个模型的预测来做出最终预测的 Bagging 技术</em></p><h3 id=随机森林-random-forest>随机森林 (Random Forest)
<a hidden class=anchor href=#%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97-random-forest><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p>此技术使用训练样本的子集以及特征的子集来构建多个分割树。构建了多个决策树以适应每个训练集。样本/特征的分布通常以随机模式实现。</p><h3 id=extra-trees-ensemble>Extra-Trees Ensemble
<a hidden class=anchor href=#extra-trees-ensemble><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p>这是另一种集成技术，其中来自许多 <a href=https://builtin.com/data-science/classification-tree target=_blank>决策树</a>
的预测组合在一起。与随机森林类似，它结合了大量的决策树。但是，额外的树在随机选择分割时使用整个样本。</p><blockquote><p>[!TIP]
相关阅读:</p><ul><li><a href=https://builtin.com/data-science/random-forest-python target=_blank>在 Python 中实现随机森林回归：简介</a>
<a href=https://builtin.com/data-science/random-forest-python-deep-dive target=_blank>深入了解在 Python 中实现随机森林分类</a></li></ul></blockquote><h2 id=boosting>Boosting
<a hidden class=anchor href=#boosting><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><h3 id=adaptive-boosting-adaboost>Adaptive Boosting (AdaBoost)
<a hidden class=anchor href=#adaptive-boosting-adaboost><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p>这是一个算法集合，我们在 <a href=https://www.sciencedirect.com/science/article/pii/S002200009791504X target=_blank>几个弱学习器</a>
的顶部构建模型。正如我们前面提到的，这些学习器被称为弱学习器，因为它们通常很简单，预测能力有限。AdaBoost 的适应能力使该技术成为最早成功的二元分类器之一。</p><h3 id=顺序决策树-sequential-decision-trees>顺序决策树 (Sequential Decision Trees)
<a hidden class=anchor href=#%e9%a1%ba%e5%ba%8f%e5%86%b3%e7%ad%96%e6%a0%91-sequential-decision-trees><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p>这些是这种适应性的核心，每棵树都根据先验的精度知识调整其权重。因此，我们以顺序（而不是并行）过程执行这种技术的训练。在这种技术中，对于给定的迭代次数或当错误率没有显着变化时，可以重复训练和测量估计误差的过程。</p><p><img loading=lazy src=http://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/4_ensemble-model.png alt=集成模型></p><p><em>图 4：AdaBoost 的顺序学习产生更强的学习模型</em></p><h3 id=梯度提升-gradient-boosting>梯度提升 (Gradient Boosting)
<a hidden class=anchor href=#%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87-gradient-boosting><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><p>梯度提升算法是具有高预测性能的出色技术。 <a href=https://dl.acm.org/doi/10.1145/2939672.2939785 target=_blank>Xgboost</a>
、<a href=https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf target=_blank>LightGBM</a>
和 CatBoost 是可用于回归和分类问题的常用提升算法。在他们证明有能力 <a href=https://www.are.na/block/952502 target=_blank>赢得一些 Kaggle 比赛</a>
后，他们的受欢迎程度显着增加。</p><h2 id=stacking>Stacking
<a hidden class=anchor href=#stacking><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>堆叠类似于提升模型; 它们产生更稳健的预测因子。堆叠是一个学习如何从所有弱学习者的预测中创建这样一个更强大的模型的过程。</p><p><img loading=lazy src=http://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/5_ensemble-model.png alt=集成模型></p><p><em>图 5：用于在集成架构中进行最终预测的堆叠技术</em></p><p>请注意，该算法正在从每个模型（作为特征）中学习预测。</p><p><a href="https://www.youtube.com/watch?v=Un9zObFjBH0" target=_blank>Ensemble Learners Tutorial | Video: Udacity</a></p><h2 id=blending>Blending
<a hidden class=anchor href=#blending><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>blending 类似于 stacking 方法，不同之处在于最终模型正在学习验证和测试数据集以及预测。因此，所使用的特征被扩展为包括验证集。</p><h2 id=分类问题>分类问题
<a hidden class=anchor href=#%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>分类只是一个分类过程。如果我们有多个标签，我们需要决定：我们是否应该构建一个单一的多标签分类器？或者我们是否应该构建多个二元分类器？如果我们决定构建多个二元分类器，我们需要解释每个模型预测。例如，如果我们想要识别四个对象，每个模型都会告诉您输入数据是否是该类别的成员。因此，每个模型都提供了隶属的概率。同样，我们可以构建一个结合这些分类器的最终集成模型。</p><h2 id=回归问题>回归问题
<a hidden class=anchor href=#%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>在上一个函数中，我们使用所得概率确定了最佳拟合隶属度。在回归问题中，我们不是在处理“是”或“否”的问题。相反，我们需要找到最佳预测的数值，然后我们可以对收集的预测值进行平均。</p><h2 id=聚合预测>聚合预测
<a hidden class=anchor href=#%e8%81%9a%e5%90%88%e9%a2%84%e6%b5%8b><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>当我们集成多个算法以调整预测过程以组合多个模型时，我们需要一种聚合方法。我们可以使用三种主要技术：</p><ul><li>最大投票：此技术的最终预测是基于对分类问题的多数投票做出的。</li><li>平均：此技术通常用于对预测进行平均的回归问题。我们也可以使用概率，例如，通过对最终分类进行平均。</li><li>加权平均：有时，在生成最终预测时，我们需要为某些模型/算法赋予权重。</li></ul><h2 id=如何构建集成模型示例>如何构建集成模型：示例
<a hidden class=anchor href=#%e5%a6%82%e4%bd%95%e6%9e%84%e5%bb%ba%e9%9b%86%e6%88%90%e6%a8%a1%e5%9e%8b%e7%a4%ba%e4%be%8b><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><p>我们将使用以下示例来说明如何构建集成模型。我们将使用泰坦尼克号数据集，并尝试使用不同的技术预测泰坦尼克号的生存情况。图六和图七显示了数据集的样本和目标列对乘客年龄的分布。如果你想跟着做，你可以在我的 <a href=https://github.com/malhamid/Ensemble-Models target=_blank>GitHub</a>
上找到示例代码。</p><p>泰坦尼克号数据集是需要大量特征工程的分类问题之一。我们将尝试只关注模型构建以及如何将集成模型应用于此用例。</p><p>我们将使用不同的算法和技术; 因此，我们将创建一个模型对象来提高代码的可重用性。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Model Class to be used for different ML algorithms</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ClassifierModel</span>(object):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, clf, params<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>clf <span style=color:#f92672>=</span> clf(<span style=color:#f92672>**</span>params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(self, x_train, y_train):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>clf<span style=color:#f92672>.</span>fit(x_train, y_train)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self,x,y):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>clf<span style=color:#f92672>.</span>fit(x,y)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>feature_importances</span>(self,x,y):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>clf<span style=color:#f92672>.</span>fit(x,y)<span style=color:#f92672>.</span>feature_importances_
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>clf<span style=color:#f92672>.</span>predict(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>trainModel</span>(model, x_train, y_train, x_test, n_folds, seed):
</span></span><span style=display:flex><span>    cv <span style=color:#f92672>=</span> KFold(n_splits<span style=color:#f92672>=</span> n_folds, random_state<span style=color:#f92672>=</span>seed)
</span></span><span style=display:flex><span>    scores <span style=color:#f92672>=</span> cross_val_score(model<span style=color:#f92672>.</span>clf, x_train, y_train, scoring<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;accuracy&#39;</span>, cv<span style=color:#f92672>=</span>cv, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> scores
</span></span></code></pre></div><h3 id=random-forest-classifier>Random Forest Classifier
<a hidden class=anchor href=#random-forest-classifier><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Random Forest parameters</span>
</span></span><span style=display:flex><span>rf_params <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;n_estimators&#39;</span>: <span style=color:#ae81ff>400</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;min_samples_leaf&#39;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;max_features&#39;</span> : <span style=color:#e6db74>&#39;sqrt&#39;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>rfc_model <span style=color:#f92672>=</span> ClassifierModel(clf<span style=color:#f92672>=</span>RandomForestClassifier, params<span style=color:#f92672>=</span>rf_params)
</span></span><span style=display:flex><span>rfc_scores <span style=color:#f92672>=</span> trainModel(rfc_model,x_train, y_train, x_test, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><h3 id=extra-trees-classifier>Extra Trees Classifier
<a hidden class=anchor href=#extra-trees-classifier><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Extra Trees Parameters</span>
</span></span><span style=display:flex><span>et_params <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;n_jobs&#39;</span>: <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;n_estimators&#39;</span>:<span style=color:#ae81ff>400</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;min_samples_leaf&#39;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>etc_model <span style=color:#f92672>=</span> ClassifierModel(clf<span style=color:#f92672>=</span>ExtraTreesClassifier, params<span style=color:#f92672>=</span>et_params)
</span></span><span style=display:flex><span>etc_scores <span style=color:#f92672>=</span> trainModel(etc_model,x_train, y_train, x_test, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><h3 id=adaboost-classifier>AdaBoost Classifier
<a hidden class=anchor href=#adaboost-classifier><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># AdaBoost parameters</span>
</span></span><span style=display:flex><span>ada_params <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;n_estimators&#39;</span>: <span style=color:#ae81ff>400</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;learning_rate&#39;</span> : <span style=color:#ae81ff>0.65</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>ada_model <span style=color:#f92672>=</span> ClassifierModel(clf<span style=color:#f92672>=</span>AdaBoostClassifier, params<span style=color:#f92672>=</span>ada_params)
</span></span><span style=display:flex><span>ada_scores <span style=color:#f92672>=</span> trainModel(ada_model,x_train, y_train, x_test, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><h3 id=xgboost-classifier>XGBoost Classifier
<a hidden class=anchor href=#xgboost-classifier><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h3><h1 id=gradient-boosting-parameters>Gradient Boosting parameters
<a hidden class=anchor href=#gradient-boosting-parameters><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Gradient Boosting parameters</span>
</span></span><span style=display:flex><span>gb_params <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;n_estimators&#39;</span>: <span style=color:#ae81ff>400</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>6</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>gbc_model <span style=color:#f92672>=</span> ClassifierModel(clf<span style=color:#f92672>=</span>GradientBoostingClassifier, params<span style=color:#f92672>=</span>gb_params)
</span></span><span style=display:flex><span>gbc_scores <span style=color:#f92672>=</span> trainModel(gbc_model,x_train, y_train, x_test, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>让我们将所有模型交叉验证精度结合在五个折上。</p><p>现在，让我们构建一个堆叠模型，其中一个新的更强的模型从所有这些弱学习器中学习预测。我们用于训练先前模型的标签向量将保持不变。特征是从每个分类器收集的预测。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_train <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack((etc_train_pred, rfc_train_pred, ada_train_pred, gbc_train_pred, svc_train_pred))
</span></span></code></pre></div><p>现在，让我们看看构建 XGBoost 模型仅学习生成的预测是否会表现得更好。但首先，我们将快速了解分类器预测之间的相关性。</p><p><img loading=lazy src=http://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/10_ensemble-model.png alt=集成模型></p><p><em>图 9：分类器投票标签之间的 Pearson 相关性</em></p><p>现在，我们将构建一个模型来组合来自多个贡献分类器的预测。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>trainStackModel</span>(x_train, y_train, x_test, n_folds, seed):
</span></span><span style=display:flex><span>    cv <span style=color:#f92672>=</span> KFold(n_splits<span style=color:#f92672>=</span>n_folds, random_state<span style=color:#f92672>=</span>seed)
</span></span><span style=display:flex><span>    gbm <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>XGBClassifier(
</span></span><span style=display:flex><span>        n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>2000</span>,
</span></span><span style=display:flex><span>        max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        min_child_weight<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        gamma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,                        
</span></span><span style=display:flex><span>        subsample<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>,
</span></span><span style=display:flex><span>        colsample_bytree<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>,
</span></span><span style=display:flex><span>        objective<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;binary:logistic&#39;</span>,
</span></span><span style=display:flex><span>        scale_pos_weight<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>fit(x_train, y_train)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    scores <span style=color:#f92672>=</span> cross_val_score(gbm, x_train, y_train, scoring<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;accuracy&#39;</span>, cv<span style=color:#f92672>=</span>cv)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> scores
</span></span></code></pre></div><p>之前创建的基础分类器表示 Level-0 模型，新的 XGBoost 模型表示 Level-1 模型。该组合说明了在样本数据的预测上训练的元模型。您可以在下面看到新堆叠模型的准确性与基础分类器之间的快速比较：</p><p><img loading=lazy src=http://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/11_ensemble-model.png alt=集成模型></p><h2 id=仔细考虑>仔细考虑
<a hidden class=anchor href=#%e4%bb%94%e7%bb%86%e8%80%83%e8%99%91><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></a></h2><ul><li>噪声、<a href=https://builtin.com/data-science/bias-variance-tradeoff target=_blank>偏差和方差</a>
：来自多个模型的决策组合有助于提高整体性能。因此，使用集成模型的关键原因之一是克服噪声、偏差和方差。如果集成模型没有提供集体经验来提高这种情况下的准确性，那么有必要仔细重新考虑是否有必要使用。</li><li>简单性和可 <a href=https://builtin.com/artificial-intelligence/ai-right-explanation target=_blank>解释</a>
性：机器学习模型，尤其是那些投入生产环境的模型，应该易于解释。当您使用集成模型时，您能够解释最终模型决策的机会会大大降低。</li><li>泛化：有许多说法认为集成模型具有更强的泛化能力，但其他报告的用例显示出更多的泛化错误。因此，如果没有仔细的训练过程，集成模型可能会快速产生高过拟合模型。</li><li>推理时间：尽管我们可能不愿意接受更长的模型训练时间，但推理时间仍然很关键。将集成模型部署到生产环境中时，传递多个模型所需的时间会增加，并且可能会减慢预测任务的吞吐量。</li></ul><p>集成模型是机器学习的一种极好的方法，因为它们为分类和回归问题提供了各种技术。现在您了解了集成模型的类型，我们如何构建简单的集成模型，以及它们如何提高模型的准确性。</p><nav class=pagenav><a class=prev href=/blog/posts/0003-%E8%AF%91%E4%BD%BF%E7%94%A8%E4%BE%BF%E6%90%BA%E6%A8%A1%E5%BC%8F%E7%9A%84vscode/><span class=direction>« Prev</span><br><span>【译】使用便携模式的VSCode </span></a><a class=next href=/blog/posts/0005-arxiv-2405---rethinking-scanning-strategies-with-vision-mamba-in-semantic-segmentation-of-remote-sensing-imagery---an-experimental-study/><span class=direction>Next »</span><br><span>ArXiv 2405 - Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery - An Experimental Study</span></a></nav></main><footer><div class=copyright>© 2025 Lart Pang</div><div class=attribution>Built with <a href=https://gohugo.io target=_blank>Hugo</a> & <a href=https://github.com/mnjm/kayal target=_blank>Kayal</a></div><div>Last update Mar 19, 2025</div></footer></div><script defer type=text/javascript src=/blog/js/theme.min.eaccfb587dc18aa6c89ddcfe908efce320a265a23826cb3af2032a3f425a36f9b12c233ec2743b2c1017217c7699a7cc7619483ea2257f88b2b63b7b51bbaaa8.js integrity="sha512-6sz7WH3BiqbIndz+kI784yCiZaI4Jss68gMqP0JaNvmxLCM+wnQ7LBAXIXx2mafMdhlIPqIlf4iytjt7UbuqqA=="></script><script defer type=text/javascript src=/blog/js/codecopy.min.6319d0ad3eae498a0591b4ec1d52cc0156f5c3cb95390089d7d0ac7fe6dfb183161c14f935d25b5ad896840e2a9d6a23f555a7976954d0a263c24a11272aa747.js integrity="sha512-YxnQrT6uSYoFkbTsHVLMAVb1w8uVOQCJ19Csf+bfsYMWHBT5NdJbWtiWhA4qnWoj9VWnl2lU0KJjwkoRJyqnRw=="></script><script defer type=text/javascript src=/blog/js/search.min.ad22883b43082f62bc508d1f4e145eb2c6192213144784eadc12e3144ab7af50f826fde65c4a45a68d381b1335f3ff2c189221533436acdffe2c5abd589ce6b8.js integrity="sha512-rSKIO0MIL2K8UI0fThRessYZIhMUR4Tq3BLjFEq3r1D4Jv3mXEpFpo04GxM18/8sGJIhUzQ2rN/+LFq9WJzmuA=="></script></body></html>