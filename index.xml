<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>啊，哈！</title><link>https://lartpang.github.io/blog/</link><description>Recent content on 啊，哈！</description><generator>Hugo -- 0.145.0</generator><language>en-us</language><lastBuildDate>Mon, 17 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lartpang.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>啊，哈！</title><link>https://lartpang.github.io/blog/about/about/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://lartpang.github.io/blog/about/about/</guid><description>一个简单的博客 :link: https://lartpang.github.io/blog 这是一个基于 GitHub Issue 和 Page 的简单的静态博客。</description></item><item><title>Privacy-Enhancing Technologies in Biomedical Data Science</title><link>https://lartpang.github.io/blog/posts/0001-privacy-enhancing-technologies-in-biomedical-data-science/</link><pubDate>Sat, 26 Oct 2024 13:48:07 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0001-privacy-enhancing-technologies-in-biomedical-data-science/</guid><description> 1. 引言 数据共享是生物医学创新的重要推动力。公共数据仓库和生物银行使不同组织的研究人员能够分析他们自己可能无法收集的大量人类主体数据。许多学术实验室、商业企业和医院已经联合起来形成合作联盟，共享生物医学数据，希望从数据集中提取出由于数据集规模有限而无法被单个实体访问的洞察。政府实体（例如，美国国立卫生研究院数据管理和共享政策；https://sharing.nih.gov ）和国际标准制定组织如全球基因组学和健康联盟（1）制定的政策和指南在维护生物医学界的数据共享文化中发挥了关键作用，这一传统根植于人类基因组计划等里程碑式的合作努力中。</description></item><item><title>【Zotero7】重新安装与数据配置</title><link>https://lartpang.github.io/blog/posts/0002-zotero7%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE/</link><pubDate>Sun, 01 Sep 2024 10:00:36 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0002-zotero7%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE/</guid><description> 前提 之前的附件配置是基于链接的形式，而附件是单独文件夹存储的。 新安装时，设备上已经移除了所有之前的设置（几之前的所有配置信息都被保存到了云端你自己的账号下）。 设备上只剩下了之前的pdf附件文件夹和其中的pdf附件了。 配置过程 安装最新版的Zotero7的安装包：https://www.zotero.org/download/ 安装后到设置中配置： 先到 同步 选项卡中登录你的Zotero账号密码，将基础的设置同步下来。 再配置 高级 选项卡中的已链接附件的根目录到你的附件文件夹目录。 重启Zotero即可。 常规情况下到这步应该就没问题了。 后续配置 手动安装插件市场：https://github.com/syt2/zotero-addons 安装好后从插件市场中安装附件管理器插件：Zotero Attanger 对附件管理器插件中配置新的附件采用链接形式，配置源路径、附加类型（链接）和靶路径等以适配你现有附件的存储格式。 安装其他的插件，并进行相关的配置。</description></item><item><title>【译】使用便携模式的VSCode</title><link>https://lartpang.github.io/blog/posts/0003-%E8%AF%91%E4%BD%BF%E7%94%A8%E4%BE%BF%E6%90%BA%E6%A8%A1%E5%BC%8F%E7%9A%84vscode/</link><pubDate>Tue, 30 Jul 2024 07:17:42 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0003-%E8%AF%91%E4%BD%BF%E7%94%A8%E4%BE%BF%E6%90%BA%E6%A8%A1%E5%BC%8F%E7%9A%84vscode/</guid><description> 原始文档：https://code.visualstudio.com/docs/editor/portable#_migrate-to-portable-mode
Visual Studio Code 支持便携模式 。此模式使 VS Code 创建和维护的所有数据都位于自身附近，因此可以在环境中移动。
此模式还提供了一种设置 VS Code 扩展的安装文件夹位置的方法，这对于阻止在 Windows AppData 文件夹中安装扩展的企业环境非常有用。
便携模式支持 Windows 的 ZIP 下载，Linux 的 TAR.GZ 下载，以及 macOS 的常规应用程序下载。请参阅下载页面 以查找适合您平台的正确文件。
[!note] 请勿尝试在从 Windows User or System installers 进行安装时配置便携模式。便携模式仅在 Windows ZIP 存档上受支持。另请注意，Windows ZIP压缩包不支持自动更新。
启用便携模式 Windows、Linux 解压缩 VS Code 下载后，在 VS Code 的文件夹中创建一个文件夹：data</description></item><item><title>【译】集成模型：它们是什么以及何时应该使用它们？</title><link>https://lartpang.github.io/blog/posts/0004-%E8%AF%91%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%83%E4%BB%AC%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%BD%95%E6%97%B6%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC/</link><pubDate>Thu, 25 Jul 2024 08:32:08 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0004-%E8%AF%91%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%83%E4%BB%AC%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%BD%95%E6%97%B6%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC/</guid><description> [!IMPORTANT] 原文：https://builtin.com/machine-learning/ensemble-model 有时，一个模型是不够的。在这篇集成模型指南中，我将向您介绍如何（以及何时）对机器学习模型使用集成技术。
每当我们试图做出重要决定时，我们都会尝试收集尽可能多的信息，并向专家寻求建议。我们可以收集的信息越多，我们（和我们周围的人）就越信任决策过程。
机器学习预测遵循类似的行为。模型处理给定的输入并产生结果。结果是根据模型在训练过程中看到的模式进行预测。
在许多情况下，一个模型是不够的，本文阐明了这一点。我们何时以及为何需要多个模型？我们如何训练这些模型？这些模型应该提供什么样的多样性？所以，让我们直接进入。
如果你想看一个如何构建集成模型的例子，你可以跳到最后！
[!NOTE] 什么是集成模型？
集成模型是一种机器学习方法，用于在预测过程中结合多个其他模型。这些模型称为基础估计器。集成模型提供了一种解决方案，可以克服构建单个估计器的技术挑战。
构建单一估计器的技术挑战包括：
高方差：模型对为学习特征提供的输入非常敏感。 准确性低：一个模型（或一种算法）无法拟合整个训练数据，可能无法为您提供项目所需的细微差别。 具有噪声和偏差的特征：模型在进行预测时严重依赖于太少的特征。 集成算法 单一算法可能无法对给定的数据集做出完美的预测。机器学习算法有其局限性，生成高精度的模型具有挑战性。如果我们构建并组合多个模型，我们就有机会提高整体准确性。然后，我们通过将每个模型的输出与两个目标聚合来实现模型组合：</description></item><item><title>ArXiv 2405 - Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery - An Experimental Study</title><link>https://lartpang.github.io/blog/posts/0005-arxiv-2405---rethinking-scanning-strategies-with-vision-mamba-in-semantic-segmentation-of-remote-sensing-imagery---an-experimental-study/</link><pubDate>Fri, 17 May 2024 11:08:54 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0005-arxiv-2405---rethinking-scanning-strategies-with-vision-mamba-in-semantic-segmentation-of-remote-sensing-imagery---an-experimental-study/</guid><description> Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery - An Experimental Study</description></item><item><title>ICLR 2024 - FasterViT - Fast Vision Transformers with Hierarchical Attention</title><link>https://lartpang.github.io/blog/posts/0006-iclr-2024---fastervit---fast-vision-transformers-with-hierarchical-attention/</link><pubDate>Fri, 17 May 2024 09:21:14 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0006-iclr-2024---fastervit---fast-vision-transformers-with-hierarchical-attention/</guid><description> FasterViT: Fast Vision Transformers with Hierarchical Attention 论文：https://arxiv.org/abs//2306.06189 代码：https://github.com/NVlabs/FasterViT 解读：https://mp.weixin.qq.com/s/0AXri6EUrXd6Hwf2HU5Wmg 原始文档：https://github.com/lartpang/blog/issues/18</description></item><item><title>CVPR 参会记录</title><link>https://lartpang.github.io/blog/posts/0007-cvpr-%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/</link><pubDate>Wed, 17 Apr 2024 03:32:15 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0007-cvpr-%E5%8F%82%E4%BC%9A%E8%AE%B0%E5%BD%95/</guid><description> 【获得邀请函】在 CVPR 官网参会注册后即可申请：https://cvpr.thecvf.com 【申请美签】填写 DS-160：https://ceac.state.gov/genniv 需要准备电子照片，相关要求可见：https://travel.state.gov/content/travel/en/us-visas/visa-information-resources/photos.html 美国签证DS160照片和要携带面试上交的照片必须要同一张吗？ 考虑选择申请 B1 签证。 Passport Book Number（似乎可以不填）：办理美国签证护照本编号是什么 邀请函中提供了填写 DS-160 需要的美国当地的联系人信息。 中文姓名电码查询，注意电码必须四位，如果连续的多个，可以用空格隔开：https://apps.chasedream.com/chinese-commercial-code 会议地址的信息可以从会议中心的官网上找到：https://seattleconventioncenter.com 过程中会用到很多信息和选项，可以参考这些帖子： 各种填写细节： 美国签证办理全流程攻略（上） - 火星大章鱼的文章 - 知乎 赴美签证DS160表格填写说明 如何填写美国B1-B2申请表（DS-160） 美国签证DS160表格填写样本 关于社交账号：如何回答DS-160表格上“你是否有社交网络账号”的问题 【申请面签】页面为 https://www.ustraveldocs.com/cn/zh/nonimmigrant-visa 关于 B1 签证的一些要求：https://www.ustraveldocs.com/cn/zh/business-visa 关于费用支付的一些信息：https://www.ustraveldocs.com/cn/zh/payment-options 【注意事项】 费用支付后，记住支付码，后面取消预约后还可以在新的预约中使用这个码。 预约系统有防止滥用预约的机制，尽量不要反复操作，容易被封号72小时。如果被封了，期间不要再登录系统，容易继续延长。 【面签过程】 美国签证被CHECK怎么办？一定会被拒吗？哪种情况会导致CHECK? 准备好DS-160的确认页和最终预约确认页一起打印出来，面试需要。 到日期准备参加面试吧。建议面试前多看看小红书上关于不同地区签证处的面试情况。提前做好心理准备。 【面前后续】面签之后如果没有当场通过或者被拒绝，那可能就要收走材料面临行政审查了。可以在 https://ceac.state.gov/CEACStatTracker/Status.aspx 中检索你的当前状态。 实锤！美国签证查询系统更新，行政审查的“Refused”不是最终裁决！ - 鱼总美签的文章 - 知乎 等了一个月，终于从refused变为了issued。护照寄出来，但是还没收掉。感觉时间可能不太行了。 一些建议的材料（有点旧了） ：</description></item><item><title>CVPR 2024 - SED - A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation</title><link>https://lartpang.github.io/blog/posts/0008-cvpr-2024---sed---a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/</link><pubDate>Thu, 11 Apr 2024 06:37:48 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0008-cvpr-2024---sed---a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation/</guid><description> CVPR 2024 - SED - A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation 论文：https://arxiv.org/abs/2311.15537 代码：https://github.com/xb534/SED 这篇文章提出了一种名为 SED 的简单编码器解码器，用于结合 CLIP 的 open-vocabulary 能力实现了开放词汇语义分割。在多个语义分割数据集上的实验证明了 SED 在开放词汇准确性和效率方面的优势。当使用 ConvNeXt-B 时，SED 在 ADE20K 上的 mIoU 得分为 31.6%，并且在单个 A6000 上每张图像只需 82 毫秒。</description></item><item><title>CVPR 2024 - OVFoodSeg - Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation</title><link>https://lartpang.github.io/blog/posts/0009-cvpr-2024---ovfoodseg---elevating-open-vocabulary-food-image-segmentation-via-image-informed-textual-representation/</link><pubDate>Wed, 10 Apr 2024 11:12:23 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0009-cvpr-2024---ovfoodseg---elevating-open-vocabulary-food-image-segmentation-via-image-informed-textual-representation/</guid><description> CVPR 2024 - OVFoodSeg - Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation 论文：https://arxiv.org/abs/2404.01409 主要内容 大量食材之间的类别差异、新食材的出现以及与大型食物分割数据集相关的高注释成本。现有方法主要采用封闭词汇和静态文本嵌入设置，往往无法有效处理食材，特别是新颖和多样化的食材。为此本文提出了一种新的开放词汇食品图像分割（Open-Vocabulary Food Image Segmentation）框架 OVFoodSeg，通过采用图像感知文本表示来提升开放词汇食品图像分割的能力。这一任务和框架旨在解决现有方法在处理新和多样化的食材时的不足。</description></item><item><title>CVPR 2024 - Open-Vocabulary Video Anomaly Detection</title><link>https://lartpang.github.io/blog/posts/0010-cvpr-2024---open-vocabulary-video-anomaly-detection/</link><pubDate>Wed, 10 Apr 2024 09:39:46 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0010-cvpr-2024---open-vocabulary-video-anomaly-detection/</guid><description> CVPR 2024 - Open-Vocabulary Video Anomaly Detection 论文：https://arxiv.org/abs/2311.07042</description></item><item><title>CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection</title><link>https://lartpang.github.io/blog/posts/0011-cvpr-2024---retrieval-augmented-open-vocabulary-object-detection/</link><pubDate>Wed, 10 Apr 2024 07:50:42 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0011-cvpr-2024---retrieval-augmented-open-vocabulary-object-detection/</guid><description> CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection 论文：https://arxiv.org/abs/2404.05687 代码：https://github.com/mlvlab/RALF 本文提出了一种新的开放词汇目标检测方法 Retrieval-Augmented Losses and visual Features (RALF)。RALF 通过从大型词汇库中检索词汇并增强损失函数和视觉特征来提高检测器对新类别的泛化能力。</description></item><item><title>CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts</title><link>https://lartpang.github.io/blog/posts/0012-cvpr-2024---rethinking-interactive-image-segmentationwith-low-latency-high-quality-and-diverse-prompts/</link><pubDate>Wed, 10 Apr 2024 06:51:40 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0012-cvpr-2024---rethinking-interactive-image-segmentationwith-low-latency-high-quality-and-diverse-prompts/</guid><description> CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts 论文：https://arxiv.org/bas/2404.00741 代码：https://github.com/uncbiag/SegNext</description></item><item><title>CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation</title><link>https://lartpang.github.io/blog/posts/0013-cvpr-2024---rethinking-inductive-biases-for-surface-normal-estimation/</link><pubDate>Wed, 10 Apr 2024 06:27:49 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0013-cvpr-2024---rethinking-inductive-biases-for-surface-normal-estimation/</guid><description> CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation 论文：https://arxiv.org/abs/2403.00712 代码：https://github.com/baegwangbin/DSINE 该研究重新思考了表面法线估计的归纳偏置，提出了利用逐像素射线方向并学习相邻表面法线之间相对旋转关系的方法。相比于现有采用通用密集预测模型的方法，该方法能生成清晰且平滑的预测，并且在训练数据规模更小的情况下展现出更强的泛化能力。</description></item><item><title>Rethinking Interactive Image Segmentation: Feature Space Annotation</title><link>https://lartpang.github.io/blog/posts/0014-rethinking-interactive-image-segmentation--feature-space-annotation/</link><pubDate>Wed, 10 Apr 2024 06:26:16 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0014-rethinking-interactive-image-segmentation--feature-space-annotation/</guid><description> Rethinking Interactive Image Segmentation Feature Space Annotation 论文：https://arxiv.org/abs/2101.04378 代码：https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation 本文提出了一种新的交互式图像分割方法，通过特征空间注释来同时对多张图像进行分割注释，这与现有的交互式分割方法在图像领域进行注解的方式形成了鲜明对比。研究结果表明，这种特征空间注解的方法在前景分割数据集上可以取得与最先进的方法相媲美的结果。</description></item><item><title>CVPR 2024 - Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications</title><link>https://lartpang.github.io/blog/posts/0015-cvpr-2024---efficient-deformable-convnets--rethinking-dynamic-and-sparse-operator-for-vision-applications/</link><pubDate>Wed, 10 Apr 2024 06:24:56 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0015-cvpr-2024---efficient-deformable-convnets--rethinking-dynamic-and-sparse-operator-for-vision-applications/</guid><description> CVPR 2024 - Efficient Deformable ConvNets - Rethinking Dynamic and Sparse Operator for Vision Applications</description></item><item><title>CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization</title><link>https://lartpang.github.io/blog/posts/0016-cvpr-2024---rethinking-the-evaluation-protocol-of-domain-generalization/</link><pubDate>Wed, 10 Apr 2024 06:23:52 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0016-cvpr-2024---rethinking-the-evaluation-protocol-of-domain-generalization/</guid><description> CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization</description></item><item><title>CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection</title><link>https://lartpang.github.io/blog/posts/0017-cvpr-2024---rethinking-the-up-sampling-operations-in-cnn-based-generative-network-for-generalizable-deepfake-detection/</link><pubDate>Wed, 10 Apr 2024 06:22:51 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0017-cvpr-2024---rethinking-the-up-sampling-operations-in-cnn-based-generative-network-for-generalizable-deepfake-detection/</guid><description> CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection 论文：https://arxiv.org/abs/2312.10461 代码：https://github.com/chuangchuangtan/NPR-DeepfakeDetection</description></item><item><title>ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution</title><link>https://lartpang.github.io/blog/posts/0018-iclr-2024---featup---a-model-agnostic-framework-for-features-at-any-resolution/</link><pubDate>Wed, 03 Apr 2024 05:02:42 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0018-iclr-2024---featup---a-model-agnostic-framework-for-features-at-any-resolution/</guid><description> ICLR 2024 | FeatUp: A Model-Agnostic Framework for Features at Any Resolution</description></item><item><title>Arixv 2403 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey</title><link>https://lartpang.github.io/blog/posts/0019-arixv-2403---parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey/</link><pubDate>Wed, 03 Apr 2024 04:46:25 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0019-arixv-2403---parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey/</guid><description> Arixv 2403 | Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</description></item><item><title>Tips for Qt</title><link>https://lartpang.github.io/blog/posts/0020-tips-for-qt/</link><pubDate>Mon, 25 Mar 2024 09:49:38 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0020-tips-for-qt/</guid><description> Tips for Qt Set a proper mirror for MaintenanceTool.exe From: https://mirrors.tuna.tsinghua.edu.cn/help/qt/</description></item><item><title>Six methods of indexing pixels of Mat in OpenCV</title><link>https://lartpang.github.io/blog/posts/0021-six-methods-of-indexing-pixels-of-mat-in-opencv/</link><pubDate>Mon, 25 Mar 2024 09:41:52 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0021-six-methods-of-indexing-pixels-of-mat-in-opencv/</guid><description> Six methods of indexing pixels of Mat in OpenCV .at&lt;>() // modify the pixel directly for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols; ++w) { image.at&lt;Vec3b>(h, w)[0] = 255; image.at&lt;Vec3b>(h, w)[1] = 0; image.at&lt;Vec3b>(h, w)[2] = 0; } } // modify the pixel by the reference for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols; ++w) { Vec3b&amp; bgr = image.at&lt;Vec3b>(h, w); bgr.val[0] = 0; bgr.val[1] = 255; bgr.val[2] = 0; } } // the image has one channel for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols / 2; ++w) { image.at&lt;uchar>(h, w) = 128; } } .ptr&lt;>() // use uchar type for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols / 2; ++w) { uchar* ptr = image.ptr&lt;uchar>(h, w); ptr[0] = 255; ptr[1] = 0; ptr[2] = 0; } } // use cv::Vec3b type for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols / 2; ++w) { Vec3b* ptr = image.ptr&lt;Vec3b>(h, w); ptr->val[0] = 0; ptr->val[1] = 255; ptr->val[2] = 0; } } // use the row pointer and the image has one channel for (int h = 0; h &lt; image.rows; ++h) { uchar* ptr = image.ptr(h); for (int w = 0; w &lt; image.cols / 2; ++w) { ptr[w] = 128; } } // use the pixel pointer and the image has one channel for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols / 2; ++w) { uchar* ptr = image.ptr&lt;uchar>(h, w); *ptr = 255; } } iterator // the image has three channels Mat_&lt;Vec3b>::iterator it = image.begin&lt;Vec3b>(); Mat_&lt;Vec3b>::iterator itend = image.end&lt;Vec3b>(); for (; it != itend; ++it) { (*it)[0] = 255; (*it)[1] = 0; (*it)[2] = 0; } // the image has one channel Mat_&lt;uchar>::iterator it1 = image.begin&lt;uchar>(); Mat_&lt;uchar>::iterator itend1 = image.end&lt;uchar>(); for (; it1 != itend1; ++it1) { (*it1) = 128; } .data pointer // 3 channels uchar* data = image.data; for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols / 2; ++w) { *data++ = 128; *data++ = 128; *data++ = 128; } } // 1 channel uchar* data = image.data; for (int h = 0; h &lt; image.rows; ++h) { for (int w = 0; w &lt; image.cols / 2; ++w) { *data++ = 128; } } .row() and .col() for (int i = 0; i &lt; 100; ++i) { image.row(i).setTo(Scalar(0, 0, 0)); // modify the i th row data image.col(i).setTo(Scalar(0, 0, 0)); // modify the i th column data } when isContinuous() is true Mat image = imread("..."); int nRows = image.rows; int nCols = image.cols * image.channels(); if (image.isContinuous()) { nCols = nRows * nCols; nRows = 1; } for (int h = 0; h &lt; nRows; ++h) { uchar* ptr = image.ptr&lt;uchar>(h); for (int w = 0; w &lt; nCols; ++w) { // ptr[w] = 128 ; *ptr++ = 128; } } Reference http://t.csdn.cn/bSDNn</description></item><item><title>Snippets of OpenVINO-CPP for Model Inference</title><link>https://lartpang.github.io/blog/posts/0022-snippets-of-openvino-cpp-for-model-inference/</link><pubDate>Mon, 25 Mar 2024 09:41:05 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0022-snippets-of-openvino-cpp-for-model-inference/</guid><description> Snippets of OpenVINO-CPP for Model Inference Header File #include &lt;openvino/openvino.hpp> Create Infer Request void preprocessing(std::shared_ptr&lt;ov::Model> model) { ov::preprocess::PrePostProcessor ppp(model); ppp.input().tensor().set_layout("NHWC"); // input data is NHWC from OpenCV Mat ppp.input().model().set_layout("NCHW"); // In the model, the layout is NCHW model = ppp.build(); } ov::Core core; auto model = core.read_model(model_path); # can use onnx or openvino's xml file preprocessing(model); auto compiled_model = core.compile_model(model, "CPU"); // Or without `"CPU"` auto input_port = compiled_model.input(); auto infer_request = compiled_model.create_infer_request(); Input and Output single input infer_request.set_input_tensor(blob); infer_request.crop_net.infer(); single output ov::Tensor single_output = this->point_net.get_output_tensor(0); multiple outputs ov::Tensor multi_outputs0 = this->point_net.get_output_tensor(0); ov::Tensor multi_outputs1 = this->point_net.get_output_tensor(1); OpenCV cv::Mat &lt;-> OpenVINO ov::Tensor The key to these steps is the alignment of the data layout.</description></item><item><title>Build OpenCV and OpenVINO for Windows 10 with VS 2022</title><link>https://lartpang.github.io/blog/posts/0023-build-opencv-and-openvino-for-windows-10-with-vs-2022/</link><pubDate>Mon, 25 Mar 2024 09:38:03 +0000</pubDate><guid>https://lartpang.github.io/blog/posts/0023-build-opencv-and-openvino-for-windows-10-with-vs-2022/</guid><description> Build OpenCV and OpenVINO for Windows 10 with VS 2022 In this guide, I will build the two powerful open-source libraries, i.e., OpenCV and OpenVINO for running my deeplearning model on windows 10. Interestingly, both libraries are closely associated with Intel 🖥️.</description></item></channel></rss>